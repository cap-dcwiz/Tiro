{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tiro \u00b6 Tiro provides the complete toolchain for DCWiz data interface development, data validation, data exchanging and mock data generation. Features \u00b6 The purpose of this project is to accelerate and standardize the development procedure of data interfacing for the DCWiz platform. More specifically, it provides tools to generate formal JSON schema to describe data points in a data center required by multiple use cases; automatically generate mock data for the required data points; validate and evaluate the quality of the data collected from other systems; provide the plugins of Karez framework to collect data; provide the data pump of Utinni toolkit for making use of the collected data or mock data. Getting Started \u00b6 Please refer to the Getting Started section for more information.","title":"Home"},{"location":"#tiro","text":"Tiro provides the complete toolchain for DCWiz data interface development, data validation, data exchanging and mock data generation.","title":"Tiro"},{"location":"#features","text":"The purpose of this project is to accelerate and standardize the development procedure of data interfacing for the DCWiz platform. More specifically, it provides tools to generate formal JSON schema to describe data points in a data center required by multiple use cases; automatically generate mock data for the required data points; validate and evaluate the quality of the data collected from other systems; provide the plugins of Karez framework to collect data; provide the data pump of Utinni toolkit for making use of the collected data or mock data.","title":"Features"},{"location":"#getting-started","text":"Please refer to the Getting Started section for more information.","title":"Getting Started"},{"location":"license/","text":"MIT License Copyright (c) 2022, cap-dcwiz Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"quickstart/","text":"Quickstart \u00b6 Defining a scenario and multiple use cases \u00b6 To use tiro tools, first, we need to define a scenario ( example ) file and several uses files ( exmaple ). Generating data schema and examples \u00b6 tiro schema show config/scenario.yaml config/use1.yaml -o schema.json This command will generate the schema and save it to this file . Or, without the -o option, it will print the generated schema to stdout. Usually, the schema can be a little bit difficult for human to read. In such cases, the following command can generate an example that matches the schema. tiro schema example config/scenario.yaml config/use1.yaml -o example.json The generated json example can be viewed here . Again, without the -o option, it will print the generated example to stdout. Mock data generater \u00b6 The following command will setup a web server that generates mock data for each data point required by at least one use case. tiro mock serve config/scenario.yaml config/use1.yaml -p 8000 Once the server has started, the documentation can be accessed on http://127.0.0.1:8000/docs . Essentially, there will be an endpoint for each data point. For example, the data point with the path Room.8f14567a-c582-11ec-852a-aa966665d395.Telemetry.Temperature can be fetched from GET http://127.0.0.1:8000/points/Room.8f14567a-c582-11ec-852a-aa966665d395.Telemetry.Temperature . The list of all data point path can be retrieved from GET http://127.0.0.1:8000/points/ . With the server running, one can deploy a karez framwork to collect mock data points and test the application without intergating to real data center. Data validator \u00b6 Sometimes, one may want to find out whether the data collection procedure can successfully retrieve all the data points it requires. This usually happens when the application and data collection procedure are developed by different teams. In this case, a validator server can be setup to validate the collected data. tiro validate serve config/scenario.yaml config/use1.yaml -p 8001 -r 60 Again, once the server has started, the documentation can be accessed on http://127.0.0.1:8001/docs . The server will continuously receive data points, and validate the collected data every 60 seconds. Basically, it will assemble all the data points received within the minutes and combine them into a json like the example , and validate the json against the schema . If the validation is passed, it should indicate that all the required data points has been collected. The interval can be adjust. Alternatively, there also a Karez aggregator that can work as a validator, check here for more details. Karez plugins \u00b6 There several Karez plugins can be find in the project: tiro.plugins.karez.ConnectorForMockServer : a connector plugin to collect data from the mock server or any other server provides the same endpoints. tiro.plugins.karez.TiroConverter : a converter to format Tiro data. (To be detailed.) tiro.plugins.karez.DispatcherForMockServer : a connector to dispatch data collection tasks from the mocker server tiro.plugins.karez.ValidationAggregator : an aggregator works as a validator, it listens to all data point collection messages and validate the data periodically. The validation logs will be write to a log file. tiro.plugins.karez.ArangoAggregator : an aggregator to store tiro data points in the ArangoDB graph database. (Details to be discussed later.) A example Karez configuration can be found here . Also, an complete stack docker compose example involving the full procedure of mock data generation, data collection and data validation can be found here . Data collection protocol \u00b6 To be complete. Use of collected data \u00b6 To be complete.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#defining-a-scenario-and-multiple-use-cases","text":"To use tiro tools, first, we need to define a scenario ( example ) file and several uses files ( exmaple ).","title":"Defining a scenario and multiple use cases"},{"location":"quickstart/#generating-data-schema-and-examples","text":"tiro schema show config/scenario.yaml config/use1.yaml -o schema.json This command will generate the schema and save it to this file . Or, without the -o option, it will print the generated schema to stdout. Usually, the schema can be a little bit difficult for human to read. In such cases, the following command can generate an example that matches the schema. tiro schema example config/scenario.yaml config/use1.yaml -o example.json The generated json example can be viewed here . Again, without the -o option, it will print the generated example to stdout.","title":"Generating data schema and examples"},{"location":"quickstart/#mock-data-generater","text":"The following command will setup a web server that generates mock data for each data point required by at least one use case. tiro mock serve config/scenario.yaml config/use1.yaml -p 8000 Once the server has started, the documentation can be accessed on http://127.0.0.1:8000/docs . Essentially, there will be an endpoint for each data point. For example, the data point with the path Room.8f14567a-c582-11ec-852a-aa966665d395.Telemetry.Temperature can be fetched from GET http://127.0.0.1:8000/points/Room.8f14567a-c582-11ec-852a-aa966665d395.Telemetry.Temperature . The list of all data point path can be retrieved from GET http://127.0.0.1:8000/points/ . With the server running, one can deploy a karez framwork to collect mock data points and test the application without intergating to real data center.","title":"Mock data generater"},{"location":"quickstart/#data-validator","text":"Sometimes, one may want to find out whether the data collection procedure can successfully retrieve all the data points it requires. This usually happens when the application and data collection procedure are developed by different teams. In this case, a validator server can be setup to validate the collected data. tiro validate serve config/scenario.yaml config/use1.yaml -p 8001 -r 60 Again, once the server has started, the documentation can be accessed on http://127.0.0.1:8001/docs . The server will continuously receive data points, and validate the collected data every 60 seconds. Basically, it will assemble all the data points received within the minutes and combine them into a json like the example , and validate the json against the schema . If the validation is passed, it should indicate that all the required data points has been collected. The interval can be adjust. Alternatively, there also a Karez aggregator that can work as a validator, check here for more details.","title":"Data validator"},{"location":"quickstart/#karez-plugins","text":"There several Karez plugins can be find in the project: tiro.plugins.karez.ConnectorForMockServer : a connector plugin to collect data from the mock server or any other server provides the same endpoints. tiro.plugins.karez.TiroConverter : a converter to format Tiro data. (To be detailed.) tiro.plugins.karez.DispatcherForMockServer : a connector to dispatch data collection tasks from the mocker server tiro.plugins.karez.ValidationAggregator : an aggregator works as a validator, it listens to all data point collection messages and validate the data periodically. The validation logs will be write to a log file. tiro.plugins.karez.ArangoAggregator : an aggregator to store tiro data points in the ArangoDB graph database. (Details to be discussed later.) A example Karez configuration can be found here . Also, an complete stack docker compose example involving the full procedure of mock data generation, data collection and data validation can be found here .","title":"Karez plugins"},{"location":"quickstart/#data-collection-protocol","text":"To be complete.","title":"Data collection protocol"},{"location":"quickstart/#use-of-collected-data","text":"To be complete.","title":"Use of collected data"},{"location":"background/glossary/","text":"Glossary \u00b6 Entity , Asset A Entity or a Asset is a device, equipment or virtual group in an IoT environment. An entity can have several child entities. Data Point A telemetry or attribute of the asset. Telemetry A Telemetry is a data point that can be measured or observed. It is a time series data point that changes over time. Attribute A Attribute is a data point that represents the state or characteristic of the asset. It is a static data point. Scenario A Scenario contains a set of assets and their relations. The assets are organized in a tree structure. Uses A Uses file contains a set of data points that are required by a use case. The data points are assigned to the assets in the scenario. Schema A JSON schema to describe the scenario and the information of data points required by multiple use cases. Asset Path A path to an asset or a data point in the scenario. The path is a list of asset types and names from the root asset to the target asset. For example, an asset with path \"DataHall.data_hall.Rack.rack_1.Server.server_1\" is the server named server_1 in a rack named rack_1 in a data hall named data_hall . And the data point with path \"DataHall.data_hall.Rack.rack_1.Server.server_1.Telemetry.temperature\" is the temperature telemetry of the server named server_1 in a rack named rack_1 in a data hall named data_hall . In some occasions, the Telemetry or Attribute before the data point name can be omitted. Type Path Asset Path without asset and data point types. For example, the type path of the asset with path \"DataHall.data_hall.Rack.rack_1.Server.server_1\" is \"DataHall.Rack.Server\" .","title":"Glossary"},{"location":"background/glossary/#glossary","text":"Entity , Asset A Entity or a Asset is a device, equipment or virtual group in an IoT environment. An entity can have several child entities. Data Point A telemetry or attribute of the asset. Telemetry A Telemetry is a data point that can be measured or observed. It is a time series data point that changes over time. Attribute A Attribute is a data point that represents the state or characteristic of the asset. It is a static data point. Scenario A Scenario contains a set of assets and their relations. The assets are organized in a tree structure. Uses A Uses file contains a set of data points that are required by a use case. The data points are assigned to the assets in the scenario. Schema A JSON schema to describe the scenario and the information of data points required by multiple use cases. Asset Path A path to an asset or a data point in the scenario. The path is a list of asset types and names from the root asset to the target asset. For example, an asset with path \"DataHall.data_hall.Rack.rack_1.Server.server_1\" is the server named server_1 in a rack named rack_1 in a data hall named data_hall . And the data point with path \"DataHall.data_hall.Rack.rack_1.Server.server_1.Telemetry.temperature\" is the temperature telemetry of the server named server_1 in a rack named rack_1 in a data hall named data_hall . In some occasions, the Telemetry or Attribute before the data point name can be omitted. Type Path Asset Path without asset and data point types. For example, the type path of the asset with path \"DataHall.data_hall.Rack.rack_1.Server.server_1\" is \"DataHall.Rack.Server\" .","title":"Glossary"},{"location":"background/materials/","text":"Materials \u00b6 Slides \u00b6 DCWiz Data Workflow DCWiz Data Modules Related Projects: \u00b6 Utinni Karez Tiro-asset-library","title":"Materials"},{"location":"background/materials/#materials","text":"","title":"Materials"},{"location":"background/materials/#slides","text":"DCWiz Data Workflow DCWiz Data Modules","title":"Slides"},{"location":"background/materials/#related-projects","text":"Utinni Karez Tiro-asset-library","title":"Related Projects:"},{"location":"background/motivation/","text":"Why Tiro? \u00b6 The idea of Tiro starts from the attempts to standardize the protocol to collect data from data centers for DCWiz system. It has then been extended to a complete toolchain for DCWiz data workflow. Tiro tries to solve the following problems: How to share a standard data model for IoT system among different module developers and system integrators? The data users and data collector are usually different people. The data model should be able to be shared and understood by both of them. How to ease the data interface development? Tiro tries to extract the common pattern of the data interface development and automate the process. The data interface developer should only focus on data themselves and the toolchain can handle the rest. How to decouple the development between data collector and data user? The data collector should be able to develop the data interface without knowing the data user. The data user should be able to use the data without knowing the data collector. More specifically, There should be a simple way for data users to declare their data requirements and a simple way for data collectors to collect the requirements from different users and fulfill them. This requires the tools to collect and assemble the requirements from different users, the tools to convert the collected data to standard model, and the tools to validate the collected data. Also, the data user's development should not be blocked by the data collector's development. Since the requirements and shared data models are clearly defined, there should be some data mockers to generate mock data, based on the requirements and mode, for the data user to conduct their development first. How to properly organise the data collected from heterogeneous data sources in databases or storages, and how to provide the data to upper services and applications in a unified manner? The data collected from different data sources should be stored in a unified way. Luckily, we have already developed libraries Karez and Utinni for collecting and using data from heterogeneous sources. Tiro can be plugged into the architecture by providing proper Karez roles and Utinni DataPump . Therefore, Trio provides tools as shown in the following workflow, which covers the whole process of data collection, data validation, data exchanging, mock data generation and finally data usage. One example use scenario of Tiro is shown in the following figure, where the data collection development and data usage development can be parallelized. In this figure, above the dashed line is the data collection development, and below the dashed line is the data usage development. The data collector can develop the data collection interface without interfering the data user. Instead, they can validate the data independently. The data user can develop their data usage actually connecting to the real system. Instead, they can use the mock data to develop their data usage first. After both development works are done, the system integrator can simply replace the mock data generator will the real data collector, or replace the validator with the real application, the system should be ready to go. That standard data model that shared between both sides should be able to ensure there is no mismatch between the data collector and data user.","title":"Motivation"},{"location":"background/motivation/#why-tiro","text":"The idea of Tiro starts from the attempts to standardize the protocol to collect data from data centers for DCWiz system. It has then been extended to a complete toolchain for DCWiz data workflow. Tiro tries to solve the following problems: How to share a standard data model for IoT system among different module developers and system integrators? The data users and data collector are usually different people. The data model should be able to be shared and understood by both of them. How to ease the data interface development? Tiro tries to extract the common pattern of the data interface development and automate the process. The data interface developer should only focus on data themselves and the toolchain can handle the rest. How to decouple the development between data collector and data user? The data collector should be able to develop the data interface without knowing the data user. The data user should be able to use the data without knowing the data collector. More specifically, There should be a simple way for data users to declare their data requirements and a simple way for data collectors to collect the requirements from different users and fulfill them. This requires the tools to collect and assemble the requirements from different users, the tools to convert the collected data to standard model, and the tools to validate the collected data. Also, the data user's development should not be blocked by the data collector's development. Since the requirements and shared data models are clearly defined, there should be some data mockers to generate mock data, based on the requirements and mode, for the data user to conduct their development first. How to properly organise the data collected from heterogeneous data sources in databases or storages, and how to provide the data to upper services and applications in a unified manner? The data collected from different data sources should be stored in a unified way. Luckily, we have already developed libraries Karez and Utinni for collecting and using data from heterogeneous sources. Tiro can be plugged into the architecture by providing proper Karez roles and Utinni DataPump . Therefore, Trio provides tools as shown in the following workflow, which covers the whole process of data collection, data validation, data exchanging, mock data generation and finally data usage. One example use scenario of Tiro is shown in the following figure, where the data collection development and data usage development can be parallelized. In this figure, above the dashed line is the data collection development, and below the dashed line is the data usage development. The data collector can develop the data collection interface without interfering the data user. Instead, they can validate the data independently. The data user can develop their data usage actually connecting to the real system. Instead, they can use the mock data to develop their data usage first. After both development works are done, the system integrator can simply replace the mock data generator will the real data collector, or replace the validator with the real application, the system should be ready to go. That standard data model that shared between both sides should be able to ensure there is no mismatch between the data collector and data user.","title":"Why Tiro?"},{"location":"get_started/","text":"Get Started \u00b6 New to Tiro ? Start here! Installation : Install the library. Scenario : Create a scenario. Basic Tools : Learn the basic features of Tiro by using the built-in CLI tools. Karez Integration : Learn how to build a complete data collection workflow using Tiro and Karez Utinni Integration : Learn how to use Tiro and Utinni to use the collected data.","title":"Get Started"},{"location":"get_started/#get-started","text":"New to Tiro ? Start here! Installation : Install the library. Scenario : Create a scenario. Basic Tools : Learn the basic features of Tiro by using the built-in CLI tools. Karez Integration : Learn how to build a complete data collection workflow using Tiro and Karez Utinni Integration : Learn how to use Tiro and Utinni to use the collected data.","title":"Get Started"},{"location":"get_started/basic_tools/","text":"Basic Tools \u00b6 The basic features of Tiro are provided by the tiro command line tool. In last section, we have seen how to use tiro schema command to generate an example of data point collection that conforms to the scenario definition and use case requirements. In this section, we will introduce the other basic tools provided by tiro command. Schema and Example \u00b6 Generate Formal Schema \u00b6 One common task in DCWiz data interface development is to find the list of data point info that are required by multiple use cases. This information should include not only the list of data point names, but also the data type, unit, hierarchy structure, etc. Tiro provides the tool to gather data requirements from different services and applications and generate a formal JSON schema from the scenario. The schema can be used to describe the data requirement, to validate the data actually collected, or to generate mock data. To generate the schema, run the following command: $ tiro schema show scenario.yaml use-srv1.yaml use-srv2.yaml -o schema.json Here, we demonstrate the schema generation for two use cases. The generated schema.json file should be like: schema.json { \"title\" : \"Scenario\" , \"type\" : \"object\" , \"properties\" : { \"DataHall\" : { \"title\" : \"Datahall\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall\" } } }, \"required\" : [ \"DataHall\" ], \"definitions\" : { \"Scenario_DataHall_CRAC_ActivePower\" : { \"title\" : \"Scenario_DataHall_CRAC_ActivePower\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 1000 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_CRAC_SupplyTemperature\" : { \"title\" : \"Scenario_DataHall_CRAC_SupplyTemperature\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 50 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_CRAC_ReturnTemperature\" : { \"title\" : \"Scenario_DataHall_CRAC_ReturnTemperature\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 50 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_CRAC_Telemetry\" : { \"title\" : \"Scenario_DataHall_CRAC_Telemetry\" , \"type\" : \"object\" , \"properties\" : { \"ActivePower\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_ActivePower\" }, \"SupplyTemperature\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_SupplyTemperature\" }, \"ReturnTemperature\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_ReturnTemperature\" } }, \"required\" : [ \"ActivePower\" , \"SupplyTemperature\" , \"ReturnTemperature\" ] }, \"Scenario_DataHall_CRAC\" : { \"title\" : \"Scenario_DataHall_CRAC\" , \"type\" : \"object\" , \"properties\" : { \"Telemetry\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_Telemetry\" } }, \"required\" : [ \"Telemetry\" ] }, \"Scenario_DataHall_Rack_Server_FlowRate\" : { \"title\" : \"Scenario_DataHall_Rack_Server_FlowRate\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 1000 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_Rack_Server_Telemetry\" : { \"title\" : \"Scenario_DataHall_Rack_Server_Telemetry\" , \"type\" : \"object\" , \"properties\" : { \"FlowRate\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Server_FlowRate\" } }, \"required\" : [ \"FlowRate\" ] }, \"Scenario_DataHall_Rack_Server\" : { \"title\" : \"Scenario_DataHall_Rack_Server\" , \"type\" : \"object\" , \"properties\" : { \"Telemetry\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Server_Telemetry\" } }, \"required\" : [ \"Telemetry\" ] }, \"Scenario_DataHall_Rack_ActivePower\" : { \"title\" : \"Scenario_DataHall_Rack_ActivePower\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 1000 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_Rack_Telemetry\" : { \"title\" : \"Scenario_DataHall_Rack_Telemetry\" , \"type\" : \"object\" , \"properties\" : { \"ActivePower\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_ActivePower\" } }, \"required\" : [ \"ActivePower\" ] }, \"Scenario_DataHall_Rack\" : { \"title\" : \"Scenario_DataHall_Rack\" , \"type\" : \"object\" , \"properties\" : { \"Server\" : { \"title\" : \"Server\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Server\" } }, \"Telemetry\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Telemetry\" } }, \"required\" : [ \"Server\" , \"Telemetry\" ] }, \"Scenario_DataHall\" : { \"title\" : \"Scenario_DataHall\" , \"type\" : \"object\" , \"properties\" : { \"CRAC\" : { \"title\" : \"Crac\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC\" } }, \"Rack\" : { \"title\" : \"Rack\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack\" } } }, \"required\" : [ \"CRAC\" , \"Rack\" ] } } } Then, this file can be distributed to the client or system integrator to as a reference for required information. Or it can be shared among different teams using as a reference for the data model. This is a great way to share the data model with other systems and to ensure that the data model is consistent across the entire system. Generate Example \u00b6 The JSON schema is good for precise data model definition. However, it is not easy to read and understand. Therefore, one can also use the following command to generate an example JSON file for better understanding of the data model. $ tiro schema example -c scenario.yaml use-srv1.yaml use-srv2.yaml -o example.json Use Schema to Validate Data \u00b6 The JSON schema can be used to validate the data model. For example, here we let's generate an example file that considering only srv2's requirements. $ tiro schema example scenario.yaml use-srv2.yaml -o data_srv2.json Then, we can validate the data model using the JSON schema which considers both srv1 and srv2's requirements. $ tiro validate test scenario.yaml use-srv1.yaml use-srv2.yaml data_srv2.json Validation failed! ValidationError( model='Scenario', errors=[ { 'loc': ('DataHall', 'data_hall_0', 'CRAC', 'crac_0', 'Telemetry', 'ActivePower'), 'msg': 'field required', 'type': 'value_error.missing' }, { 'loc': ('DataHall', 'data_hall_0', 'Rack', 'rack_0', 'Telemetry'), 'msg': 'field required', 'type': 'value_error.missing' }, { 'loc': ('DataHall', 'data_hall_0', 'Rack', 'rack_1', 'Telemetry'), 'msg': 'field required', 'type': 'value_error.missing' } ] ) As we can see, the validation failed because the data points required by srv1 are missing. The tool can then explicitly tell us which data points are missing. Tip However, in most cases, the system integrator to provide such a json file for validate. It provides more convient tool to directly integrate the validation process into the data collection process. Please refer to Data Collection for more details. Mocking Data \u00b6 A big challenger in DCWiz projects is the lack of real data. Usually, the system can only be connected to the real environment after the system is deployed. However, the system needs to be tested and validated before deployment. In this case, mock data is required to simulate the real data. Tiro provides a tool to generate mock data that conforms to the scenario definition and data point requirements. Service \u00b6 Running the mocking service is as simple as running the following command: $ tiro mock serve scenario.yaml use-srv1.yaml use-srv2.yaml This command will start a server that listens on port 8000. The documentation of the server can be found at http://localhost:8000/docs . Endpoints \u00b6 Main endpoints of the mocking service include: GET /sample : generate a sample snapshot of the scenario $ curl -X 'GET' \\ 'http://127.0.0.1:8001/sample?change_attrs=false' \\ -H 'accept: application/json' { \"DataHall\": { \"data_hall_0\": { \"CRAC\": { \"crac_0\": { \"Telemetry\": { \"ActivePower\": { \"value\": 656.65, \"timestamp\": \"2022-09-26T23:45:50.938966\" }, \"SupplyTemperature\": { \"value\": 13.95, \"timestamp\": \"2022-09-26T23:45:50.895489\" }, \"ReturnTemperature\": { \"value\": 6.38, \"timestamp\": \"2022-09-26T23:45:50.508399\" } } } }, \"Rack\": { \"rack_0\": { \"Server\": { \"server_0\": { \"Telemetry\": { \"FlowRate\": { \"value\": 995.96, \"timestamp\": \"2022-09-26T23:45:50.491637\" } } }, \"server_1\": { \"Telemetry\": { \"FlowRate\": { \"value\": 933.36, \"timestamp\": \"2022-09-26T23:45:50.984197\" } } } }, \"Telemetry\": { \"ActivePower\": { \"value\": 460.68, \"timestamp\": \"2022-09-26T23:45:50.117701\" } } }, \"rack_1\": { \"Server\": { \"server_2\": { \"Telemetry\": { \"FlowRate\": { \"value\": 841.52, \"timestamp\": \"2022-09-26T23:45:50.017445\" } } }, \"server_3\": { \"Telemetry\": { \"FlowRate\": { \"value\": 115.27, \"timestamp\": \"2022-09-26T23:45:50.686074\" } } }, \"server_4\": { \"Telemetry\": { \"FlowRate\": { \"value\": 330.98, \"timestamp\": \"2022-09-26T23:45:50.208213\" } } } }, \"Telemetry\": { \"ActivePower\": { \"value\": 827.3, \"timestamp\": \"2022-09-26T23:45:50.432735\" } } } } } } } GET /points : list all data points in the scenario $ curl -X 'GET' \\ 'http://127.0.0.1:8001/points/' \\ -H 'accept: application/json' [ \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.ActivePower\", \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.SupplyTemperature\", \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.ReturnTemperature\", \"DataHall.data_hall_0.Rack.rack_0.Telemetry.ActivePower\", \"DataHall.data_hall_0.Rack.rack_0.Server.server_0.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_0.Server.server_1.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_1.Telemetry.ActivePower\", \"DataHall.data_hall_0.Rack.rack_1.Server.server_2.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_1.Server.server_3.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_1.Server.server_4.Telemetry.FlowRate\" ] GET /points/{point} : get current value of a data point $ curl -X 'GET' \\ 'http://127.0.0.1:8001/points/DataHall.data_hall_0.Rack.rack_0.Telemetry.ActivePower' \\ -H 'accept: application/json' { \"value\": 378.61, \"timestamp\": \"2022-09-26T23:48:55.812522\" } The mocking service can then be used as a generator of mock data for system development and testing. Validation \u00b6 In a large system, it would be common that the data collector and the data consumer are belongs to different system modules that developed by different teams. In this case, the data consumer may not be able to validate the completeness and correctness of the data collected from the data collector. To solve this problem, Tiro provides a data validation tool. In previous section , we have shown how to use the schema to validate a data collection. However, in real environment, the data points may not be collected in a batch and with a formal format. Instead, the discrete data points may be collected from different sources in a discrete manner. In this case, the previous validation method may not be applicable. Therefore, in this section we introduce a validation tool that can discretely receive data points and validate them against the schema. Service \u00b6 A validation service can be setup by running the following command: $ tiro validate serve scenario.yaml use-srv1.yaml use-srv2.yaml -p 8001 -r 60 The service works as an HTTP server that listens on port 8001. The documentation of the server can be found at http://localhost:8001/docs . Basically, the data collector needs to send data points to the service via the /points/{path} endpoint. And the service will periodically gather the data collected and validate them against the scenario definition. The validation result can be retrieved via the /result endpoint. By running the above command, the service will validate the data collected every 60 seconds. Now, let's try to validate the data collected from the mocking service. First, we need to start the mocking service (in another terminal): $ tiro mock serve scenario.yaml use-srv1.yaml use-srv2.yaml -p 8002 Then, we need to forward the data points from the mocking service to the validation service. There is a built-in tool can do this. Run the following command in another terminal: $ tiro mock push -d 127 .0.0.1:8001 -r 127 .0.0.1:8002 Forwarding DataHall.data_hall_0.CRAC.crac_0.Telemetry.ActivePower Forwarding DataHall.data_hall_0.CRAC.crac_0.Telemetry.ReturnTemperature Forwarding DataHall.data_hall_0.CRAC.crac_0.Telemetry.SupplyTemperature Forwarding DataHall.data_hall_0.Rack.rack_0.Telemetry.ActivePower Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_0.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_1.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_2.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_3.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_4.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_1.Telemetry.ActivePower Forwarding DataHall.data_hall_0.Rack.rack_1.Server.server_5.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_1.Server.server_6.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_1.Server.server_7.Telemetry.FlowRate We should see some messages like above showing that the data points are being forwarded. Results \u00b6 Now, we can check the validation result by visiting http://localhost:8001/result . $ curl -X 'GET' \\ 'http://127.0.0.1:8001/results' \\ -H 'accept: text/plain' Validation Period: 2022-09-27 00:33:19.492201 -- 2022-09-27 00:34:10.997081 Successful! Validation Period: 2022-09-27 00:32:19.259650 -- 2022-09-27 00:33:19.259650 Successful! Validation Period: 2022-09-27 00:24:34.889349 -- 2022-09-27 00:25:34.889349 Failed! 1 validation error for Scenario DataHall field required (type=value_error.missing)\u23ce Depending on the timing, you may see different results. The validation result is a list of validation results. Each validation result contains the start and end time of the validation period, and the validation result itself. The validation result can be either Successful or Failed . If the validation result is Failed , the validation result will also contain the validation errors, e.g. missing data points, data points with invalid value, etc.","title":"Basic Tools"},{"location":"get_started/basic_tools/#basic-tools","text":"The basic features of Tiro are provided by the tiro command line tool. In last section, we have seen how to use tiro schema command to generate an example of data point collection that conforms to the scenario definition and use case requirements. In this section, we will introduce the other basic tools provided by tiro command.","title":"Basic Tools"},{"location":"get_started/basic_tools/#schema-and-example","text":"","title":"Schema and Example"},{"location":"get_started/basic_tools/#generate-formal-schema","text":"One common task in DCWiz data interface development is to find the list of data point info that are required by multiple use cases. This information should include not only the list of data point names, but also the data type, unit, hierarchy structure, etc. Tiro provides the tool to gather data requirements from different services and applications and generate a formal JSON schema from the scenario. The schema can be used to describe the data requirement, to validate the data actually collected, or to generate mock data. To generate the schema, run the following command: $ tiro schema show scenario.yaml use-srv1.yaml use-srv2.yaml -o schema.json Here, we demonstrate the schema generation for two use cases. The generated schema.json file should be like: schema.json { \"title\" : \"Scenario\" , \"type\" : \"object\" , \"properties\" : { \"DataHall\" : { \"title\" : \"Datahall\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall\" } } }, \"required\" : [ \"DataHall\" ], \"definitions\" : { \"Scenario_DataHall_CRAC_ActivePower\" : { \"title\" : \"Scenario_DataHall_CRAC_ActivePower\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 1000 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_CRAC_SupplyTemperature\" : { \"title\" : \"Scenario_DataHall_CRAC_SupplyTemperature\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 50 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_CRAC_ReturnTemperature\" : { \"title\" : \"Scenario_DataHall_CRAC_ReturnTemperature\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 50 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_CRAC_Telemetry\" : { \"title\" : \"Scenario_DataHall_CRAC_Telemetry\" , \"type\" : \"object\" , \"properties\" : { \"ActivePower\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_ActivePower\" }, \"SupplyTemperature\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_SupplyTemperature\" }, \"ReturnTemperature\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_ReturnTemperature\" } }, \"required\" : [ \"ActivePower\" , \"SupplyTemperature\" , \"ReturnTemperature\" ] }, \"Scenario_DataHall_CRAC\" : { \"title\" : \"Scenario_DataHall_CRAC\" , \"type\" : \"object\" , \"properties\" : { \"Telemetry\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC_Telemetry\" } }, \"required\" : [ \"Telemetry\" ] }, \"Scenario_DataHall_Rack_Server_FlowRate\" : { \"title\" : \"Scenario_DataHall_Rack_Server_FlowRate\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 1000 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_Rack_Server_Telemetry\" : { \"title\" : \"Scenario_DataHall_Rack_Server_Telemetry\" , \"type\" : \"object\" , \"properties\" : { \"FlowRate\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Server_FlowRate\" } }, \"required\" : [ \"FlowRate\" ] }, \"Scenario_DataHall_Rack_Server\" : { \"title\" : \"Scenario_DataHall_Rack_Server\" , \"type\" : \"object\" , \"properties\" : { \"Telemetry\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Server_Telemetry\" } }, \"required\" : [ \"Telemetry\" ] }, \"Scenario_DataHall_Rack_ActivePower\" : { \"title\" : \"Scenario_DataHall_Rack_ActivePower\" , \"description\" : \"Base Pydantic Model to representing a data point\" , \"type\" : \"object\" , \"properties\" : { \"value\" : { \"minimum\" : 0 , \"maximum\" : 1000 , \"type\" : \"number\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" } }, \"required\" : [ \"value\" , \"timestamp\" ], \"unit\" : null }, \"Scenario_DataHall_Rack_Telemetry\" : { \"title\" : \"Scenario_DataHall_Rack_Telemetry\" , \"type\" : \"object\" , \"properties\" : { \"ActivePower\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_ActivePower\" } }, \"required\" : [ \"ActivePower\" ] }, \"Scenario_DataHall_Rack\" : { \"title\" : \"Scenario_DataHall_Rack\" , \"type\" : \"object\" , \"properties\" : { \"Server\" : { \"title\" : \"Server\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Server\" } }, \"Telemetry\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack_Telemetry\" } }, \"required\" : [ \"Server\" , \"Telemetry\" ] }, \"Scenario_DataHall\" : { \"title\" : \"Scenario_DataHall\" , \"type\" : \"object\" , \"properties\" : { \"CRAC\" : { \"title\" : \"Crac\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_CRAC\" } }, \"Rack\" : { \"title\" : \"Rack\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/Scenario_DataHall_Rack\" } } }, \"required\" : [ \"CRAC\" , \"Rack\" ] } } } Then, this file can be distributed to the client or system integrator to as a reference for required information. Or it can be shared among different teams using as a reference for the data model. This is a great way to share the data model with other systems and to ensure that the data model is consistent across the entire system.","title":"Generate Formal Schema"},{"location":"get_started/basic_tools/#generate-example","text":"The JSON schema is good for precise data model definition. However, it is not easy to read and understand. Therefore, one can also use the following command to generate an example JSON file for better understanding of the data model. $ tiro schema example -c scenario.yaml use-srv1.yaml use-srv2.yaml -o example.json","title":"Generate Example"},{"location":"get_started/basic_tools/#use-schema-to-validate-data","text":"The JSON schema can be used to validate the data model. For example, here we let's generate an example file that considering only srv2's requirements. $ tiro schema example scenario.yaml use-srv2.yaml -o data_srv2.json Then, we can validate the data model using the JSON schema which considers both srv1 and srv2's requirements. $ tiro validate test scenario.yaml use-srv1.yaml use-srv2.yaml data_srv2.json Validation failed! ValidationError( model='Scenario', errors=[ { 'loc': ('DataHall', 'data_hall_0', 'CRAC', 'crac_0', 'Telemetry', 'ActivePower'), 'msg': 'field required', 'type': 'value_error.missing' }, { 'loc': ('DataHall', 'data_hall_0', 'Rack', 'rack_0', 'Telemetry'), 'msg': 'field required', 'type': 'value_error.missing' }, { 'loc': ('DataHall', 'data_hall_0', 'Rack', 'rack_1', 'Telemetry'), 'msg': 'field required', 'type': 'value_error.missing' } ] ) As we can see, the validation failed because the data points required by srv1 are missing. The tool can then explicitly tell us which data points are missing. Tip However, in most cases, the system integrator to provide such a json file for validate. It provides more convient tool to directly integrate the validation process into the data collection process. Please refer to Data Collection for more details.","title":"Use Schema to Validate Data"},{"location":"get_started/basic_tools/#mocking-data","text":"A big challenger in DCWiz projects is the lack of real data. Usually, the system can only be connected to the real environment after the system is deployed. However, the system needs to be tested and validated before deployment. In this case, mock data is required to simulate the real data. Tiro provides a tool to generate mock data that conforms to the scenario definition and data point requirements.","title":"Mocking Data"},{"location":"get_started/basic_tools/#service","text":"Running the mocking service is as simple as running the following command: $ tiro mock serve scenario.yaml use-srv1.yaml use-srv2.yaml This command will start a server that listens on port 8000. The documentation of the server can be found at http://localhost:8000/docs .","title":"Service"},{"location":"get_started/basic_tools/#endpoints","text":"Main endpoints of the mocking service include: GET /sample : generate a sample snapshot of the scenario $ curl -X 'GET' \\ 'http://127.0.0.1:8001/sample?change_attrs=false' \\ -H 'accept: application/json' { \"DataHall\": { \"data_hall_0\": { \"CRAC\": { \"crac_0\": { \"Telemetry\": { \"ActivePower\": { \"value\": 656.65, \"timestamp\": \"2022-09-26T23:45:50.938966\" }, \"SupplyTemperature\": { \"value\": 13.95, \"timestamp\": \"2022-09-26T23:45:50.895489\" }, \"ReturnTemperature\": { \"value\": 6.38, \"timestamp\": \"2022-09-26T23:45:50.508399\" } } } }, \"Rack\": { \"rack_0\": { \"Server\": { \"server_0\": { \"Telemetry\": { \"FlowRate\": { \"value\": 995.96, \"timestamp\": \"2022-09-26T23:45:50.491637\" } } }, \"server_1\": { \"Telemetry\": { \"FlowRate\": { \"value\": 933.36, \"timestamp\": \"2022-09-26T23:45:50.984197\" } } } }, \"Telemetry\": { \"ActivePower\": { \"value\": 460.68, \"timestamp\": \"2022-09-26T23:45:50.117701\" } } }, \"rack_1\": { \"Server\": { \"server_2\": { \"Telemetry\": { \"FlowRate\": { \"value\": 841.52, \"timestamp\": \"2022-09-26T23:45:50.017445\" } } }, \"server_3\": { \"Telemetry\": { \"FlowRate\": { \"value\": 115.27, \"timestamp\": \"2022-09-26T23:45:50.686074\" } } }, \"server_4\": { \"Telemetry\": { \"FlowRate\": { \"value\": 330.98, \"timestamp\": \"2022-09-26T23:45:50.208213\" } } } }, \"Telemetry\": { \"ActivePower\": { \"value\": 827.3, \"timestamp\": \"2022-09-26T23:45:50.432735\" } } } } } } } GET /points : list all data points in the scenario $ curl -X 'GET' \\ 'http://127.0.0.1:8001/points/' \\ -H 'accept: application/json' [ \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.ActivePower\", \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.SupplyTemperature\", \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.ReturnTemperature\", \"DataHall.data_hall_0.Rack.rack_0.Telemetry.ActivePower\", \"DataHall.data_hall_0.Rack.rack_0.Server.server_0.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_0.Server.server_1.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_1.Telemetry.ActivePower\", \"DataHall.data_hall_0.Rack.rack_1.Server.server_2.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_1.Server.server_3.Telemetry.FlowRate\", \"DataHall.data_hall_0.Rack.rack_1.Server.server_4.Telemetry.FlowRate\" ] GET /points/{point} : get current value of a data point $ curl -X 'GET' \\ 'http://127.0.0.1:8001/points/DataHall.data_hall_0.Rack.rack_0.Telemetry.ActivePower' \\ -H 'accept: application/json' { \"value\": 378.61, \"timestamp\": \"2022-09-26T23:48:55.812522\" } The mocking service can then be used as a generator of mock data for system development and testing.","title":"Endpoints"},{"location":"get_started/basic_tools/#validation","text":"In a large system, it would be common that the data collector and the data consumer are belongs to different system modules that developed by different teams. In this case, the data consumer may not be able to validate the completeness and correctness of the data collected from the data collector. To solve this problem, Tiro provides a data validation tool. In previous section , we have shown how to use the schema to validate a data collection. However, in real environment, the data points may not be collected in a batch and with a formal format. Instead, the discrete data points may be collected from different sources in a discrete manner. In this case, the previous validation method may not be applicable. Therefore, in this section we introduce a validation tool that can discretely receive data points and validate them against the schema.","title":"Validation"},{"location":"get_started/basic_tools/#service_1","text":"A validation service can be setup by running the following command: $ tiro validate serve scenario.yaml use-srv1.yaml use-srv2.yaml -p 8001 -r 60 The service works as an HTTP server that listens on port 8001. The documentation of the server can be found at http://localhost:8001/docs . Basically, the data collector needs to send data points to the service via the /points/{path} endpoint. And the service will periodically gather the data collected and validate them against the scenario definition. The validation result can be retrieved via the /result endpoint. By running the above command, the service will validate the data collected every 60 seconds. Now, let's try to validate the data collected from the mocking service. First, we need to start the mocking service (in another terminal): $ tiro mock serve scenario.yaml use-srv1.yaml use-srv2.yaml -p 8002 Then, we need to forward the data points from the mocking service to the validation service. There is a built-in tool can do this. Run the following command in another terminal: $ tiro mock push -d 127 .0.0.1:8001 -r 127 .0.0.1:8002 Forwarding DataHall.data_hall_0.CRAC.crac_0.Telemetry.ActivePower Forwarding DataHall.data_hall_0.CRAC.crac_0.Telemetry.ReturnTemperature Forwarding DataHall.data_hall_0.CRAC.crac_0.Telemetry.SupplyTemperature Forwarding DataHall.data_hall_0.Rack.rack_0.Telemetry.ActivePower Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_0.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_1.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_2.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_3.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_0.Server.server_4.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_1.Telemetry.ActivePower Forwarding DataHall.data_hall_0.Rack.rack_1.Server.server_5.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_1.Server.server_6.Telemetry.FlowRate Forwarding DataHall.data_hall_0.Rack.rack_1.Server.server_7.Telemetry.FlowRate We should see some messages like above showing that the data points are being forwarded.","title":"Service"},{"location":"get_started/basic_tools/#results","text":"Now, we can check the validation result by visiting http://localhost:8001/result . $ curl -X 'GET' \\ 'http://127.0.0.1:8001/results' \\ -H 'accept: text/plain' Validation Period: 2022-09-27 00:33:19.492201 -- 2022-09-27 00:34:10.997081 Successful! Validation Period: 2022-09-27 00:32:19.259650 -- 2022-09-27 00:33:19.259650 Successful! Validation Period: 2022-09-27 00:24:34.889349 -- 2022-09-27 00:25:34.889349 Failed! 1 validation error for Scenario DataHall field required (type=value_error.missing)\u23ce Depending on the timing, you may see different results. The validation result is a list of validation results. Each validation result contains the start and end time of the validation period, and the validation result itself. The validation result can be either Successful or Failed . If the validation result is Failed , the validation result will also contain the validation errors, e.g. missing data points, data points with invalid value, etc.","title":"Results"},{"location":"get_started/installation/","text":"It is recommendation to use poetry to install Tiro . To install poetry, please refer to poetry installation guide . After poetry is installed, you can install Tiro by running the following command: $ poetry add git+https://github.com/cap-dcwiz/Tiro.git#main","title":"Installation"},{"location":"get_started/karez_integration/","text":"Karez Integration \u00b6 We have developed a data collection framework for DCWiz called Karez . Tiro provides deep integration with Karez. In this section, we will show how to use Tiro and Karaz together to build the complete data collection and processing workflow for a IoT system. Prerequisites \u00b6 Karez should already be declared as a dependency of Tiro . Thus, you do not need to install Karez separately. However, before we start, we still need to set up InfluxDB and ArangoDB . Tiro uses InfluxDB to store the collected telemetries for assets and uses ArangoDB to store the attributes and relationships of the assets. To simplify the setup process, we will use Docker Compose to start the two databases and the Karez components. Please refer to the Docker documentation to install Docker Compose. Let's create a file named docker-compose.yml with the following content: docker-compose.yml version : '3.9' services : influxdb : image : influxdb:alpine restart : always ports : - 8086:8086 environment : DOCKER_INFLUXDB_INIT_MODE : setup DOCKER_INFLUXDB_INIT_USERNAME : tiro DOCKER_INFLUXDB_INIT_PASSWORD : tiro-password DOCKER_INFLUXDB_INIT_ORG : tiro DOCKER_INFLUXDB_INIT_BUCKET : tiro DOCKER_INFLUXDB_INIT_ADMIN_TOKEN : tiro-token arangodb : image : arangodb/arangodb:3.8.6 platform : linux/amd64 restart : always ports : - 8529:8529 environment : ARANGO_ROOT_PASSWORD : tiro-password command : arangod --server.endpoint tcp://0.0.0.0:8529 nats-server : image : nats restart : always ports : - 4222:4222 - 8222:8222 Overview \u00b6 In the following subsections, we will set up following components: A mock server to generate mock data; A set of Karez roles to collect the mock data; A set of Karez roles to transform the mock data; A validator to validate the collected data; A set of Karez roles to persist the collected data in databases. The complete source code of the example can be found under the example/karez directory of the Tiro repository. Directory Setup \u00b6 Let's create a directory named karez and follow the following steps to set up the directory structure and initial files: Create a sub-directory name scenario to store the scenario files. After that, copy the scenario.yaml , use-srv1.yaml and use-srv2.yaml into this directory. Create subdirectory named karez and create four subdirectories named dispatcher , connector , converter and aggregator in the karez directory. We will use these directories to arrange Karez roles. Create a subdirectory named config to store various configuration files. Create a file named karez.yml in config directory. This file will be used to configure the Karez components. Copy the just created docker-compose.yml into the karez directory and run the following command to start the databases and Karez components: $ docker-compose up -d --build Mock Server \u00b6 Run the following command to create a mock server: $ tiro mock serve scenario/scenario.yaml scenario/use-srv1.yaml scenario/use-srv2.yaml Alternative, you can also add the following section to the docker-compose.yml file docker-compose.yml mock-server : image : ghcr.io/cap-dcwiz/tiro restart : always volumes : - ./scenario:/tiro/scenario - ./test-assets.py:/tiro/test-assets.p command : tiro mock serve scenario/scenario.yaml scenario/use-srv1.yaml scenario/use-srv2.yaml -h 0.0.0.0 After that, you need to run docker-compose up -d to start the mock server. Karez Collector \u00b6 Here, we will config Karez to collect data point from the mock server. Tiro provides a connector plugin ConnectorForMockServer and a dispatcher plugin DispatcherForMockServer to collect data from the mock server. We will use these two plugins to collect data from the mock server. First, let's add a tiro_mock.py file to the karez/connector directory with the following content: karez/connector/tiro_mock.py from tiro.plugins.karez import ConnectorForMockServer as Connector And add another tiro_mock.py file to the karez/dispatcher directory with the following content: karez/dispatcher/tiro_mock.py from tiro.plugins.karez import DispatcherForMockServer as Dispatcher After that, let's add the following section to the karez.yaml file: config/karez.yaml dispatchers : - type : tiro_mock connector : tiro_mock by : path batch_size : 10 interval : 10 mode : burst base_url : http://localhost:8000 connectors : - type : tiro_mock base_url : http://localhost:8000 by : path Run the following command to varify the configuration: $ karez test -c config/karez.yaml -p karez/ -d tiro_mock -n tiro_mock { 'path': 'DataHall.data_hall_0.CRAC.crac_0.Telemetry.ReturnTemperature', 'result': {'value': 38.15, 'timestamp': '2022-09-27T10:32:38.364195'}, '_karez': {'category': 'telemetry'} } If you see similar output, it means the configuration is correct. Otherwise, you may use the -d or -n option to varify the configuration of a specific dispatcher or connector. Karez Validator \u00b6 Tiro provides an aggregator plugin ValidationAggregator to validate the collected data. Now let's validate our collected data using this plugin. First, let's add a tiro_validate.py file to the karez/aggregator directory with the following content: karez/aggregator/tiro_validate.py from tiro.plugins.karez import ValidationAggregator as Aggregator After that, let's add the following section to the karez.yaml file: config/karez.yaml converters : - name : update_category_for_validator type : filter_and_update_meta key : category rename : telemetry : validation_data attribute : validation_data aggregators : - type : tiro_validate category : validation_data scenario : scenario/scenario.yaml uses : \"scenario/use-srv1.yaml,scenario/use-srv2.yaml\" retention : 10 log_file : validation.log Also, add the highlighted lines to the connectors - tiro_mock section: config/karez.yaml connectors : - type : tiro_mock base_url : http://localhost:8000 by : path converters : - update_category_for_validator With the above configuration, we have set up a validation aggregator listening on the validation_data category. The validation aggregator will validate the collected data against the scenario files scenario/scenario.yaml , scenario/use-srv1.yaml and scenario/use-srv2.yaml . The validation result will be stored in the validation.log file as well as printed to the console. The validation will be executed every 10 seconds, using the last 10 seconds of data. Now, we can deploy the Karez workflow by running the following command: $ karez deploy -c config/karez.yaml -p karez/ Wait for around 10 seconds, and you will see the validation result printed to the console: [KAREZ] Configurations: ['config/karez.yaml']. [KAREZ] NATS address: nats://localhost:4222. [KAREZ] Launched 1 Converters. [KAREZ] Launched 1 Connectors. [KAREZ] Launched 1 Dispatchers. [KAREZ] Launched 1 Aggregators. Collection size: 11 { \"start\": \"2022-09-27T11:39:24.255426\", \"end\": \"2022-09-27T11:39:34.255426\", \"valid\": true, \"exception\": null } The above result means the collected data is valid. If the collected data is invalid, the valid field will be set to false and the exception field will contain the error message. Meanwhile, the validation will be written to the validation.log file as well. Note Because currently we are using the mocking service, which always produce corrent data, the validation result will always be true . In a real environment, we can simply replace the dispatcher and connector with the real service, and the validation will be performed on the real data. This provides a way to validate the real data collection process. Data Persistence \u00b6 Now, let's add more Karez roles to persist the collected data. Tiro stores the data in both InfluxDB and ArangoDB, where the former is used to store the time series data and the latter is used to store static attributes and relationships. Preprocessing \u00b6 Before storing the data to databases, some preprocessing is needed to convert data format and add some metadata. Tiro provides a converter plugin TiroPreprocessConverter to do these. First, let's add a tiro_preprocess.py file to the karez/converter directory with the following content: kalrez/converter/tiro_preprocess.py from tiro.plugins.karez import TiroPreprocessConverter as Converter Then, add the following content in the converters section of the karez.yaml file: config/karez.yaml - type : tiro_preprocess tz_infos : SGT : Aisa/Singapore next : - influx_line_protocol - update_category_for_arangodb Also, we need to add the convertor into the processing pipeline by adding the following highlighted line to the connectors - tiro_mock section: config/karez.yaml connectors : - type : tiro_mock base_url : http://localhost:8000 by : path converter : - update_category_for_validator - tiro_preprocess We can run the following command to test the configuration: $ karez test -c config/karez.yaml -p karez/ -d tiro_mock -n tiro_mock -v tiro_preprocess { 'path': 'DataHall.CRAC.ReturnTemperature', 'asset_path': 'DataHall.data_hall_0.CRAC.crac_0', 'DataHall': 'data_hall_0', 'CRAC': 'crac_0', 'type': 'Telemetry', 'field': 'ReturnTemperature', 'value': 23.84, 'timestamp': 1664251882.666721 } InfluxDB \u00b6 We will use Telegraf to send the data to InfluxDB. First, let add the following service in the docker-compose.yml file: docker-compose.yml telegraf : image : telegraf restart : always volumes : - ./config/telegraf.conf:/etc/telegraf/telegraf.conf Then, let's add a telegraf.conf file with the following content in the config directory: config/telegraf.conf [[outputs.influxdb_v2]] urls = [ \"http://localhost:8086\" ] token = \"tiro-token\" organization = \"tiro\" bucket = \"tiro\" [[inputs.nats_consumer]] servers = [ \"nats://localhost:4222\" ] subjects = [ \"karez.telemetry.>\" ] queue_group = \"karez_telegraf\" data_format = \"influx\" The above configuration tells Telegraf to listen on the karez.telemetry.* subject and send the data to InfluxDB. After adding/modifying the configuration, you need to run docker-compose up -d to update the services. Now, let's add the following converter in the converters sections of karez.yaml : config/karez.yaml - type : influx_line_protocol measurement : tiro_telemetry field_name : field field_value : value This converter translates the preprocessed data into the InfluxDB line protocol format. The measurement field specifies the measurement name, and the field_name and field_value fields specify the columns that will be used as field and value in the line protocol. ArangoDB \u00b6 Tiro also provides an aggregator plugin ArangoAggregator to store the data to ArangoDB. First, let's add a arango_db.py file to the karez/aggregator directory with the following content: karez/aggregator/arango_db.py from tiro.plugins.karez import ArangoAggregator as Aggregator Then, to avoid confusing the data with those sending to InfluxDB and validator, we added another converter to mark the data as graph_data . config/karez.yaml - name : update_category_for_arangodb type : filter_and_update_meta key : category rename : telemetry : graph_data attribute : graph_data After that, we can add the following aggregator in the aggregators section of karez.yaml : config/karez.yaml - type : arango_db category : graph_data scenario : scenario/scenario.yaml uses : \"scenario/use-srv1.yaml,scenario/use-srv2.yaml\" db_name : tiro graph_name : scenario hosts : http://localhost:8529 auth : password : tiro-password Deployment \u00b6 Now, we can re-deploy the Karez service to start the data persistence process. $ karez deploy -c config/karez.yaml -p karez/ [KAREZ] Configurations: ['config/karez.yaml']. [KAREZ] NATS address: nats://localhost:4222. [KAREZ] Launched 4 Converters. [KAREZ] Launched 1 Connectors. [KAREZ] Launched 1 Dispatchers. [KAREZ] Launched 2 Aggregators. Note We can also include the karez workflow and mock service in docker-compose.yml to deploy them together. In that case, remember to change different hostnames corresponding to the services. Now, one should be able to access http://localhost:8086 to see the data in InfluxDB or access http://localhost:8529 to see the data in ArangoDB. In next section, we will show how to use Utinni to access the data easily.","title":"Karez Integration"},{"location":"get_started/karez_integration/#karez-integration","text":"We have developed a data collection framework for DCWiz called Karez . Tiro provides deep integration with Karez. In this section, we will show how to use Tiro and Karaz together to build the complete data collection and processing workflow for a IoT system.","title":"Karez Integration"},{"location":"get_started/karez_integration/#prerequisites","text":"Karez should already be declared as a dependency of Tiro . Thus, you do not need to install Karez separately. However, before we start, we still need to set up InfluxDB and ArangoDB . Tiro uses InfluxDB to store the collected telemetries for assets and uses ArangoDB to store the attributes and relationships of the assets. To simplify the setup process, we will use Docker Compose to start the two databases and the Karez components. Please refer to the Docker documentation to install Docker Compose. Let's create a file named docker-compose.yml with the following content: docker-compose.yml version : '3.9' services : influxdb : image : influxdb:alpine restart : always ports : - 8086:8086 environment : DOCKER_INFLUXDB_INIT_MODE : setup DOCKER_INFLUXDB_INIT_USERNAME : tiro DOCKER_INFLUXDB_INIT_PASSWORD : tiro-password DOCKER_INFLUXDB_INIT_ORG : tiro DOCKER_INFLUXDB_INIT_BUCKET : tiro DOCKER_INFLUXDB_INIT_ADMIN_TOKEN : tiro-token arangodb : image : arangodb/arangodb:3.8.6 platform : linux/amd64 restart : always ports : - 8529:8529 environment : ARANGO_ROOT_PASSWORD : tiro-password command : arangod --server.endpoint tcp://0.0.0.0:8529 nats-server : image : nats restart : always ports : - 4222:4222 - 8222:8222","title":"Prerequisites"},{"location":"get_started/karez_integration/#overview","text":"In the following subsections, we will set up following components: A mock server to generate mock data; A set of Karez roles to collect the mock data; A set of Karez roles to transform the mock data; A validator to validate the collected data; A set of Karez roles to persist the collected data in databases. The complete source code of the example can be found under the example/karez directory of the Tiro repository.","title":"Overview"},{"location":"get_started/karez_integration/#directory-setup","text":"Let's create a directory named karez and follow the following steps to set up the directory structure and initial files: Create a sub-directory name scenario to store the scenario files. After that, copy the scenario.yaml , use-srv1.yaml and use-srv2.yaml into this directory. Create subdirectory named karez and create four subdirectories named dispatcher , connector , converter and aggregator in the karez directory. We will use these directories to arrange Karez roles. Create a subdirectory named config to store various configuration files. Create a file named karez.yml in config directory. This file will be used to configure the Karez components. Copy the just created docker-compose.yml into the karez directory and run the following command to start the databases and Karez components: $ docker-compose up -d --build","title":"Directory Setup"},{"location":"get_started/karez_integration/#mock-server","text":"Run the following command to create a mock server: $ tiro mock serve scenario/scenario.yaml scenario/use-srv1.yaml scenario/use-srv2.yaml Alternative, you can also add the following section to the docker-compose.yml file docker-compose.yml mock-server : image : ghcr.io/cap-dcwiz/tiro restart : always volumes : - ./scenario:/tiro/scenario - ./test-assets.py:/tiro/test-assets.p command : tiro mock serve scenario/scenario.yaml scenario/use-srv1.yaml scenario/use-srv2.yaml -h 0.0.0.0 After that, you need to run docker-compose up -d to start the mock server.","title":"Mock Server"},{"location":"get_started/karez_integration/#karez-collector","text":"Here, we will config Karez to collect data point from the mock server. Tiro provides a connector plugin ConnectorForMockServer and a dispatcher plugin DispatcherForMockServer to collect data from the mock server. We will use these two plugins to collect data from the mock server. First, let's add a tiro_mock.py file to the karez/connector directory with the following content: karez/connector/tiro_mock.py from tiro.plugins.karez import ConnectorForMockServer as Connector And add another tiro_mock.py file to the karez/dispatcher directory with the following content: karez/dispatcher/tiro_mock.py from tiro.plugins.karez import DispatcherForMockServer as Dispatcher After that, let's add the following section to the karez.yaml file: config/karez.yaml dispatchers : - type : tiro_mock connector : tiro_mock by : path batch_size : 10 interval : 10 mode : burst base_url : http://localhost:8000 connectors : - type : tiro_mock base_url : http://localhost:8000 by : path Run the following command to varify the configuration: $ karez test -c config/karez.yaml -p karez/ -d tiro_mock -n tiro_mock { 'path': 'DataHall.data_hall_0.CRAC.crac_0.Telemetry.ReturnTemperature', 'result': {'value': 38.15, 'timestamp': '2022-09-27T10:32:38.364195'}, '_karez': {'category': 'telemetry'} } If you see similar output, it means the configuration is correct. Otherwise, you may use the -d or -n option to varify the configuration of a specific dispatcher or connector.","title":"Karez Collector"},{"location":"get_started/karez_integration/#karez-validator","text":"Tiro provides an aggregator plugin ValidationAggregator to validate the collected data. Now let's validate our collected data using this plugin. First, let's add a tiro_validate.py file to the karez/aggregator directory with the following content: karez/aggregator/tiro_validate.py from tiro.plugins.karez import ValidationAggregator as Aggregator After that, let's add the following section to the karez.yaml file: config/karez.yaml converters : - name : update_category_for_validator type : filter_and_update_meta key : category rename : telemetry : validation_data attribute : validation_data aggregators : - type : tiro_validate category : validation_data scenario : scenario/scenario.yaml uses : \"scenario/use-srv1.yaml,scenario/use-srv2.yaml\" retention : 10 log_file : validation.log Also, add the highlighted lines to the connectors - tiro_mock section: config/karez.yaml connectors : - type : tiro_mock base_url : http://localhost:8000 by : path converters : - update_category_for_validator With the above configuration, we have set up a validation aggregator listening on the validation_data category. The validation aggregator will validate the collected data against the scenario files scenario/scenario.yaml , scenario/use-srv1.yaml and scenario/use-srv2.yaml . The validation result will be stored in the validation.log file as well as printed to the console. The validation will be executed every 10 seconds, using the last 10 seconds of data. Now, we can deploy the Karez workflow by running the following command: $ karez deploy -c config/karez.yaml -p karez/ Wait for around 10 seconds, and you will see the validation result printed to the console: [KAREZ] Configurations: ['config/karez.yaml']. [KAREZ] NATS address: nats://localhost:4222. [KAREZ] Launched 1 Converters. [KAREZ] Launched 1 Connectors. [KAREZ] Launched 1 Dispatchers. [KAREZ] Launched 1 Aggregators. Collection size: 11 { \"start\": \"2022-09-27T11:39:24.255426\", \"end\": \"2022-09-27T11:39:34.255426\", \"valid\": true, \"exception\": null } The above result means the collected data is valid. If the collected data is invalid, the valid field will be set to false and the exception field will contain the error message. Meanwhile, the validation will be written to the validation.log file as well. Note Because currently we are using the mocking service, which always produce corrent data, the validation result will always be true . In a real environment, we can simply replace the dispatcher and connector with the real service, and the validation will be performed on the real data. This provides a way to validate the real data collection process.","title":"Karez Validator"},{"location":"get_started/karez_integration/#data-persistence","text":"Now, let's add more Karez roles to persist the collected data. Tiro stores the data in both InfluxDB and ArangoDB, where the former is used to store the time series data and the latter is used to store static attributes and relationships.","title":"Data Persistence"},{"location":"get_started/karez_integration/#preprocessing","text":"Before storing the data to databases, some preprocessing is needed to convert data format and add some metadata. Tiro provides a converter plugin TiroPreprocessConverter to do these. First, let's add a tiro_preprocess.py file to the karez/converter directory with the following content: kalrez/converter/tiro_preprocess.py from tiro.plugins.karez import TiroPreprocessConverter as Converter Then, add the following content in the converters section of the karez.yaml file: config/karez.yaml - type : tiro_preprocess tz_infos : SGT : Aisa/Singapore next : - influx_line_protocol - update_category_for_arangodb Also, we need to add the convertor into the processing pipeline by adding the following highlighted line to the connectors - tiro_mock section: config/karez.yaml connectors : - type : tiro_mock base_url : http://localhost:8000 by : path converter : - update_category_for_validator - tiro_preprocess We can run the following command to test the configuration: $ karez test -c config/karez.yaml -p karez/ -d tiro_mock -n tiro_mock -v tiro_preprocess { 'path': 'DataHall.CRAC.ReturnTemperature', 'asset_path': 'DataHall.data_hall_0.CRAC.crac_0', 'DataHall': 'data_hall_0', 'CRAC': 'crac_0', 'type': 'Telemetry', 'field': 'ReturnTemperature', 'value': 23.84, 'timestamp': 1664251882.666721 }","title":"Preprocessing"},{"location":"get_started/karez_integration/#influxdb","text":"We will use Telegraf to send the data to InfluxDB. First, let add the following service in the docker-compose.yml file: docker-compose.yml telegraf : image : telegraf restart : always volumes : - ./config/telegraf.conf:/etc/telegraf/telegraf.conf Then, let's add a telegraf.conf file with the following content in the config directory: config/telegraf.conf [[outputs.influxdb_v2]] urls = [ \"http://localhost:8086\" ] token = \"tiro-token\" organization = \"tiro\" bucket = \"tiro\" [[inputs.nats_consumer]] servers = [ \"nats://localhost:4222\" ] subjects = [ \"karez.telemetry.>\" ] queue_group = \"karez_telegraf\" data_format = \"influx\" The above configuration tells Telegraf to listen on the karez.telemetry.* subject and send the data to InfluxDB. After adding/modifying the configuration, you need to run docker-compose up -d to update the services. Now, let's add the following converter in the converters sections of karez.yaml : config/karez.yaml - type : influx_line_protocol measurement : tiro_telemetry field_name : field field_value : value This converter translates the preprocessed data into the InfluxDB line protocol format. The measurement field specifies the measurement name, and the field_name and field_value fields specify the columns that will be used as field and value in the line protocol.","title":"InfluxDB"},{"location":"get_started/karez_integration/#arangodb","text":"Tiro also provides an aggregator plugin ArangoAggregator to store the data to ArangoDB. First, let's add a arango_db.py file to the karez/aggregator directory with the following content: karez/aggregator/arango_db.py from tiro.plugins.karez import ArangoAggregator as Aggregator Then, to avoid confusing the data with those sending to InfluxDB and validator, we added another converter to mark the data as graph_data . config/karez.yaml - name : update_category_for_arangodb type : filter_and_update_meta key : category rename : telemetry : graph_data attribute : graph_data After that, we can add the following aggregator in the aggregators section of karez.yaml : config/karez.yaml - type : arango_db category : graph_data scenario : scenario/scenario.yaml uses : \"scenario/use-srv1.yaml,scenario/use-srv2.yaml\" db_name : tiro graph_name : scenario hosts : http://localhost:8529 auth : password : tiro-password","title":"ArangoDB"},{"location":"get_started/karez_integration/#deployment","text":"Now, we can re-deploy the Karez service to start the data persistence process. $ karez deploy -c config/karez.yaml -p karez/ [KAREZ] Configurations: ['config/karez.yaml']. [KAREZ] NATS address: nats://localhost:4222. [KAREZ] Launched 4 Converters. [KAREZ] Launched 1 Connectors. [KAREZ] Launched 1 Dispatchers. [KAREZ] Launched 2 Aggregators. Note We can also include the karez workflow and mock service in docker-compose.yml to deploy them together. In that case, remember to change different hostnames corresponding to the services. Now, one should be able to access http://localhost:8086 to see the data in InfluxDB or access http://localhost:8529 to see the data in ArangoDB. In next section, we will show how to use Utinni to access the data easily.","title":"Deployment"},{"location":"get_started/scenario/","text":"Create Scenario \u00b6 Scenario defines the hierarchy of the assets and the relationships between them. It is a tree structure where each node is an asset and each edge is a relationship between two assets. Scenario \u00b6 The first step to use Tiro is to define the scenario. The scenario is defined in a YAML file. Let's define a simple scenario scenario.yaml DataHall : $number : 1 $type : DataHall Rack : $number : 2 $type : Rack Server : $number : 2-5 $type : Server CRAC : $number : 1 $type : CRAC This file defines a scenario of data that contains two racks and 1 CRAC, and each rack contains 2-5 servers. The $asset_library_name is the name of the asset library that contains the asset types. The $number is the number of assets of the same type. The $type is the type of the asset, which should be a name of a Entity class defined in the asset library. Asset Library \u00b6 Let's define a simple asset library. This step is optional and should be able to reuse for different scenarios. tiro_assets.py from functools import partial from faker import Faker from pydantic import confloat from tiro.core import Entity from tiro.core.model import Telemetry default_faker = Faker () def RangedFloatTelemetry ( ge , le , unit = None , right_digits = 2 , faker = default_faker ) -> Telemetry : return Telemetry ( confloat ( ge = ge , le = le ), unit , faker = partial ( faker . pyfloat , right_digits = right_digits , min_value = ge , max_value = le ), ) class DataHall ( Entity ): \"\"\"Data hall asset.\"\"\" RoomTemperature = RangedFloatTelemetry ( - 50 , 50 ) ChilledWaterSupplyTemperature = RangedFloatTelemetry ( 0 , 1000 ) ChilledWaterReturnTemperature = RangedFloatTelemetry ( 0 , 1000 ) class Rack ( Entity ): ActivePower : RangedFloatTelemetry ( 0 , 1000 ) FrontTemperatures : RangedFloatTelemetry ( 0 , 60 ) BackTemperature : RangedFloatTelemetry ( 0 , 60 ) Temperature : RangedFloatTelemetry ( 0 , 60 ) class Server ( Entity ): ActivePower : RangedFloatTelemetry ( 0 , 1000 ) HeatLoad : RangedFloatTelemetry ( 0 , 1000 ) FlowRate : RangedFloatTelemetry ( 0 , 1000 ) CPUTemperature : RangedFloatTelemetry ( 0 , 150 ) class CRAC ( Entity ): ActivePower : RangedFloatTelemetry ( 0 , 1000 ) SupplyTemperature : RangedFloatTelemetry ( 0 , 50 ) ReturnTemperature : RangedFloatTelemetry ( 0 , 50 ) FanSpeed : RangedFloatTelemetry ( 0 , 1000 ) Tiro-asset-library For DCWiz project, we have pre-defined an asset library that contains common assets and data points in a data center. You can find it in the tiro-assets repository. After defining the asset library, let's declare the library name and path in the scenario file. scenario.yaml $asset_library_name : test_assets $asset_library_path : ./ DataHall : $number : 1 $type : DataHall Rack : $number : 2 $type : Rack Server : $number : 2-5 $type : Server CRAC : $number : 1 $type : CRAC The defined scenario can be shared among different applications or services. However, to actually use the scenario, each application or service still needs to declare the data points it wants in a \"use\" file, the data points declared must have been defined in the asset library. For example, one can declare active power, supply temperature, return temperature or fan speed for CRAC, or heat load, flow rate or CPU temperature for server. The use file is also a YAML file. Uses \u00b6 Now, let's define two use files for service \"srv1\" and \"srv2\", respectively. Assume that srv1 pays more attentions to the power consumption of the data center, while srv2 cares more about the cooling performance of the data center. use-srv1.yaml use-srv2.yaml - DataHall : - CRAC : - ActivePower - Rack : - ActivePower - DataHall : - Rack : - ChilledWaterSupplyTemperature - ChilledWaterReturnTemperature - Server : - FlowRate - CRAC : - SupplyTemperature - ReturnTemperature Test \u00b6 Now, we can test our scenario, asset library and use files. Let's try to generate an example snapshot of the scenario. $ tiro schema example -c scenario.yaml use-srv1.yaml use-srv2.yaml -o example.json We should see similar context in a example.json file. There should be generated data for all the data points declared in both uses files. example.json { \"DataHall\" : { \"data_hall_0\" : { \"CRAC\" : { \"crac_0\" : { \"ReturnTemperature\" : 26.47 , \"SupplyTemperature\" : 36.29 , \"ActivePower\" : 745.14 } }, \"Rack\" : { \"rack_0\" : { \"Server\" : { \"server_0\" : { \"FlowRate\" : 116.64 }, \"server_1\" : { \"FlowRate\" : 673.49 }, \"server_2\" : { \"FlowRate\" : 190.28 }, \"server_3\" : { \"FlowRate\" : 533.99 }, \"server_4\" : { \"FlowRate\" : 917.35 } }, \"ActivePower\" : 63.23 }, \"rack_1\" : { \"Server\" : { \"server_5\" : { \"FlowRate\" : 739.63 }, \"server_6\" : { \"FlowRate\" : 65.5 } }, \"ActivePower\" : 913.24 } } } } }","title":"Scenario"},{"location":"get_started/scenario/#create-scenario","text":"Scenario defines the hierarchy of the assets and the relationships between them. It is a tree structure where each node is an asset and each edge is a relationship between two assets.","title":"Create Scenario"},{"location":"get_started/scenario/#scenario","text":"The first step to use Tiro is to define the scenario. The scenario is defined in a YAML file. Let's define a simple scenario scenario.yaml DataHall : $number : 1 $type : DataHall Rack : $number : 2 $type : Rack Server : $number : 2-5 $type : Server CRAC : $number : 1 $type : CRAC This file defines a scenario of data that contains two racks and 1 CRAC, and each rack contains 2-5 servers. The $asset_library_name is the name of the asset library that contains the asset types. The $number is the number of assets of the same type. The $type is the type of the asset, which should be a name of a Entity class defined in the asset library.","title":"Scenario"},{"location":"get_started/scenario/#asset-library","text":"Let's define a simple asset library. This step is optional and should be able to reuse for different scenarios. tiro_assets.py from functools import partial from faker import Faker from pydantic import confloat from tiro.core import Entity from tiro.core.model import Telemetry default_faker = Faker () def RangedFloatTelemetry ( ge , le , unit = None , right_digits = 2 , faker = default_faker ) -> Telemetry : return Telemetry ( confloat ( ge = ge , le = le ), unit , faker = partial ( faker . pyfloat , right_digits = right_digits , min_value = ge , max_value = le ), ) class DataHall ( Entity ): \"\"\"Data hall asset.\"\"\" RoomTemperature = RangedFloatTelemetry ( - 50 , 50 ) ChilledWaterSupplyTemperature = RangedFloatTelemetry ( 0 , 1000 ) ChilledWaterReturnTemperature = RangedFloatTelemetry ( 0 , 1000 ) class Rack ( Entity ): ActivePower : RangedFloatTelemetry ( 0 , 1000 ) FrontTemperatures : RangedFloatTelemetry ( 0 , 60 ) BackTemperature : RangedFloatTelemetry ( 0 , 60 ) Temperature : RangedFloatTelemetry ( 0 , 60 ) class Server ( Entity ): ActivePower : RangedFloatTelemetry ( 0 , 1000 ) HeatLoad : RangedFloatTelemetry ( 0 , 1000 ) FlowRate : RangedFloatTelemetry ( 0 , 1000 ) CPUTemperature : RangedFloatTelemetry ( 0 , 150 ) class CRAC ( Entity ): ActivePower : RangedFloatTelemetry ( 0 , 1000 ) SupplyTemperature : RangedFloatTelemetry ( 0 , 50 ) ReturnTemperature : RangedFloatTelemetry ( 0 , 50 ) FanSpeed : RangedFloatTelemetry ( 0 , 1000 ) Tiro-asset-library For DCWiz project, we have pre-defined an asset library that contains common assets and data points in a data center. You can find it in the tiro-assets repository. After defining the asset library, let's declare the library name and path in the scenario file. scenario.yaml $asset_library_name : test_assets $asset_library_path : ./ DataHall : $number : 1 $type : DataHall Rack : $number : 2 $type : Rack Server : $number : 2-5 $type : Server CRAC : $number : 1 $type : CRAC The defined scenario can be shared among different applications or services. However, to actually use the scenario, each application or service still needs to declare the data points it wants in a \"use\" file, the data points declared must have been defined in the asset library. For example, one can declare active power, supply temperature, return temperature or fan speed for CRAC, or heat load, flow rate or CPU temperature for server. The use file is also a YAML file.","title":"Asset Library"},{"location":"get_started/scenario/#uses","text":"Now, let's define two use files for service \"srv1\" and \"srv2\", respectively. Assume that srv1 pays more attentions to the power consumption of the data center, while srv2 cares more about the cooling performance of the data center. use-srv1.yaml use-srv2.yaml - DataHall : - CRAC : - ActivePower - Rack : - ActivePower - DataHall : - Rack : - ChilledWaterSupplyTemperature - ChilledWaterReturnTemperature - Server : - FlowRate - CRAC : - SupplyTemperature - ReturnTemperature","title":"Uses"},{"location":"get_started/scenario/#test","text":"Now, we can test our scenario, asset library and use files. Let's try to generate an example snapshot of the scenario. $ tiro schema example -c scenario.yaml use-srv1.yaml use-srv2.yaml -o example.json We should see similar context in a example.json file. There should be generated data for all the data points declared in both uses files. example.json { \"DataHall\" : { \"data_hall_0\" : { \"CRAC\" : { \"crac_0\" : { \"ReturnTemperature\" : 26.47 , \"SupplyTemperature\" : 36.29 , \"ActivePower\" : 745.14 } }, \"Rack\" : { \"rack_0\" : { \"Server\" : { \"server_0\" : { \"FlowRate\" : 116.64 }, \"server_1\" : { \"FlowRate\" : 673.49 }, \"server_2\" : { \"FlowRate\" : 190.28 }, \"server_3\" : { \"FlowRate\" : 533.99 }, \"server_4\" : { \"FlowRate\" : 917.35 } }, \"ActivePower\" : 63.23 }, \"rack_1\" : { \"Server\" : { \"server_5\" : { \"FlowRate\" : 739.63 }, \"server_6\" : { \"FlowRate\" : 65.5 } }, \"ActivePower\" : 913.24 } } } } }","title":"Test"},{"location":"get_started/utinni_integration/","text":"Utinni Integration \u00b6 Utinni Utinni is in a private repository; thus, it is not included in the dependencies of this project. You will need to install it separately. There is a pre-compilled wheel file in /deps directory. You can install it by running the following command: $ pip install deps/utinni-*.whl Or, you can add Utinni into your project dependencies. For more information, please refer to https://github.com/cap-dcwiz/Utinni for more information. Or contact us directly. Attention Please also refer to Utinni for the concepts and usages of Utinni. This section will assume that you have already known the basic concepts of Utinni. Tiro is designed to be integrated with Utinni . An Utinni pump has been provided to access the data collected and managed by Tiro, following the unified data model defined by the scenario and uses files. In this section, we show how to use Utinni-based data tools to access the data collected by the workflow in last section . Data Pump \u00b6 Tiro provides a data pump that can extract tiro-collected data from InfluxDB and ArangoDB and organise them according to the scenario created. >>> from pathlib import Path >>> from datetime import timedelta >>> from utinni import Context >>> from tiro.core import Scenario >>> from tiro.plugins.utinni import TiroTSPump >>> >>> # Define the pump >>> scenario = Scenario . from_yaml ( Path ( \"scenario/scenario.yaml\" ), >>> Path ( \"scenario/use-srv1.yaml\" ), Path ( \"scenario/use-srv2.yaml\" )) >>> pump = TiroTSPump ( >>> scenario = scenario , >>> influxdb_url = \"http://localhost:8086\" , >>> influxdb_token = \"tiro-token\" , >>> influxdb_org = \"tiro\" , >>> arangodb_db = \"tiro\" , >>> arangodb_graph = \"scenario\" , >>> arangodb_hosts = \"http://localhost:8529\" , >>> arangodb_auth = dict ( password = \"tiro-password\" ) >>> ) . bind ( >>> influxdb_bucket = \"tiro\" , >>> influxdb_measurement = \"tiro_telemetry\" >>> ) >>> >>> # Create a context and add the pump to it >>> context = Context () >>> context . add_pump ( \"tiro\" , pump ) Then, we can define tiro_table s. There are two types of tiro tables. One is for time-series, or historian, data, and the other is for status data. Query Historian Data \u00b6 The historian table can be created by passing type=\"historian\" to the table constructor. >>> rack_power = context . tiro_table ( >>> \" %% Rack%ActivePower\" , >>> type = \"historian\" , >>> column = \"Rack\" , >>> )[ \"ActivePower\" ] Later, we can bind a time period to context (or pump, table) and access its real value. >>> context . bind ( >>> start =- timedelta ( hours = 1 ), >>> step = timedelta ( minutes = 5 ) >>> ) >>> rack_power . value rack_0 rack_1 2022-09-27 15:00:00+08:00 583.51 124.94 2022-09-27 15:05:00+08:00 240.44 670.64 2022-09-27 15:10:00+08:00 152.52 999.12 2022-09-27 15:15:00+08:00 866.90 969.20 2022-09-27 15:20:00+08:00 554.68 357.52 2022-09-27 15:25:00+08:00 38.95 897.61 2022-09-27 15:30:00+08:00 350.29 580.90 2022-09-27 15:35:00+08:00 284.33 167.15 2022-09-27 15:40:00+08:00 816.69 758.66 2022-09-27 15:45:00+08:00 260.58 320.97 2022-09-27 15:50:00+08:00 360.74 735.50 2022-09-27 15:55:00+08:00 908.48 221.26 2022-09-27 16:00:00+08:00 48.59 830.35 Or, we can create a table to directly calculate the mean flow rate of servers on each rack. >>> rack_flow = context . tiro_table ( >>> \" %% Server %F lowRate\" , >>> type = \"historian\" , >>> column = \"Rack\" , >>> asset_agg_fn = \"sum\" >>> )[ \"FlowRate\" ] >>> rack_flow . value rack_0 rack_1 2022-09-27 15:10:00+08:00 2314.28 1595.97 2022-09-27 15:15:00+08:00 1684.89 725.40 2022-09-27 15:20:00+08:00 1704.01 1146.60 2022-09-27 15:25:00+08:00 2469.59 888.24 2022-09-27 15:30:00+08:00 1484.55 1831.56 2022-09-27 15:35:00+08:00 1395.31 1706.43 2022-09-27 15:40:00+08:00 1489.70 1741.37 2022-09-27 15:45:00+08:00 1842.68 1008.99 2022-09-27 15:50:00+08:00 1593.35 1911.79 2022-09-27 15:55:00+08:00 1888.52 999.81 2022-09-27 16:00:00+08:00 1587.14 2006.45 2022-09-27 16:05:00+08:00 1752.09 1323.92 2022-09-27 16:10:00+08:00 1629.37 1596.03 Query Status Data \u00b6 The status table can be created by passing type=\"status\" to the table constructor. This table contains the latest values of data points instead of time-series data. For example, the following code creates a table to query the active power of all assets that have such data points >>> power_status = context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> value_only = True , >>> ) >>> power_status . value { 'DataHall': { 'data_hall_0': { 'CRAC': {'crac_0': {'Telemetry': {'ActivePower': 57.83}}}, 'Rack': { 'rack_0': {'Telemetry': {'ActivePower': 955.42}}, 'rack_1': {'Telemetry': {'ActivePower': 600.6}} } } } } Note that by default, the \"status\" table are free-from tables, whole values are dictionaries. If we want the data been converted to a pandas dataframe, we can pass as_dataframe=True to the table constructor. >>> power_table = context . tiro_table ( >>> \" %% FlowRate\" , >>> type = \"status\" , >>> as_dataframe = True , >>> include_tags = [ \"Rack\" , \"Server\" ], >>> value_only = True , >>> )[ \"FlowRate\" ] >>> power_table . value Rack Server value 0 rack_0 server_0 876.44 1 rack_0 server_1 443.27 2 rack_0 server_2 369.87 3 rack_1 server_3 535.70 4 rack_1 server_4 476.11 5 rack_1 server_5 160.21 Info Please also refer to the Tiro Query Guide for more explainations on how to query data from Tiro.","title":"Utinni Integration"},{"location":"get_started/utinni_integration/#utinni-integration","text":"Utinni Utinni is in a private repository; thus, it is not included in the dependencies of this project. You will need to install it separately. There is a pre-compilled wheel file in /deps directory. You can install it by running the following command: $ pip install deps/utinni-*.whl Or, you can add Utinni into your project dependencies. For more information, please refer to https://github.com/cap-dcwiz/Utinni for more information. Or contact us directly. Attention Please also refer to Utinni for the concepts and usages of Utinni. This section will assume that you have already known the basic concepts of Utinni. Tiro is designed to be integrated with Utinni . An Utinni pump has been provided to access the data collected and managed by Tiro, following the unified data model defined by the scenario and uses files. In this section, we show how to use Utinni-based data tools to access the data collected by the workflow in last section .","title":"Utinni Integration"},{"location":"get_started/utinni_integration/#data-pump","text":"Tiro provides a data pump that can extract tiro-collected data from InfluxDB and ArangoDB and organise them according to the scenario created. >>> from pathlib import Path >>> from datetime import timedelta >>> from utinni import Context >>> from tiro.core import Scenario >>> from tiro.plugins.utinni import TiroTSPump >>> >>> # Define the pump >>> scenario = Scenario . from_yaml ( Path ( \"scenario/scenario.yaml\" ), >>> Path ( \"scenario/use-srv1.yaml\" ), Path ( \"scenario/use-srv2.yaml\" )) >>> pump = TiroTSPump ( >>> scenario = scenario , >>> influxdb_url = \"http://localhost:8086\" , >>> influxdb_token = \"tiro-token\" , >>> influxdb_org = \"tiro\" , >>> arangodb_db = \"tiro\" , >>> arangodb_graph = \"scenario\" , >>> arangodb_hosts = \"http://localhost:8529\" , >>> arangodb_auth = dict ( password = \"tiro-password\" ) >>> ) . bind ( >>> influxdb_bucket = \"tiro\" , >>> influxdb_measurement = \"tiro_telemetry\" >>> ) >>> >>> # Create a context and add the pump to it >>> context = Context () >>> context . add_pump ( \"tiro\" , pump ) Then, we can define tiro_table s. There are two types of tiro tables. One is for time-series, or historian, data, and the other is for status data.","title":"Data Pump"},{"location":"get_started/utinni_integration/#query-historian-data","text":"The historian table can be created by passing type=\"historian\" to the table constructor. >>> rack_power = context . tiro_table ( >>> \" %% Rack%ActivePower\" , >>> type = \"historian\" , >>> column = \"Rack\" , >>> )[ \"ActivePower\" ] Later, we can bind a time period to context (or pump, table) and access its real value. >>> context . bind ( >>> start =- timedelta ( hours = 1 ), >>> step = timedelta ( minutes = 5 ) >>> ) >>> rack_power . value rack_0 rack_1 2022-09-27 15:00:00+08:00 583.51 124.94 2022-09-27 15:05:00+08:00 240.44 670.64 2022-09-27 15:10:00+08:00 152.52 999.12 2022-09-27 15:15:00+08:00 866.90 969.20 2022-09-27 15:20:00+08:00 554.68 357.52 2022-09-27 15:25:00+08:00 38.95 897.61 2022-09-27 15:30:00+08:00 350.29 580.90 2022-09-27 15:35:00+08:00 284.33 167.15 2022-09-27 15:40:00+08:00 816.69 758.66 2022-09-27 15:45:00+08:00 260.58 320.97 2022-09-27 15:50:00+08:00 360.74 735.50 2022-09-27 15:55:00+08:00 908.48 221.26 2022-09-27 16:00:00+08:00 48.59 830.35 Or, we can create a table to directly calculate the mean flow rate of servers on each rack. >>> rack_flow = context . tiro_table ( >>> \" %% Server %F lowRate\" , >>> type = \"historian\" , >>> column = \"Rack\" , >>> asset_agg_fn = \"sum\" >>> )[ \"FlowRate\" ] >>> rack_flow . value rack_0 rack_1 2022-09-27 15:10:00+08:00 2314.28 1595.97 2022-09-27 15:15:00+08:00 1684.89 725.40 2022-09-27 15:20:00+08:00 1704.01 1146.60 2022-09-27 15:25:00+08:00 2469.59 888.24 2022-09-27 15:30:00+08:00 1484.55 1831.56 2022-09-27 15:35:00+08:00 1395.31 1706.43 2022-09-27 15:40:00+08:00 1489.70 1741.37 2022-09-27 15:45:00+08:00 1842.68 1008.99 2022-09-27 15:50:00+08:00 1593.35 1911.79 2022-09-27 15:55:00+08:00 1888.52 999.81 2022-09-27 16:00:00+08:00 1587.14 2006.45 2022-09-27 16:05:00+08:00 1752.09 1323.92 2022-09-27 16:10:00+08:00 1629.37 1596.03","title":"Query Historian Data"},{"location":"get_started/utinni_integration/#query-status-data","text":"The status table can be created by passing type=\"status\" to the table constructor. This table contains the latest values of data points instead of time-series data. For example, the following code creates a table to query the active power of all assets that have such data points >>> power_status = context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> value_only = True , >>> ) >>> power_status . value { 'DataHall': { 'data_hall_0': { 'CRAC': {'crac_0': {'Telemetry': {'ActivePower': 57.83}}}, 'Rack': { 'rack_0': {'Telemetry': {'ActivePower': 955.42}}, 'rack_1': {'Telemetry': {'ActivePower': 600.6}} } } } } Note that by default, the \"status\" table are free-from tables, whole values are dictionaries. If we want the data been converted to a pandas dataframe, we can pass as_dataframe=True to the table constructor. >>> power_table = context . tiro_table ( >>> \" %% FlowRate\" , >>> type = \"status\" , >>> as_dataframe = True , >>> include_tags = [ \"Rack\" , \"Server\" ], >>> value_only = True , >>> )[ \"FlowRate\" ] >>> power_table . value Rack Server value 0 rack_0 server_0 876.44 1 rack_0 server_1 443.27 2 rack_0 server_2 369.87 3 rack_1 server_3 535.70 4 rack_1 server_4 476.11 5 rack_1 server_5 160.21 Info Please also refer to the Tiro Query Guide for more explainations on how to query data from Tiro.","title":"Query Status Data"},{"location":"reference/","text":"core special \u00b6 mock \u00b6 MockedEntity ( MockedItem ) \u00b6 Mock data generator for an entity class Source code in tiro/core/mock.py class MockedEntity ( MockedItem ): \"\"\"Mock data generator for an entity class\"\"\" def __init__ ( self , entity_type : Optional [ str ], * args , uuid = None , ** kwargs ): super ( MockedEntity , self ) . __init__ ( * args , ** kwargs ) self . children : dict [ str , dict [ str , MockedEntity ]] = {} # self.uuid: Optional[str] = uuid or str(uuid1()) self . uuid : Optional [ str ] = uuid or self . gen_uuid () self . entity_type : str = entity_type self . _initialised : bool = False self . _path : Optional [ str ] = None for dp_type in DataPointInfo . SUB_CLASSES : setattr ( self , camel_to_snake ( dp_type . __name__ ), {}) ref_dps = self . reference . get_data_points ( self . path ) if ref_dps is not None : ref_dps = set ( ref_dps ) for k in self . prototype . data_points (): v = self . prototype . data_point_info [ k ] dps = getattr ( self , camel_to_snake ( v . __class__ . __name__ )) k = camel_to_snake ( k ) if ref_dps is None or k in ref_dps and k not in dps : dps [ k ] = MockedDataPoint ( prototype = v , name = k , parent = self , reference = self . reference ) def generate ( self , regenerate : bool , include_data_points : bool , change_attrs : bool , use_default : bool , ) -> \"MockedEntity\" : if not self . _initialised or regenerate : self . children = {} for k , v in self . prototype . children . items (): _children = {} entity_type = camel_to_snake ( k ) prototype = self . prototype . child_info [ k ] number = prototype . number_faker () if prototype . ids and number > len ( prototype . ids ): logging . warning ( f \"Faking number ( { number } )is greater the length of predefined IDs ( { len ( prototype . ids ) } .\" f \"Only { len ( prototype . ids ) } instances will be generated.\" ) child_path = ( f \" { self . path }{ PATH_SEP }{ entity_type } \" if self . path else entity_type ) uuids = self . reference . get_children ( child_path ) if uuids is None : if prototype . ids : uuids = prototype . ids else : uuids = [ None for _ in range ( number )] else : uuids = list ( uuids . keys ()) number = len ( uuids ) for uuid in uuids [: number ]: entity = MockedEntity ( entity_type = entity_type , prototype = v , parent = self , reference = self . reference , uuid = uuid , ) _children [ entity . uuid ] = entity . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) self . children [ entity_type ] = _children self . _initialised = True if include_data_points : self . _generate_data_points ( change_attrs = change_attrs or regenerate , use_default = use_default ) return self def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res def _generate_data_points ( self , use_default , ** kwargs ) -> None : for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) v : MockedDataPoint for v in dps . values (): v . generate ( use_default = use_default , ** kwargs ) def search_entity ( self , uuid : str ) -> Optional [ \"MockedEntity\" ]: if uuid == self . uuid : return self else : for v in self . children . values (): for c in v . values (): entity = c . search_entity ( uuid ) if entity : return entity @property def path ( self ) -> str : if self . _path is None : if not self . parent : self . _path = \"\" else : self . _path = concat_path ( self . parent . path , self . entity_type , self . uuid ) return self . _path def list_entities ( self ) -> Generator [ tuple [ str , \"MockedEntity\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) yield self . path , self for v in self . children . values (): for c in v . values (): yield from c . list_entities () def list_data_points ( self , skip_default ) -> Generator [ tuple [ str , \"MockedDataPoint\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) for k , v in dps . items (): if not skip_default or v . prototype . default is None : yield concat_path ( self . path , dp_type_name , k ), v for v in self . children . values (): for c in v . values (): yield from c . list_data_points ( skip_default ) def gen_data_point ( self , dp_name : str , change_attrs = False , use_default = True ) -> dict : self . generate ( regenerate = False , include_data_points = False , change_attrs = change_attrs , use_default = use_default , ) dp = None for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) if dp_name in dps : dp = dps [ dp_name ] break if dp is None : raise KeyError ( f \"Cannot find data points { dp_name } in { self . prototype . unique_name } \" ) return dp . generate ( change_attrs = change_attrs , use_default = use_default ) . dict () def get_child ( self , path : str ) -> \"MockedEntity\" : if path : c_type , _ , path = path . partition ( PATH_SEP ) c_uuid , _ , path = path . partition ( PATH_SEP ) return self . children [ c_type ][ c_uuid ] . get_child ( path ) else : return self dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) \u00b6 Generate a complete tree starting from current entity Source code in tiro/core/mock.py def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res Mocker \u00b6 Source code in tiro/core/mock.py class Mocker : def __init__ ( self , entity : Optional [ Entity ] = None , reference : Optional [ Path | dict ] = None ): if isinstance ( reference , Path ): reference = yaml . safe_load ( reference . open ()) self . entity : MockedEntity = MockedEntity ( entity_type = None , prototype = entity , reference = Reference ( reference ) ) self . entity_cache : Optional [ dict [ str , MockedEntity ]] = None def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) def gen_data_point ( self , path : str , change_attr : bool = False , use_default : bool = True ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path , _ , dp_name = path . rpartition ( PATH_SEP ) path , _ , _ = path . rpartition ( PATH_SEP ) return self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) def gen_value_by_uuid ( self , uuid : str , change_attr : bool = False , use_default : bool = True , value_only : bool = False , ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path = self . entity . reference . search_by_uuid ( uuid ) if path is not None : path , _ , dp_name = path . rpartition ( PATH_SEP ) dp = self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) if value_only : return dp [ \"value\" ] else : return dp else : raise KeyError def list_entities ( self ) -> list [ str ]: return [ k for k , _ in self . entity . list_entities ()] def list_data_points ( self , skip_default = True ) -> list [ str ]: return [ k for k , _ in self . entity . list_data_points ( skip_default = skip_default )] def list_uuids ( self ) -> list [ str ]: return self . entity . reference . list_uuids () dict ( self , regenerate = False , include_data_points = True , change_attrs = False , skip_default = True , use_default = True ) \u00b6 Generate a complete dictionary for the tree starting from the given entity. Source code in tiro/core/mock.py def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) json ( self , regenerate = False , include_data_points = True , change_attrs = False , skip_default = True , use_default = True , ** kwargs ) \u00b6 Generate a complete dictionary for the tree starting from the entity and return the coded json string. Source code in tiro/core/mock.py def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) model \u00b6 Alias \u00b6 Alias for a data point info Source code in tiro/core/model.py class Alias : \"\"\"Alias for a data point info\"\"\" def __init__ ( self , origin : str ): self . origin = origin Attribute ( DataPointInfo ) \u00b6 Data point that seldom changes Source code in tiro/core/model.py class Attribute ( DataPointInfo ): \"\"\"Data point that seldom changes\"\"\" pass DataPoint ( GenericModel , Generic ) pydantic-model \u00b6 Base Pydantic Model to representing a data point Source code in tiro/core/model.py class DataPoint ( GenericModel , Generic [ DPT ]): \"\"\"Base Pydantic Model to representing a data point\"\"\" value : DPT timestamp : datetime _unit : Optional [ str ] = None class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"DataPoint\" ]) -> None : schema [ \"unit\" ] = model . _unit for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) DataPointInfo \u00b6 Source code in tiro/core/model.py class DataPointInfo : SUB_CLASSES = set () SUB_CLASS_NAMES = set () def __init__ ( self , type : Any , unit : Optional [ str ] = None , faker : Optional [ Callable ] = None , time_var : Optional [ timedelta ] = timedelta ( seconds = 0 ), default = None , ): self . type = type self . unit = unit self . faker = faker self . time_var = time_var self . default = default def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) def default_object ( self , cls : Optional [ Type [ DataPoint ]] = None ) -> Optional [ DataPoint | dict ]: if self . default is None : return None else : current_time = datetime . utcnow () . isoformat () if cls : return cls ( value = self . default , timestamp = current_time , _unit = self . unit ) else : return dict ( value = self . default , timestamp = current_time , unit = self . unit ) __init_subclass__ ( ** kwargs ) classmethod special \u00b6 This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) Entity \u00b6 Source code in tiro/core/model.py class Entity : class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"Entity\" ]) -> None : schema . pop ( \"title\" , None ) for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) data_point_info : dict [ str , DataPointInfo ] = {} def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} def __init__ ( self , parent : Optional [ \"Entity\" ] = None , ): self . children : dict [ str , Entity ] = {} self . parent : Optional [ Entity ] = parent self . _used_data_points : set [ str ] = set () self . uses = set () @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) @property def name ( self ) -> str : return self . __class__ . __name__ @property def unique_name ( self ) -> str : if self . parent and \"_\" not in self . name : unique_name = f \" { self . parent . unique_name } _ { self . __class__ . __name__ } \" else : unique_name = self . name return unique_name def _parse_use_yaml ( self , d , prefix = \"\" ) -> Generator [ str , None , None ]: if isinstance ( d , list ): for item in d : yield from self . _parse_use_yaml ( item , prefix = prefix ) elif isinstance ( d , dict ): for k , v in d . items (): yield from self . _parse_use_yaml ( v , prefix = concat_path ( prefix , k )) elif isinstance ( d , str ): yield concat_path ( prefix , d ) def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self @classmethod def update_defaults ( cls , ** defaults ): for key , value in defaults . items (): cls . data_point_info [ key ] . default = value def _create_date_points_model ( self , dp_category : Type [ DataPoint ], hide_dp_values : bool ) -> tuple [ Optional [ Type [ BaseModel ]], bool ]: \"\"\"Dynamically generate Pydantic model for all data points in the entity.\"\"\" info = { k : v for k , v in self . data_point_info . items () if isinstance ( v , dp_category ) and k in self . _used_data_points } if not info : return None , False sub_models : dict [ str , tuple [ type , Any ]] = {} is_optional = True for dp_name , dp_info in info . items (): dp_model_name = f \" { self . name } _ { dp_name } \" if hide_dp_values : dp_type = dict else : model_cache = self . __class__ . cached_data_point_model if dp_model_name not in model_cache : model_cache [ dp_model_name ] = type ( f \" { self . name } _ { dp_name } \" , ( DataPoint [ dp_info . type ],), dict ( _unit = dp_info . unit ), ) dp_type = model_cache [ dp_model_name ] if dp_info . default is not None and not hide_dp_values : sub_models [ camel_to_snake ( dp_name )] = Optional [ dp_type ], dp_info . default_object ( dp_type ) else : is_optional = False sub_models [ camel_to_snake ( dp_name )] = dp_type , ... return ( create_model ( f \" { self . unique_name } _ { dp_category . __name__ } \" , ** sub_models ), is_optional , ) def _create_entities_model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ dict [ str , tuple [ type , Any ]], bool ]: \"\"\"Dynamically generate Pydantic model for the entity type.\"\"\" fields = {} is_optional = True for name , ins in self . children . items (): sub_model_list_values , sub_is_optional = ins . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) child_info = self . child_info [ name ] if child_info . ids : sub_model_list = dict [ Literal [ tuple ( child_info . ids )], sub_model_list_values ] else : sub_model_list = dict [ str , sub_model_list_values ] if sub_is_optional and not require_all_children : fields [ camel_to_snake ( name )] = Optional [ sub_model_list ], {} else : is_optional = False fields [ camel_to_snake ( name )] = sub_model_list , ... return fields , is_optional def _model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ Type [ BaseModel ], bool ]: \"\"\"Generate a complete Pydantic model for a model tree staring from current entity.\"\"\" fields , is_optional = self . _create_entities_model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) for dp_category in DataPointInfo . SUB_CLASSES : dp_model , sub_is_optional = self . _create_date_points_model ( dp_category , hide_dp_values = hide_dp_values ) if dp_model : if sub_is_optional : fields |= { camel_to_snake ( dp_category . __name__ ): ( Optional [ dp_model ], {}) } else : fields |= { camel_to_snake ( dp_category . __name__ ): ( dp_model , ... )} is_optional &= sub_is_optional return create_model ( self . unique_name , config = self . Config , ** fields ), is_optional def model ( self , hide_dp_values : bool = False , require_all_children : bool = True ): return self . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children )[ 0 ] def __getattr__ ( self , item : str ) -> RequireHelper : return RequireHelper ( item , self ) def data_points ( self , used_only : bool = True ) -> list [ str ]: if used_only : return list ( self . _used_data_points ) else : return list ( self . data_point_info . keys ()) def query_data_point_info ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . query_data_point_info ( path [ 1 :]) elif path [ 0 ] in self . data_point_info : return self . data_point_info [ path [ 0 ]] def default_values ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . default_values ( path [ 1 :]) else : return {} else : res = {} for k in self . _used_data_points : dp_info = self . data_point_info [ k ] if dp_info . default : res [ k ] = dp_info . default_object () | dict ( type = dp_info . __class__ . __name__ ) return res @classmethod def use_selection_model ( cls , name_prefix = \"\" ): telemetry_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Telemetry ) ) attribute_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Attribute ) ) name = f \" { name_prefix } . { cls . __name__ } \" . strip ( \".\" ) model_kwargs = dict () if telemetry_names : model_kwargs [ \"telemetry\" ] = Optional [ list [ Literal [ telemetry_names ]]], Field ( None , unique_items = True ) if attribute_names : model_kwargs [ \"attribute\" ] = Optional [ list [ Literal [ attribute_names ]]], Field ( None , unique_items = True ) for k , v in cls . child_info . items (): model_kwargs [ k ] = Optional [ v . cls . use_selection_model ( name_prefix = name ) ], Field ( None ) return create_model ( name , ** model_kwargs ) def all_required_paths ( self , prefix = None ) -> Generator [ str , None , None ]: prefix = prefix or \"\" for name , child in self . children . items (): yield from child . all_required_paths ( concat_path ( prefix , name )) for dp in self . _used_data_points : yield concat_path ( prefix , dp ) def all_required_edges ( self , self_name = None ) -> Generator [ tuple [ str , str , str ], None , None ]: self_name = self_name or self . name for child_name , child in self . children . items (): yield \"is_parent_of\" , self_name , child_name yield from child . all_required_edges ( child_name ) for dp_name in self . _used_data_points : yield \"has_data_point\" , self_name , self . data_point_info [ dp_name ] . __class__ . __name__ def match_data_points ( self , pattern_or_uses : str | dict | Path , paths : list [ str ] = None ) -> Iterable [ str ]: if isinstance ( pattern_or_uses , str ): return filter ( partial ( self . path_match , pattern_or_uses ), paths or self . all_required_paths (), ) else : valid_paths = set ( decouple_uses ( pattern_or_uses )) return filter ( valid_paths . __contains__ , paths or self . all_required_paths ()) @staticmethod def path_match ( pattern : str , path : str ) -> bool : if pattern is not None : return bool ( re . fullmatch ( format_regex ( pattern ), path )) else : return True @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) def to_compact ( self , data ): res = { k : { c : self . children [ k ] . to_compact ( cv ) for c , cv in v . items ()} for k , v in data . items () if k in self . children } for dp_type in DataPointInfo . SUB_CLASS_NAMES : if dp_type in data : res |= { k : v [ \"value\" ] for k , v in data [ dp_type ] . items ()} return res __init_subclass__ ( ** kwargs ) classmethod special \u00b6 This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} create ( name , * entities , * , base_classes = None , asset_library_path = None , asset_library_name = 'tiro.assets' , ** entity_dict ) classmethod \u00b6 Dynamically create the entity class according to the entity name defined in an asset library Source code in tiro/core/model.py @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) create_from_define_string ( name , defs , prefix = '' , asset_library_path = None , asset_library_name = 'tiro.assets' ) classmethod \u00b6 Dynamically create the entity from a dictionary containing model infos Source code in tiro/core/model.py @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) many ( * args , ** kwargs ) classmethod \u00b6 Return a list of the entities with the same type Source code in tiro/core/model.py @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) requires ( self , * paths , * , yaml = None ) \u00b6 Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. Source code in tiro/core/model.py def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self EntityList \u00b6 Holding the information of a list of entity with the same type Source code in tiro/core/model.py class EntityList : \"\"\"Holding the information of a list of entity with the same type\"\"\" def __init__ ( self , cls : Type [ \"Entity\" ], faking_number : Optional [ Callable | int ] = None , ids : Optional [ list [ str ]] = None , ): self . cls = cls self . ids = ids if self . ids is not None : if faking_number is None : faking_number = len ( self . ids ) elif isinstance ( faking_number , int ) and faking_number > len ( self . ids ): raise RuntimeError ( \"When ids is provided, faking_number must be less than the length of ids.\" ) if isinstance ( faking_number , int ): self . number_faker = lambda : faking_number else : self . number_faker = faking_number def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent ) new_entity ( self , parent = None ) \u00b6 Generate an entity instance Source code in tiro/core/model.py def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent ) RequireHelper \u00b6 Helper class for requiring children or data points Source code in tiro/core/model.py class RequireHelper : \"\"\"Helper class for requiring children or data points\"\"\" def __init__ ( self , component : str , parent : Union [ \"RequireHelper\" , \"Entity\" ]): self . component = component self . parent = parent @property def path ( self ) -> str : if isinstance ( self . parent , RequireHelper ): return f \" { self . parent . path } . { self . component } \" else : return self . component @property def origin ( self ) -> Union [ \"RequireHelper\" , \"Entity\" ]: if isinstance ( self . parent , RequireHelper ): return self . parent . origin else : return self . parent def __getattr__ ( self , item ): return RequireHelper ( item , self ) def use ( self ) -> \"Entity\" : return self . origin . requires ( self . path ) Telemetry ( DataPointInfo ) \u00b6 Data point that dynamically changes. Source code in tiro/core/model.py class Telemetry ( DataPointInfo ): \"\"\"Data point that dynamically changes.\"\"\" pass scenario \u00b6 Scenario \u00b6 Source code in tiro/core/scenario.py class Scenario : def __init__ ( self , * entities : Entity | Type [ Entity ], asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** kw_entities : dict [ str , Entity | Type [ Entity ]], ): self . root : Entity = Entity . create ( \"Scenario\" , * entities , base_classes = None , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** kw_entities , )() @classmethod def from_yaml ( cls , scenario_data : Path | str , * uses : Path | str ): if isinstance ( scenario_data , Path ): scenario_data = scenario_data . open () . read () defs = safe_load ( scenario_data ) asset_library_path = defs . get ( f \" { YAML_META_CHAR } asset_library_path\" , None ) asset_library_name = defs . get ( f \" { YAML_META_CHAR } asset_library_name\" , \"tiro.assets\" ) ins = cls ( ** { k : Entity . create_from_define_string ( k , v , prefix = \"Scenario\" , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } ) for use in uses : if isinstance ( use , Path ): use = use . open () . read () ins . requires ( yaml = use ) return ins def __getattr__ ( self , key ): return getattr ( self . root , key ) def mocker ( self , * args , ** kwargs ): return Mocker ( self . root , * args , ** kwargs ) def validator ( self , * args , ** kwargs ): return Validator ( self . root , * args , ** kwargs ) @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) @staticmethod def data_point_path_to_path ( path : str | list [ str ]) -> str : path = split_path ( path ) return f \" { PATH_SEP . join ( path [ i ] for i in range ( 0 , len ( path ) - 2 , 2 )) }{ PATH_SEP }{ path [ - 1 ] } \" @classmethod def data_point_path_to_tags ( cls , path : str | list [ str ], tags = None ) -> dict : tags = tags or dict ( path = cls . data_point_path_to_path ( path ), asset_path = path ) path = split_path ( path ) component = path . pop ( 0 ) if component in DataPointInfo . SUB_CLASS_NAMES : tags |= dict ( type = component , field = path [ - 1 ]) else : uuid = path . pop ( 0 ) tags |= { component : uuid } cls . data_point_path_to_tags ( path , tags ) return tags def guess_missing_paths ( self , existing_paths : Optional [ list [ str ]] = None , pattern_or_uses : Optional [ str | dict | Path ] = None , ): validator = self . validator ( validate_path_only = True , require_all_children = False ) if existing_paths : for path in existing_paths : validator . collect ( path , value = {}) res = validator . validate () if isinstance ( pattern_or_uses , dict ) or isinstance ( pattern_or_uses , Path ): valid_paths = set ( decouple_uses ( pattern_or_uses )) else : valid_paths = None while not res . valid : for error in res . exception . errors (): missing_path = PATH_SEP . join ( error [ \"loc\" ]) validator . collect ( missing_path , value = {}) path = self . data_point_path_to_path ( missing_path ) if valid_paths is None : path_is_required = self . path_match ( pattern_or_uses , path ) else : path_is_required = path in valid_paths if path_is_required and self . query_data_point_info ( path ): yield missing_path res = validator . validate () decompose_data ( path , value , info = None ) classmethod \u00b6 Decompose a dict to separate data points. Source code in tiro/core/scenario.py @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) validate \u00b6 ValidationResult dataclass \u00b6 ValidationResult(start: datetime.datetime, end: datetime.datetime, valid: bool, exception: Union[jsonschema.exceptions.ValidationError, pydantic.error_wrappers.ValidationError, NoneType]) Source code in tiro/core/validate.py @dataclass class ValidationResult : start : datetime end : datetime valid : bool exception : Optional [ JSONSchemaValidatorError | PydanticValidationError ] def __str__ ( self ): msg = f \"Validation Period: { self . start } -- { self . end } \\n \" if self . valid : msg += \"Successful!\" else : msg += f \"Failed! \\n { str ( self . exception ) } \" return msg def json ( self ) -> str : return json . dumps ( self . info ()) def info ( self ) -> dict : return dict ( start = self . start . isoformat (), end = self . end . isoformat (), valid = self . valid , exception = self . serialise_exception (), ) def serialise_exception ( self ) -> Optional [ dict ]: if self . exception is None : return None elif isinstance ( self . exception , PydanticValidationError ): return self . exception . errors () elif isinstance ( self . exception , JSONSchemaValidatorError ): return dict ( message = self . exception . message , path = self . exception . json_path , description = str ( self . exception ), ) Validator \u00b6 Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. Source code in tiro/core/validate.py class Validator : \"\"\" Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. \"\"\" def __init__ ( self , entity : Entity = None , schema : dict = None , retention : int = 0 , log : bool = True , log_size : int = 100 , validate_path_only : bool = False , require_all_children : bool = True , ): if entity : self . model : Type [ BaseModel ] = entity . model ( hide_dp_values = validate_path_only , require_all_children = require_all_children , ) self . schema = None else : self . model = None self . schema = schema self . _data = {} self . retention : Optional [ timedelta ] = timedelta ( seconds = retention if retention > 0 else 1e9 ) self . data_create_time : datetime = datetime . now () self . log : deque [ ValidationResult ] = deque ( maxlen = log_size if log else 1 ) self . _collect_count = 0 def reset_data ( self ) -> None : self . _collect_count = 0 self . _data = {} if self . retention : self . data_create_time = datetime . now () def __enter__ ( self ): self . reset_data () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . reset_data () def validate_retention ( self ): if datetime . now () - self . data_create_time > self . retention : self . validate () self . reset_data () def collect ( self , path : str , value : Any ): self . validate_retention () insert_data_point_to_dict ( path , value , self . _data ) self . _collect_count += 1 def validate ( self ) -> ValidationResult : period_start = self . data_create_time period_end = datetime . now () if period_end - period_start > self . retention : period_end = period_start + self . retention try : if self . model : self . model . parse_obj ( self . _data ) elif self . schema : validate ( instance = self . _data , schema = self . schema ) res = ValidationResult ( period_start , period_end , True , None ) except Exception as e : res = ValidationResult ( period_start , period_end , False , e ) if self . log and self . log [ 0 ] . start == period_start : self . log [ 0 ] = res else : self . log . appendleft ( res ) return res def validate_dict ( self , content : dict ): self . _data = content return self . validate () @property def last_validation_start_time ( self ): if self . log : return self . log [ 0 ] . start else : return datetime . min @property def last_result ( self ): if self . log : return self . log [ 0 ] else : return None @property def current_collection_size ( self ): return self . _collect_count plugins special \u00b6 karez special \u00b6 connector \u00b6 ConnectorForMockServer ( RestfulConnectorBase ) \u00b6 Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result fetch_data ( self , client , entities ) async \u00b6 Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result dispatcher \u00b6 DispatcherForMockServer ( DispatcherBase ) \u00b6 Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json () load_entities ( self ) async \u00b6 Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"tiro"},{"location":"reference/#tiro.core","text":"","title":"core"},{"location":"reference/#tiro.core.mock","text":"","title":"mock"},{"location":"reference/#tiro.core.mock.MockedEntity","text":"Mock data generator for an entity class Source code in tiro/core/mock.py class MockedEntity ( MockedItem ): \"\"\"Mock data generator for an entity class\"\"\" def __init__ ( self , entity_type : Optional [ str ], * args , uuid = None , ** kwargs ): super ( MockedEntity , self ) . __init__ ( * args , ** kwargs ) self . children : dict [ str , dict [ str , MockedEntity ]] = {} # self.uuid: Optional[str] = uuid or str(uuid1()) self . uuid : Optional [ str ] = uuid or self . gen_uuid () self . entity_type : str = entity_type self . _initialised : bool = False self . _path : Optional [ str ] = None for dp_type in DataPointInfo . SUB_CLASSES : setattr ( self , camel_to_snake ( dp_type . __name__ ), {}) ref_dps = self . reference . get_data_points ( self . path ) if ref_dps is not None : ref_dps = set ( ref_dps ) for k in self . prototype . data_points (): v = self . prototype . data_point_info [ k ] dps = getattr ( self , camel_to_snake ( v . __class__ . __name__ )) k = camel_to_snake ( k ) if ref_dps is None or k in ref_dps and k not in dps : dps [ k ] = MockedDataPoint ( prototype = v , name = k , parent = self , reference = self . reference ) def generate ( self , regenerate : bool , include_data_points : bool , change_attrs : bool , use_default : bool , ) -> \"MockedEntity\" : if not self . _initialised or regenerate : self . children = {} for k , v in self . prototype . children . items (): _children = {} entity_type = camel_to_snake ( k ) prototype = self . prototype . child_info [ k ] number = prototype . number_faker () if prototype . ids and number > len ( prototype . ids ): logging . warning ( f \"Faking number ( { number } )is greater the length of predefined IDs ( { len ( prototype . ids ) } .\" f \"Only { len ( prototype . ids ) } instances will be generated.\" ) child_path = ( f \" { self . path }{ PATH_SEP }{ entity_type } \" if self . path else entity_type ) uuids = self . reference . get_children ( child_path ) if uuids is None : if prototype . ids : uuids = prototype . ids else : uuids = [ None for _ in range ( number )] else : uuids = list ( uuids . keys ()) number = len ( uuids ) for uuid in uuids [: number ]: entity = MockedEntity ( entity_type = entity_type , prototype = v , parent = self , reference = self . reference , uuid = uuid , ) _children [ entity . uuid ] = entity . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) self . children [ entity_type ] = _children self . _initialised = True if include_data_points : self . _generate_data_points ( change_attrs = change_attrs or regenerate , use_default = use_default ) return self def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res def _generate_data_points ( self , use_default , ** kwargs ) -> None : for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) v : MockedDataPoint for v in dps . values (): v . generate ( use_default = use_default , ** kwargs ) def search_entity ( self , uuid : str ) -> Optional [ \"MockedEntity\" ]: if uuid == self . uuid : return self else : for v in self . children . values (): for c in v . values (): entity = c . search_entity ( uuid ) if entity : return entity @property def path ( self ) -> str : if self . _path is None : if not self . parent : self . _path = \"\" else : self . _path = concat_path ( self . parent . path , self . entity_type , self . uuid ) return self . _path def list_entities ( self ) -> Generator [ tuple [ str , \"MockedEntity\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) yield self . path , self for v in self . children . values (): for c in v . values (): yield from c . list_entities () def list_data_points ( self , skip_default ) -> Generator [ tuple [ str , \"MockedDataPoint\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) for k , v in dps . items (): if not skip_default or v . prototype . default is None : yield concat_path ( self . path , dp_type_name , k ), v for v in self . children . values (): for c in v . values (): yield from c . list_data_points ( skip_default ) def gen_data_point ( self , dp_name : str , change_attrs = False , use_default = True ) -> dict : self . generate ( regenerate = False , include_data_points = False , change_attrs = change_attrs , use_default = use_default , ) dp = None for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) if dp_name in dps : dp = dps [ dp_name ] break if dp is None : raise KeyError ( f \"Cannot find data points { dp_name } in { self . prototype . unique_name } \" ) return dp . generate ( change_attrs = change_attrs , use_default = use_default ) . dict () def get_child ( self , path : str ) -> \"MockedEntity\" : if path : c_type , _ , path = path . partition ( PATH_SEP ) c_uuid , _ , path = path . partition ( PATH_SEP ) return self . children [ c_type ][ c_uuid ] . get_child ( path ) else : return self","title":"MockedEntity"},{"location":"reference/#tiro.core.mock.MockedEntity.dict","text":"Generate a complete tree starting from current entity Source code in tiro/core/mock.py def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res","title":"dict()"},{"location":"reference/#tiro.core.mock.Mocker","text":"Source code in tiro/core/mock.py class Mocker : def __init__ ( self , entity : Optional [ Entity ] = None , reference : Optional [ Path | dict ] = None ): if isinstance ( reference , Path ): reference = yaml . safe_load ( reference . open ()) self . entity : MockedEntity = MockedEntity ( entity_type = None , prototype = entity , reference = Reference ( reference ) ) self . entity_cache : Optional [ dict [ str , MockedEntity ]] = None def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) def gen_data_point ( self , path : str , change_attr : bool = False , use_default : bool = True ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path , _ , dp_name = path . rpartition ( PATH_SEP ) path , _ , _ = path . rpartition ( PATH_SEP ) return self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) def gen_value_by_uuid ( self , uuid : str , change_attr : bool = False , use_default : bool = True , value_only : bool = False , ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path = self . entity . reference . search_by_uuid ( uuid ) if path is not None : path , _ , dp_name = path . rpartition ( PATH_SEP ) dp = self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) if value_only : return dp [ \"value\" ] else : return dp else : raise KeyError def list_entities ( self ) -> list [ str ]: return [ k for k , _ in self . entity . list_entities ()] def list_data_points ( self , skip_default = True ) -> list [ str ]: return [ k for k , _ in self . entity . list_data_points ( skip_default = skip_default )] def list_uuids ( self ) -> list [ str ]: return self . entity . reference . list_uuids ()","title":"Mocker"},{"location":"reference/#tiro.core.mock.Mocker.dict","text":"Generate a complete dictionary for the tree starting from the given entity. Source code in tiro/core/mock.py def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , )","title":"dict()"},{"location":"reference/#tiro.core.mock.Mocker.json","text":"Generate a complete dictionary for the tree starting from the entity and return the coded json string. Source code in tiro/core/mock.py def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs )","title":"json()"},{"location":"reference/#tiro.core.model","text":"","title":"model"},{"location":"reference/#tiro.core.model.Alias","text":"Alias for a data point info Source code in tiro/core/model.py class Alias : \"\"\"Alias for a data point info\"\"\" def __init__ ( self , origin : str ): self . origin = origin","title":"Alias"},{"location":"reference/#tiro.core.model.Attribute","text":"Data point that seldom changes Source code in tiro/core/model.py class Attribute ( DataPointInfo ): \"\"\"Data point that seldom changes\"\"\" pass","title":"Attribute"},{"location":"reference/#tiro.core.model.DataPoint","text":"Base Pydantic Model to representing a data point Source code in tiro/core/model.py class DataPoint ( GenericModel , Generic [ DPT ]): \"\"\"Base Pydantic Model to representing a data point\"\"\" value : DPT timestamp : datetime _unit : Optional [ str ] = None class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"DataPoint\" ]) -> None : schema [ \"unit\" ] = model . _unit for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None )","title":"DataPoint"},{"location":"reference/#tiro.core.model.DataPointInfo","text":"Source code in tiro/core/model.py class DataPointInfo : SUB_CLASSES = set () SUB_CLASS_NAMES = set () def __init__ ( self , type : Any , unit : Optional [ str ] = None , faker : Optional [ Callable ] = None , time_var : Optional [ timedelta ] = timedelta ( seconds = 0 ), default = None , ): self . type = type self . unit = unit self . faker = faker self . time_var = time_var self . default = default def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) def default_object ( self , cls : Optional [ Type [ DataPoint ]] = None ) -> Optional [ DataPoint | dict ]: if self . default is None : return None else : current_time = datetime . utcnow () . isoformat () if cls : return cls ( value = self . default , timestamp = current_time , _unit = self . unit ) else : return dict ( value = self . default , timestamp = current_time , unit = self . unit )","title":"DataPointInfo"},{"location":"reference/#tiro.core.model.DataPointInfo.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ )","title":"__init_subclass__()"},{"location":"reference/#tiro.core.model.Entity","text":"Source code in tiro/core/model.py class Entity : class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"Entity\" ]) -> None : schema . pop ( \"title\" , None ) for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) data_point_info : dict [ str , DataPointInfo ] = {} def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} def __init__ ( self , parent : Optional [ \"Entity\" ] = None , ): self . children : dict [ str , Entity ] = {} self . parent : Optional [ Entity ] = parent self . _used_data_points : set [ str ] = set () self . uses = set () @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) @property def name ( self ) -> str : return self . __class__ . __name__ @property def unique_name ( self ) -> str : if self . parent and \"_\" not in self . name : unique_name = f \" { self . parent . unique_name } _ { self . __class__ . __name__ } \" else : unique_name = self . name return unique_name def _parse_use_yaml ( self , d , prefix = \"\" ) -> Generator [ str , None , None ]: if isinstance ( d , list ): for item in d : yield from self . _parse_use_yaml ( item , prefix = prefix ) elif isinstance ( d , dict ): for k , v in d . items (): yield from self . _parse_use_yaml ( v , prefix = concat_path ( prefix , k )) elif isinstance ( d , str ): yield concat_path ( prefix , d ) def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self @classmethod def update_defaults ( cls , ** defaults ): for key , value in defaults . items (): cls . data_point_info [ key ] . default = value def _create_date_points_model ( self , dp_category : Type [ DataPoint ], hide_dp_values : bool ) -> tuple [ Optional [ Type [ BaseModel ]], bool ]: \"\"\"Dynamically generate Pydantic model for all data points in the entity.\"\"\" info = { k : v for k , v in self . data_point_info . items () if isinstance ( v , dp_category ) and k in self . _used_data_points } if not info : return None , False sub_models : dict [ str , tuple [ type , Any ]] = {} is_optional = True for dp_name , dp_info in info . items (): dp_model_name = f \" { self . name } _ { dp_name } \" if hide_dp_values : dp_type = dict else : model_cache = self . __class__ . cached_data_point_model if dp_model_name not in model_cache : model_cache [ dp_model_name ] = type ( f \" { self . name } _ { dp_name } \" , ( DataPoint [ dp_info . type ],), dict ( _unit = dp_info . unit ), ) dp_type = model_cache [ dp_model_name ] if dp_info . default is not None and not hide_dp_values : sub_models [ camel_to_snake ( dp_name )] = Optional [ dp_type ], dp_info . default_object ( dp_type ) else : is_optional = False sub_models [ camel_to_snake ( dp_name )] = dp_type , ... return ( create_model ( f \" { self . unique_name } _ { dp_category . __name__ } \" , ** sub_models ), is_optional , ) def _create_entities_model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ dict [ str , tuple [ type , Any ]], bool ]: \"\"\"Dynamically generate Pydantic model for the entity type.\"\"\" fields = {} is_optional = True for name , ins in self . children . items (): sub_model_list_values , sub_is_optional = ins . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) child_info = self . child_info [ name ] if child_info . ids : sub_model_list = dict [ Literal [ tuple ( child_info . ids )], sub_model_list_values ] else : sub_model_list = dict [ str , sub_model_list_values ] if sub_is_optional and not require_all_children : fields [ camel_to_snake ( name )] = Optional [ sub_model_list ], {} else : is_optional = False fields [ camel_to_snake ( name )] = sub_model_list , ... return fields , is_optional def _model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ Type [ BaseModel ], bool ]: \"\"\"Generate a complete Pydantic model for a model tree staring from current entity.\"\"\" fields , is_optional = self . _create_entities_model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) for dp_category in DataPointInfo . SUB_CLASSES : dp_model , sub_is_optional = self . _create_date_points_model ( dp_category , hide_dp_values = hide_dp_values ) if dp_model : if sub_is_optional : fields |= { camel_to_snake ( dp_category . __name__ ): ( Optional [ dp_model ], {}) } else : fields |= { camel_to_snake ( dp_category . __name__ ): ( dp_model , ... )} is_optional &= sub_is_optional return create_model ( self . unique_name , config = self . Config , ** fields ), is_optional def model ( self , hide_dp_values : bool = False , require_all_children : bool = True ): return self . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children )[ 0 ] def __getattr__ ( self , item : str ) -> RequireHelper : return RequireHelper ( item , self ) def data_points ( self , used_only : bool = True ) -> list [ str ]: if used_only : return list ( self . _used_data_points ) else : return list ( self . data_point_info . keys ()) def query_data_point_info ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . query_data_point_info ( path [ 1 :]) elif path [ 0 ] in self . data_point_info : return self . data_point_info [ path [ 0 ]] def default_values ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . default_values ( path [ 1 :]) else : return {} else : res = {} for k in self . _used_data_points : dp_info = self . data_point_info [ k ] if dp_info . default : res [ k ] = dp_info . default_object () | dict ( type = dp_info . __class__ . __name__ ) return res @classmethod def use_selection_model ( cls , name_prefix = \"\" ): telemetry_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Telemetry ) ) attribute_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Attribute ) ) name = f \" { name_prefix } . { cls . __name__ } \" . strip ( \".\" ) model_kwargs = dict () if telemetry_names : model_kwargs [ \"telemetry\" ] = Optional [ list [ Literal [ telemetry_names ]]], Field ( None , unique_items = True ) if attribute_names : model_kwargs [ \"attribute\" ] = Optional [ list [ Literal [ attribute_names ]]], Field ( None , unique_items = True ) for k , v in cls . child_info . items (): model_kwargs [ k ] = Optional [ v . cls . use_selection_model ( name_prefix = name ) ], Field ( None ) return create_model ( name , ** model_kwargs ) def all_required_paths ( self , prefix = None ) -> Generator [ str , None , None ]: prefix = prefix or \"\" for name , child in self . children . items (): yield from child . all_required_paths ( concat_path ( prefix , name )) for dp in self . _used_data_points : yield concat_path ( prefix , dp ) def all_required_edges ( self , self_name = None ) -> Generator [ tuple [ str , str , str ], None , None ]: self_name = self_name or self . name for child_name , child in self . children . items (): yield \"is_parent_of\" , self_name , child_name yield from child . all_required_edges ( child_name ) for dp_name in self . _used_data_points : yield \"has_data_point\" , self_name , self . data_point_info [ dp_name ] . __class__ . __name__ def match_data_points ( self , pattern_or_uses : str | dict | Path , paths : list [ str ] = None ) -> Iterable [ str ]: if isinstance ( pattern_or_uses , str ): return filter ( partial ( self . path_match , pattern_or_uses ), paths or self . all_required_paths (), ) else : valid_paths = set ( decouple_uses ( pattern_or_uses )) return filter ( valid_paths . __contains__ , paths or self . all_required_paths ()) @staticmethod def path_match ( pattern : str , path : str ) -> bool : if pattern is not None : return bool ( re . fullmatch ( format_regex ( pattern ), path )) else : return True @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) def to_compact ( self , data ): res = { k : { c : self . children [ k ] . to_compact ( cv ) for c , cv in v . items ()} for k , v in data . items () if k in self . children } for dp_type in DataPointInfo . SUB_CLASS_NAMES : if dp_type in data : res |= { k : v [ \"value\" ] for k , v in data [ dp_type ] . items ()} return res","title":"Entity"},{"location":"reference/#tiro.core.model.Entity.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {}","title":"__init_subclass__()"},{"location":"reference/#tiro.core.model.Entity.create","text":"Dynamically create the entity class according to the entity name defined in an asset library Source code in tiro/core/model.py @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann ))","title":"create()"},{"location":"reference/#tiro.core.model.Entity.create_from_define_string","text":"Dynamically create the entity from a dictionary containing model infos Source code in tiro/core/model.py @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args )","title":"create_from_define_string()"},{"location":"reference/#tiro.core.model.Entity.many","text":"Return a list of the entities with the same type Source code in tiro/core/model.py @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs )","title":"many()"},{"location":"reference/#tiro.core.model.Entity.requires","text":"Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. Source code in tiro/core/model.py def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self","title":"requires()"},{"location":"reference/#tiro.core.model.EntityList","text":"Holding the information of a list of entity with the same type Source code in tiro/core/model.py class EntityList : \"\"\"Holding the information of a list of entity with the same type\"\"\" def __init__ ( self , cls : Type [ \"Entity\" ], faking_number : Optional [ Callable | int ] = None , ids : Optional [ list [ str ]] = None , ): self . cls = cls self . ids = ids if self . ids is not None : if faking_number is None : faking_number = len ( self . ids ) elif isinstance ( faking_number , int ) and faking_number > len ( self . ids ): raise RuntimeError ( \"When ids is provided, faking_number must be less than the length of ids.\" ) if isinstance ( faking_number , int ): self . number_faker = lambda : faking_number else : self . number_faker = faking_number def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent )","title":"EntityList"},{"location":"reference/#tiro.core.model.EntityList.new_entity","text":"Generate an entity instance Source code in tiro/core/model.py def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent )","title":"new_entity()"},{"location":"reference/#tiro.core.model.RequireHelper","text":"Helper class for requiring children or data points Source code in tiro/core/model.py class RequireHelper : \"\"\"Helper class for requiring children or data points\"\"\" def __init__ ( self , component : str , parent : Union [ \"RequireHelper\" , \"Entity\" ]): self . component = component self . parent = parent @property def path ( self ) -> str : if isinstance ( self . parent , RequireHelper ): return f \" { self . parent . path } . { self . component } \" else : return self . component @property def origin ( self ) -> Union [ \"RequireHelper\" , \"Entity\" ]: if isinstance ( self . parent , RequireHelper ): return self . parent . origin else : return self . parent def __getattr__ ( self , item ): return RequireHelper ( item , self ) def use ( self ) -> \"Entity\" : return self . origin . requires ( self . path )","title":"RequireHelper"},{"location":"reference/#tiro.core.model.Telemetry","text":"Data point that dynamically changes. Source code in tiro/core/model.py class Telemetry ( DataPointInfo ): \"\"\"Data point that dynamically changes.\"\"\" pass","title":"Telemetry"},{"location":"reference/#tiro.core.scenario","text":"","title":"scenario"},{"location":"reference/#tiro.core.scenario.Scenario","text":"Source code in tiro/core/scenario.py class Scenario : def __init__ ( self , * entities : Entity | Type [ Entity ], asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** kw_entities : dict [ str , Entity | Type [ Entity ]], ): self . root : Entity = Entity . create ( \"Scenario\" , * entities , base_classes = None , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** kw_entities , )() @classmethod def from_yaml ( cls , scenario_data : Path | str , * uses : Path | str ): if isinstance ( scenario_data , Path ): scenario_data = scenario_data . open () . read () defs = safe_load ( scenario_data ) asset_library_path = defs . get ( f \" { YAML_META_CHAR } asset_library_path\" , None ) asset_library_name = defs . get ( f \" { YAML_META_CHAR } asset_library_name\" , \"tiro.assets\" ) ins = cls ( ** { k : Entity . create_from_define_string ( k , v , prefix = \"Scenario\" , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } ) for use in uses : if isinstance ( use , Path ): use = use . open () . read () ins . requires ( yaml = use ) return ins def __getattr__ ( self , key ): return getattr ( self . root , key ) def mocker ( self , * args , ** kwargs ): return Mocker ( self . root , * args , ** kwargs ) def validator ( self , * args , ** kwargs ): return Validator ( self . root , * args , ** kwargs ) @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) @staticmethod def data_point_path_to_path ( path : str | list [ str ]) -> str : path = split_path ( path ) return f \" { PATH_SEP . join ( path [ i ] for i in range ( 0 , len ( path ) - 2 , 2 )) }{ PATH_SEP }{ path [ - 1 ] } \" @classmethod def data_point_path_to_tags ( cls , path : str | list [ str ], tags = None ) -> dict : tags = tags or dict ( path = cls . data_point_path_to_path ( path ), asset_path = path ) path = split_path ( path ) component = path . pop ( 0 ) if component in DataPointInfo . SUB_CLASS_NAMES : tags |= dict ( type = component , field = path [ - 1 ]) else : uuid = path . pop ( 0 ) tags |= { component : uuid } cls . data_point_path_to_tags ( path , tags ) return tags def guess_missing_paths ( self , existing_paths : Optional [ list [ str ]] = None , pattern_or_uses : Optional [ str | dict | Path ] = None , ): validator = self . validator ( validate_path_only = True , require_all_children = False ) if existing_paths : for path in existing_paths : validator . collect ( path , value = {}) res = validator . validate () if isinstance ( pattern_or_uses , dict ) or isinstance ( pattern_or_uses , Path ): valid_paths = set ( decouple_uses ( pattern_or_uses )) else : valid_paths = None while not res . valid : for error in res . exception . errors (): missing_path = PATH_SEP . join ( error [ \"loc\" ]) validator . collect ( missing_path , value = {}) path = self . data_point_path_to_path ( missing_path ) if valid_paths is None : path_is_required = self . path_match ( pattern_or_uses , path ) else : path_is_required = path in valid_paths if path_is_required and self . query_data_point_info ( path ): yield missing_path res = validator . validate ()","title":"Scenario"},{"location":"reference/#tiro.core.scenario.Scenario.decompose_data","text":"Decompose a dict to separate data points. Source code in tiro/core/scenario.py @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info )","title":"decompose_data()"},{"location":"reference/#tiro.core.validate","text":"","title":"validate"},{"location":"reference/#tiro.core.validate.ValidationResult","text":"ValidationResult(start: datetime.datetime, end: datetime.datetime, valid: bool, exception: Union[jsonschema.exceptions.ValidationError, pydantic.error_wrappers.ValidationError, NoneType]) Source code in tiro/core/validate.py @dataclass class ValidationResult : start : datetime end : datetime valid : bool exception : Optional [ JSONSchemaValidatorError | PydanticValidationError ] def __str__ ( self ): msg = f \"Validation Period: { self . start } -- { self . end } \\n \" if self . valid : msg += \"Successful!\" else : msg += f \"Failed! \\n { str ( self . exception ) } \" return msg def json ( self ) -> str : return json . dumps ( self . info ()) def info ( self ) -> dict : return dict ( start = self . start . isoformat (), end = self . end . isoformat (), valid = self . valid , exception = self . serialise_exception (), ) def serialise_exception ( self ) -> Optional [ dict ]: if self . exception is None : return None elif isinstance ( self . exception , PydanticValidationError ): return self . exception . errors () elif isinstance ( self . exception , JSONSchemaValidatorError ): return dict ( message = self . exception . message , path = self . exception . json_path , description = str ( self . exception ), )","title":"ValidationResult"},{"location":"reference/#tiro.core.validate.Validator","text":"Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. Source code in tiro/core/validate.py class Validator : \"\"\" Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. \"\"\" def __init__ ( self , entity : Entity = None , schema : dict = None , retention : int = 0 , log : bool = True , log_size : int = 100 , validate_path_only : bool = False , require_all_children : bool = True , ): if entity : self . model : Type [ BaseModel ] = entity . model ( hide_dp_values = validate_path_only , require_all_children = require_all_children , ) self . schema = None else : self . model = None self . schema = schema self . _data = {} self . retention : Optional [ timedelta ] = timedelta ( seconds = retention if retention > 0 else 1e9 ) self . data_create_time : datetime = datetime . now () self . log : deque [ ValidationResult ] = deque ( maxlen = log_size if log else 1 ) self . _collect_count = 0 def reset_data ( self ) -> None : self . _collect_count = 0 self . _data = {} if self . retention : self . data_create_time = datetime . now () def __enter__ ( self ): self . reset_data () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . reset_data () def validate_retention ( self ): if datetime . now () - self . data_create_time > self . retention : self . validate () self . reset_data () def collect ( self , path : str , value : Any ): self . validate_retention () insert_data_point_to_dict ( path , value , self . _data ) self . _collect_count += 1 def validate ( self ) -> ValidationResult : period_start = self . data_create_time period_end = datetime . now () if period_end - period_start > self . retention : period_end = period_start + self . retention try : if self . model : self . model . parse_obj ( self . _data ) elif self . schema : validate ( instance = self . _data , schema = self . schema ) res = ValidationResult ( period_start , period_end , True , None ) except Exception as e : res = ValidationResult ( period_start , period_end , False , e ) if self . log and self . log [ 0 ] . start == period_start : self . log [ 0 ] = res else : self . log . appendleft ( res ) return res def validate_dict ( self , content : dict ): self . _data = content return self . validate () @property def last_validation_start_time ( self ): if self . log : return self . log [ 0 ] . start else : return datetime . min @property def last_result ( self ): if self . log : return self . log [ 0 ] else : return None @property def current_collection_size ( self ): return self . _collect_count","title":"Validator"},{"location":"reference/#tiro.plugins","text":"","title":"plugins"},{"location":"reference/#tiro.plugins.karez","text":"","title":"karez"},{"location":"reference/#tiro.plugins.karez.connector","text":"","title":"connector"},{"location":"reference/#tiro.plugins.karez.connector.ConnectorForMockServer","text":"Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"ConnectorForMockServer"},{"location":"reference/#tiro.plugins.karez.connector.ConnectorForMockServer.fetch_data","text":"Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"fetch_data()"},{"location":"reference/#tiro.plugins.karez.dispatcher","text":"","title":"dispatcher"},{"location":"reference/#tiro.plugins.karez.dispatcher.DispatcherForMockServer","text":"Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"DispatcherForMockServer"},{"location":"reference/#tiro.plugins.karez.dispatcher.DispatcherForMockServer.load_entities","text":"Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"load_entities()"},{"location":"reference/SUMMARY/","text":"tiro cli draft mock schema validate core draft mock model scenario utils validate plugins graph agent aql qpath karez aggregator connector converter dispatcher utinni","title":"SUMMARY"},{"location":"reference/cli/","text":"","title":"cli"},{"location":"reference/cli/draft/","text":"","title":"draft"},{"location":"reference/cli/mock/","text":"","title":"mock"},{"location":"reference/cli/schema/","text":"","title":"schema"},{"location":"reference/cli/validate/","text":"","title":"validate"},{"location":"reference/core/","text":"mock \u00b6 MockedEntity ( MockedItem ) \u00b6 Mock data generator for an entity class Source code in tiro/core/mock.py class MockedEntity ( MockedItem ): \"\"\"Mock data generator for an entity class\"\"\" def __init__ ( self , entity_type : Optional [ str ], * args , uuid = None , ** kwargs ): super ( MockedEntity , self ) . __init__ ( * args , ** kwargs ) self . children : dict [ str , dict [ str , MockedEntity ]] = {} # self.uuid: Optional[str] = uuid or str(uuid1()) self . uuid : Optional [ str ] = uuid or self . gen_uuid () self . entity_type : str = entity_type self . _initialised : bool = False self . _path : Optional [ str ] = None for dp_type in DataPointInfo . SUB_CLASSES : setattr ( self , camel_to_snake ( dp_type . __name__ ), {}) ref_dps = self . reference . get_data_points ( self . path ) if ref_dps is not None : ref_dps = set ( ref_dps ) for k in self . prototype . data_points (): v = self . prototype . data_point_info [ k ] dps = getattr ( self , camel_to_snake ( v . __class__ . __name__ )) k = camel_to_snake ( k ) if ref_dps is None or k in ref_dps and k not in dps : dps [ k ] = MockedDataPoint ( prototype = v , name = k , parent = self , reference = self . reference ) def generate ( self , regenerate : bool , include_data_points : bool , change_attrs : bool , use_default : bool , ) -> \"MockedEntity\" : if not self . _initialised or regenerate : self . children = {} for k , v in self . prototype . children . items (): _children = {} entity_type = camel_to_snake ( k ) prototype = self . prototype . child_info [ k ] number = prototype . number_faker () if prototype . ids and number > len ( prototype . ids ): logging . warning ( f \"Faking number ( { number } )is greater the length of predefined IDs ( { len ( prototype . ids ) } .\" f \"Only { len ( prototype . ids ) } instances will be generated.\" ) child_path = ( f \" { self . path }{ PATH_SEP }{ entity_type } \" if self . path else entity_type ) uuids = self . reference . get_children ( child_path ) if uuids is None : if prototype . ids : uuids = prototype . ids else : uuids = [ None for _ in range ( number )] else : uuids = list ( uuids . keys ()) number = len ( uuids ) for uuid in uuids [: number ]: entity = MockedEntity ( entity_type = entity_type , prototype = v , parent = self , reference = self . reference , uuid = uuid , ) _children [ entity . uuid ] = entity . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) self . children [ entity_type ] = _children self . _initialised = True if include_data_points : self . _generate_data_points ( change_attrs = change_attrs or regenerate , use_default = use_default ) return self def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res def _generate_data_points ( self , use_default , ** kwargs ) -> None : for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) v : MockedDataPoint for v in dps . values (): v . generate ( use_default = use_default , ** kwargs ) def search_entity ( self , uuid : str ) -> Optional [ \"MockedEntity\" ]: if uuid == self . uuid : return self else : for v in self . children . values (): for c in v . values (): entity = c . search_entity ( uuid ) if entity : return entity @property def path ( self ) -> str : if self . _path is None : if not self . parent : self . _path = \"\" else : self . _path = concat_path ( self . parent . path , self . entity_type , self . uuid ) return self . _path def list_entities ( self ) -> Generator [ tuple [ str , \"MockedEntity\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) yield self . path , self for v in self . children . values (): for c in v . values (): yield from c . list_entities () def list_data_points ( self , skip_default ) -> Generator [ tuple [ str , \"MockedDataPoint\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) for k , v in dps . items (): if not skip_default or v . prototype . default is None : yield concat_path ( self . path , dp_type_name , k ), v for v in self . children . values (): for c in v . values (): yield from c . list_data_points ( skip_default ) def gen_data_point ( self , dp_name : str , change_attrs = False , use_default = True ) -> dict : self . generate ( regenerate = False , include_data_points = False , change_attrs = change_attrs , use_default = use_default , ) dp = None for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) if dp_name in dps : dp = dps [ dp_name ] break if dp is None : raise KeyError ( f \"Cannot find data points { dp_name } in { self . prototype . unique_name } \" ) return dp . generate ( change_attrs = change_attrs , use_default = use_default ) . dict () def get_child ( self , path : str ) -> \"MockedEntity\" : if path : c_type , _ , path = path . partition ( PATH_SEP ) c_uuid , _ , path = path . partition ( PATH_SEP ) return self . children [ c_type ][ c_uuid ] . get_child ( path ) else : return self dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) \u00b6 Generate a complete tree starting from current entity Source code in tiro/core/mock.py def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res Mocker \u00b6 Source code in tiro/core/mock.py class Mocker : def __init__ ( self , entity : Optional [ Entity ] = None , reference : Optional [ Path | dict ] = None ): if isinstance ( reference , Path ): reference = yaml . safe_load ( reference . open ()) self . entity : MockedEntity = MockedEntity ( entity_type = None , prototype = entity , reference = Reference ( reference ) ) self . entity_cache : Optional [ dict [ str , MockedEntity ]] = None def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) def gen_data_point ( self , path : str , change_attr : bool = False , use_default : bool = True ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path , _ , dp_name = path . rpartition ( PATH_SEP ) path , _ , _ = path . rpartition ( PATH_SEP ) return self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) def gen_value_by_uuid ( self , uuid : str , change_attr : bool = False , use_default : bool = True , value_only : bool = False , ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path = self . entity . reference . search_by_uuid ( uuid ) if path is not None : path , _ , dp_name = path . rpartition ( PATH_SEP ) dp = self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) if value_only : return dp [ \"value\" ] else : return dp else : raise KeyError def list_entities ( self ) -> list [ str ]: return [ k for k , _ in self . entity . list_entities ()] def list_data_points ( self , skip_default = True ) -> list [ str ]: return [ k for k , _ in self . entity . list_data_points ( skip_default = skip_default )] def list_uuids ( self ) -> list [ str ]: return self . entity . reference . list_uuids () dict ( self , regenerate = False , include_data_points = True , change_attrs = False , skip_default = True , use_default = True ) \u00b6 Generate a complete dictionary for the tree starting from the given entity. Source code in tiro/core/mock.py def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) json ( self , regenerate = False , include_data_points = True , change_attrs = False , skip_default = True , use_default = True , ** kwargs ) \u00b6 Generate a complete dictionary for the tree starting from the entity and return the coded json string. Source code in tiro/core/mock.py def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) model \u00b6 Alias \u00b6 Alias for a data point info Source code in tiro/core/model.py class Alias : \"\"\"Alias for a data point info\"\"\" def __init__ ( self , origin : str ): self . origin = origin Attribute ( DataPointInfo ) \u00b6 Data point that seldom changes Source code in tiro/core/model.py class Attribute ( DataPointInfo ): \"\"\"Data point that seldom changes\"\"\" pass DataPoint ( GenericModel , Generic ) pydantic-model \u00b6 Base Pydantic Model to representing a data point Source code in tiro/core/model.py class DataPoint ( GenericModel , Generic [ DPT ]): \"\"\"Base Pydantic Model to representing a data point\"\"\" value : DPT timestamp : datetime _unit : Optional [ str ] = None class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"DataPoint\" ]) -> None : schema [ \"unit\" ] = model . _unit for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) DataPointInfo \u00b6 Source code in tiro/core/model.py class DataPointInfo : SUB_CLASSES = set () SUB_CLASS_NAMES = set () def __init__ ( self , type : Any , unit : Optional [ str ] = None , faker : Optional [ Callable ] = None , time_var : Optional [ timedelta ] = timedelta ( seconds = 0 ), default = None , ): self . type = type self . unit = unit self . faker = faker self . time_var = time_var self . default = default def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) def default_object ( self , cls : Optional [ Type [ DataPoint ]] = None ) -> Optional [ DataPoint | dict ]: if self . default is None : return None else : current_time = datetime . utcnow () . isoformat () if cls : return cls ( value = self . default , timestamp = current_time , _unit = self . unit ) else : return dict ( value = self . default , timestamp = current_time , unit = self . unit ) __init_subclass__ ( ** kwargs ) classmethod special \u00b6 This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) Entity \u00b6 Source code in tiro/core/model.py class Entity : class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"Entity\" ]) -> None : schema . pop ( \"title\" , None ) for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) data_point_info : dict [ str , DataPointInfo ] = {} def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} def __init__ ( self , parent : Optional [ \"Entity\" ] = None , ): self . children : dict [ str , Entity ] = {} self . parent : Optional [ Entity ] = parent self . _used_data_points : set [ str ] = set () self . uses = set () @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) @property def name ( self ) -> str : return self . __class__ . __name__ @property def unique_name ( self ) -> str : if self . parent and \"_\" not in self . name : unique_name = f \" { self . parent . unique_name } _ { self . __class__ . __name__ } \" else : unique_name = self . name return unique_name def _parse_use_yaml ( self , d , prefix = \"\" ) -> Generator [ str , None , None ]: if isinstance ( d , list ): for item in d : yield from self . _parse_use_yaml ( item , prefix = prefix ) elif isinstance ( d , dict ): for k , v in d . items (): yield from self . _parse_use_yaml ( v , prefix = concat_path ( prefix , k )) elif isinstance ( d , str ): yield concat_path ( prefix , d ) def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self @classmethod def update_defaults ( cls , ** defaults ): for key , value in defaults . items (): cls . data_point_info [ key ] . default = value def _create_date_points_model ( self , dp_category : Type [ DataPoint ], hide_dp_values : bool ) -> tuple [ Optional [ Type [ BaseModel ]], bool ]: \"\"\"Dynamically generate Pydantic model for all data points in the entity.\"\"\" info = { k : v for k , v in self . data_point_info . items () if isinstance ( v , dp_category ) and k in self . _used_data_points } if not info : return None , False sub_models : dict [ str , tuple [ type , Any ]] = {} is_optional = True for dp_name , dp_info in info . items (): dp_model_name = f \" { self . name } _ { dp_name } \" if hide_dp_values : dp_type = dict else : model_cache = self . __class__ . cached_data_point_model if dp_model_name not in model_cache : model_cache [ dp_model_name ] = type ( f \" { self . name } _ { dp_name } \" , ( DataPoint [ dp_info . type ],), dict ( _unit = dp_info . unit ), ) dp_type = model_cache [ dp_model_name ] if dp_info . default is not None and not hide_dp_values : sub_models [ camel_to_snake ( dp_name )] = Optional [ dp_type ], dp_info . default_object ( dp_type ) else : is_optional = False sub_models [ camel_to_snake ( dp_name )] = dp_type , ... return ( create_model ( f \" { self . unique_name } _ { dp_category . __name__ } \" , ** sub_models ), is_optional , ) def _create_entities_model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ dict [ str , tuple [ type , Any ]], bool ]: \"\"\"Dynamically generate Pydantic model for the entity type.\"\"\" fields = {} is_optional = True for name , ins in self . children . items (): sub_model_list_values , sub_is_optional = ins . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) child_info = self . child_info [ name ] if child_info . ids : sub_model_list = dict [ Literal [ tuple ( child_info . ids )], sub_model_list_values ] else : sub_model_list = dict [ str , sub_model_list_values ] if sub_is_optional and not require_all_children : fields [ camel_to_snake ( name )] = Optional [ sub_model_list ], {} else : is_optional = False fields [ camel_to_snake ( name )] = sub_model_list , ... return fields , is_optional def _model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ Type [ BaseModel ], bool ]: \"\"\"Generate a complete Pydantic model for a model tree staring from current entity.\"\"\" fields , is_optional = self . _create_entities_model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) for dp_category in DataPointInfo . SUB_CLASSES : dp_model , sub_is_optional = self . _create_date_points_model ( dp_category , hide_dp_values = hide_dp_values ) if dp_model : if sub_is_optional : fields |= { camel_to_snake ( dp_category . __name__ ): ( Optional [ dp_model ], {}) } else : fields |= { camel_to_snake ( dp_category . __name__ ): ( dp_model , ... )} is_optional &= sub_is_optional return create_model ( self . unique_name , config = self . Config , ** fields ), is_optional def model ( self , hide_dp_values : bool = False , require_all_children : bool = True ): return self . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children )[ 0 ] def __getattr__ ( self , item : str ) -> RequireHelper : return RequireHelper ( item , self ) def data_points ( self , used_only : bool = True ) -> list [ str ]: if used_only : return list ( self . _used_data_points ) else : return list ( self . data_point_info . keys ()) def query_data_point_info ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . query_data_point_info ( path [ 1 :]) elif path [ 0 ] in self . data_point_info : return self . data_point_info [ path [ 0 ]] def default_values ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . default_values ( path [ 1 :]) else : return {} else : res = {} for k in self . _used_data_points : dp_info = self . data_point_info [ k ] if dp_info . default : res [ k ] = dp_info . default_object () | dict ( type = dp_info . __class__ . __name__ ) return res @classmethod def use_selection_model ( cls , name_prefix = \"\" ): telemetry_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Telemetry ) ) attribute_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Attribute ) ) name = f \" { name_prefix } . { cls . __name__ } \" . strip ( \".\" ) model_kwargs = dict () if telemetry_names : model_kwargs [ \"telemetry\" ] = Optional [ list [ Literal [ telemetry_names ]]], Field ( None , unique_items = True ) if attribute_names : model_kwargs [ \"attribute\" ] = Optional [ list [ Literal [ attribute_names ]]], Field ( None , unique_items = True ) for k , v in cls . child_info . items (): model_kwargs [ k ] = Optional [ v . cls . use_selection_model ( name_prefix = name ) ], Field ( None ) return create_model ( name , ** model_kwargs ) def all_required_paths ( self , prefix = None ) -> Generator [ str , None , None ]: prefix = prefix or \"\" for name , child in self . children . items (): yield from child . all_required_paths ( concat_path ( prefix , name )) for dp in self . _used_data_points : yield concat_path ( prefix , dp ) def all_required_edges ( self , self_name = None ) -> Generator [ tuple [ str , str , str ], None , None ]: self_name = self_name or self . name for child_name , child in self . children . items (): yield \"is_parent_of\" , self_name , child_name yield from child . all_required_edges ( child_name ) for dp_name in self . _used_data_points : yield \"has_data_point\" , self_name , self . data_point_info [ dp_name ] . __class__ . __name__ def match_data_points ( self , pattern_or_uses : str | dict | Path , paths : list [ str ] = None ) -> Iterable [ str ]: if isinstance ( pattern_or_uses , str ): return filter ( partial ( self . path_match , pattern_or_uses ), paths or self . all_required_paths (), ) else : valid_paths = set ( decouple_uses ( pattern_or_uses )) return filter ( valid_paths . __contains__ , paths or self . all_required_paths ()) @staticmethod def path_match ( pattern : str , path : str ) -> bool : if pattern is not None : return bool ( re . fullmatch ( format_regex ( pattern ), path )) else : return True @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) def to_compact ( self , data ): res = { k : { c : self . children [ k ] . to_compact ( cv ) for c , cv in v . items ()} for k , v in data . items () if k in self . children } for dp_type in DataPointInfo . SUB_CLASS_NAMES : if dp_type in data : res |= { k : v [ \"value\" ] for k , v in data [ dp_type ] . items ()} return res __init_subclass__ ( ** kwargs ) classmethod special \u00b6 This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} create ( name , * entities , * , base_classes = None , asset_library_path = None , asset_library_name = 'tiro.assets' , ** entity_dict ) classmethod \u00b6 Dynamically create the entity class according to the entity name defined in an asset library Source code in tiro/core/model.py @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) create_from_define_string ( name , defs , prefix = '' , asset_library_path = None , asset_library_name = 'tiro.assets' ) classmethod \u00b6 Dynamically create the entity from a dictionary containing model infos Source code in tiro/core/model.py @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) many ( * args , ** kwargs ) classmethod \u00b6 Return a list of the entities with the same type Source code in tiro/core/model.py @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) requires ( self , * paths , * , yaml = None ) \u00b6 Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. Source code in tiro/core/model.py def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self EntityList \u00b6 Holding the information of a list of entity with the same type Source code in tiro/core/model.py class EntityList : \"\"\"Holding the information of a list of entity with the same type\"\"\" def __init__ ( self , cls : Type [ \"Entity\" ], faking_number : Optional [ Callable | int ] = None , ids : Optional [ list [ str ]] = None , ): self . cls = cls self . ids = ids if self . ids is not None : if faking_number is None : faking_number = len ( self . ids ) elif isinstance ( faking_number , int ) and faking_number > len ( self . ids ): raise RuntimeError ( \"When ids is provided, faking_number must be less than the length of ids.\" ) if isinstance ( faking_number , int ): self . number_faker = lambda : faking_number else : self . number_faker = faking_number def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent ) new_entity ( self , parent = None ) \u00b6 Generate an entity instance Source code in tiro/core/model.py def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent ) RequireHelper \u00b6 Helper class for requiring children or data points Source code in tiro/core/model.py class RequireHelper : \"\"\"Helper class for requiring children or data points\"\"\" def __init__ ( self , component : str , parent : Union [ \"RequireHelper\" , \"Entity\" ]): self . component = component self . parent = parent @property def path ( self ) -> str : if isinstance ( self . parent , RequireHelper ): return f \" { self . parent . path } . { self . component } \" else : return self . component @property def origin ( self ) -> Union [ \"RequireHelper\" , \"Entity\" ]: if isinstance ( self . parent , RequireHelper ): return self . parent . origin else : return self . parent def __getattr__ ( self , item ): return RequireHelper ( item , self ) def use ( self ) -> \"Entity\" : return self . origin . requires ( self . path ) Telemetry ( DataPointInfo ) \u00b6 Data point that dynamically changes. Source code in tiro/core/model.py class Telemetry ( DataPointInfo ): \"\"\"Data point that dynamically changes.\"\"\" pass scenario \u00b6 Scenario \u00b6 Source code in tiro/core/scenario.py class Scenario : def __init__ ( self , * entities : Entity | Type [ Entity ], asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** kw_entities : dict [ str , Entity | Type [ Entity ]], ): self . root : Entity = Entity . create ( \"Scenario\" , * entities , base_classes = None , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** kw_entities , )() @classmethod def from_yaml ( cls , scenario_data : Path | str , * uses : Path | str ): if isinstance ( scenario_data , Path ): scenario_data = scenario_data . open () . read () defs = safe_load ( scenario_data ) asset_library_path = defs . get ( f \" { YAML_META_CHAR } asset_library_path\" , None ) asset_library_name = defs . get ( f \" { YAML_META_CHAR } asset_library_name\" , \"tiro.assets\" ) ins = cls ( ** { k : Entity . create_from_define_string ( k , v , prefix = \"Scenario\" , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } ) for use in uses : if isinstance ( use , Path ): use = use . open () . read () ins . requires ( yaml = use ) return ins def __getattr__ ( self , key ): return getattr ( self . root , key ) def mocker ( self , * args , ** kwargs ): return Mocker ( self . root , * args , ** kwargs ) def validator ( self , * args , ** kwargs ): return Validator ( self . root , * args , ** kwargs ) @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) @staticmethod def data_point_path_to_path ( path : str | list [ str ]) -> str : path = split_path ( path ) return f \" { PATH_SEP . join ( path [ i ] for i in range ( 0 , len ( path ) - 2 , 2 )) }{ PATH_SEP }{ path [ - 1 ] } \" @classmethod def data_point_path_to_tags ( cls , path : str | list [ str ], tags = None ) -> dict : tags = tags or dict ( path = cls . data_point_path_to_path ( path ), asset_path = path ) path = split_path ( path ) component = path . pop ( 0 ) if component in DataPointInfo . SUB_CLASS_NAMES : tags |= dict ( type = component , field = path [ - 1 ]) else : uuid = path . pop ( 0 ) tags |= { component : uuid } cls . data_point_path_to_tags ( path , tags ) return tags def guess_missing_paths ( self , existing_paths : Optional [ list [ str ]] = None , pattern_or_uses : Optional [ str | dict | Path ] = None , ): validator = self . validator ( validate_path_only = True , require_all_children = False ) if existing_paths : for path in existing_paths : validator . collect ( path , value = {}) res = validator . validate () if isinstance ( pattern_or_uses , dict ) or isinstance ( pattern_or_uses , Path ): valid_paths = set ( decouple_uses ( pattern_or_uses )) else : valid_paths = None while not res . valid : for error in res . exception . errors (): missing_path = PATH_SEP . join ( error [ \"loc\" ]) validator . collect ( missing_path , value = {}) path = self . data_point_path_to_path ( missing_path ) if valid_paths is None : path_is_required = self . path_match ( pattern_or_uses , path ) else : path_is_required = path in valid_paths if path_is_required and self . query_data_point_info ( path ): yield missing_path res = validator . validate () decompose_data ( path , value , info = None ) classmethod \u00b6 Decompose a dict to separate data points. Source code in tiro/core/scenario.py @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) validate \u00b6 ValidationResult dataclass \u00b6 ValidationResult(start: datetime.datetime, end: datetime.datetime, valid: bool, exception: Union[jsonschema.exceptions.ValidationError, pydantic.error_wrappers.ValidationError, NoneType]) Source code in tiro/core/validate.py @dataclass class ValidationResult : start : datetime end : datetime valid : bool exception : Optional [ JSONSchemaValidatorError | PydanticValidationError ] def __str__ ( self ): msg = f \"Validation Period: { self . start } -- { self . end } \\n \" if self . valid : msg += \"Successful!\" else : msg += f \"Failed! \\n { str ( self . exception ) } \" return msg def json ( self ) -> str : return json . dumps ( self . info ()) def info ( self ) -> dict : return dict ( start = self . start . isoformat (), end = self . end . isoformat (), valid = self . valid , exception = self . serialise_exception (), ) def serialise_exception ( self ) -> Optional [ dict ]: if self . exception is None : return None elif isinstance ( self . exception , PydanticValidationError ): return self . exception . errors () elif isinstance ( self . exception , JSONSchemaValidatorError ): return dict ( message = self . exception . message , path = self . exception . json_path , description = str ( self . exception ), ) Validator \u00b6 Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. Source code in tiro/core/validate.py class Validator : \"\"\" Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. \"\"\" def __init__ ( self , entity : Entity = None , schema : dict = None , retention : int = 0 , log : bool = True , log_size : int = 100 , validate_path_only : bool = False , require_all_children : bool = True , ): if entity : self . model : Type [ BaseModel ] = entity . model ( hide_dp_values = validate_path_only , require_all_children = require_all_children , ) self . schema = None else : self . model = None self . schema = schema self . _data = {} self . retention : Optional [ timedelta ] = timedelta ( seconds = retention if retention > 0 else 1e9 ) self . data_create_time : datetime = datetime . now () self . log : deque [ ValidationResult ] = deque ( maxlen = log_size if log else 1 ) self . _collect_count = 0 def reset_data ( self ) -> None : self . _collect_count = 0 self . _data = {} if self . retention : self . data_create_time = datetime . now () def __enter__ ( self ): self . reset_data () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . reset_data () def validate_retention ( self ): if datetime . now () - self . data_create_time > self . retention : self . validate () self . reset_data () def collect ( self , path : str , value : Any ): self . validate_retention () insert_data_point_to_dict ( path , value , self . _data ) self . _collect_count += 1 def validate ( self ) -> ValidationResult : period_start = self . data_create_time period_end = datetime . now () if period_end - period_start > self . retention : period_end = period_start + self . retention try : if self . model : self . model . parse_obj ( self . _data ) elif self . schema : validate ( instance = self . _data , schema = self . schema ) res = ValidationResult ( period_start , period_end , True , None ) except Exception as e : res = ValidationResult ( period_start , period_end , False , e ) if self . log and self . log [ 0 ] . start == period_start : self . log [ 0 ] = res else : self . log . appendleft ( res ) return res def validate_dict ( self , content : dict ): self . _data = content return self . validate () @property def last_validation_start_time ( self ): if self . log : return self . log [ 0 ] . start else : return datetime . min @property def last_result ( self ): if self . log : return self . log [ 0 ] else : return None @property def current_collection_size ( self ): return self . _collect_count","title":"core"},{"location":"reference/core/#tiro.core.mock","text":"","title":"mock"},{"location":"reference/core/#tiro.core.mock.MockedEntity","text":"Mock data generator for an entity class Source code in tiro/core/mock.py class MockedEntity ( MockedItem ): \"\"\"Mock data generator for an entity class\"\"\" def __init__ ( self , entity_type : Optional [ str ], * args , uuid = None , ** kwargs ): super ( MockedEntity , self ) . __init__ ( * args , ** kwargs ) self . children : dict [ str , dict [ str , MockedEntity ]] = {} # self.uuid: Optional[str] = uuid or str(uuid1()) self . uuid : Optional [ str ] = uuid or self . gen_uuid () self . entity_type : str = entity_type self . _initialised : bool = False self . _path : Optional [ str ] = None for dp_type in DataPointInfo . SUB_CLASSES : setattr ( self , camel_to_snake ( dp_type . __name__ ), {}) ref_dps = self . reference . get_data_points ( self . path ) if ref_dps is not None : ref_dps = set ( ref_dps ) for k in self . prototype . data_points (): v = self . prototype . data_point_info [ k ] dps = getattr ( self , camel_to_snake ( v . __class__ . __name__ )) k = camel_to_snake ( k ) if ref_dps is None or k in ref_dps and k not in dps : dps [ k ] = MockedDataPoint ( prototype = v , name = k , parent = self , reference = self . reference ) def generate ( self , regenerate : bool , include_data_points : bool , change_attrs : bool , use_default : bool , ) -> \"MockedEntity\" : if not self . _initialised or regenerate : self . children = {} for k , v in self . prototype . children . items (): _children = {} entity_type = camel_to_snake ( k ) prototype = self . prototype . child_info [ k ] number = prototype . number_faker () if prototype . ids and number > len ( prototype . ids ): logging . warning ( f \"Faking number ( { number } )is greater the length of predefined IDs ( { len ( prototype . ids ) } .\" f \"Only { len ( prototype . ids ) } instances will be generated.\" ) child_path = ( f \" { self . path }{ PATH_SEP }{ entity_type } \" if self . path else entity_type ) uuids = self . reference . get_children ( child_path ) if uuids is None : if prototype . ids : uuids = prototype . ids else : uuids = [ None for _ in range ( number )] else : uuids = list ( uuids . keys ()) number = len ( uuids ) for uuid in uuids [: number ]: entity = MockedEntity ( entity_type = entity_type , prototype = v , parent = self , reference = self . reference , uuid = uuid , ) _children [ entity . uuid ] = entity . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) self . children [ entity_type ] = _children self . _initialised = True if include_data_points : self . _generate_data_points ( change_attrs = change_attrs or regenerate , use_default = use_default ) return self def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res def _generate_data_points ( self , use_default , ** kwargs ) -> None : for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) v : MockedDataPoint for v in dps . values (): v . generate ( use_default = use_default , ** kwargs ) def search_entity ( self , uuid : str ) -> Optional [ \"MockedEntity\" ]: if uuid == self . uuid : return self else : for v in self . children . values (): for c in v . values (): entity = c . search_entity ( uuid ) if entity : return entity @property def path ( self ) -> str : if self . _path is None : if not self . parent : self . _path = \"\" else : self . _path = concat_path ( self . parent . path , self . entity_type , self . uuid ) return self . _path def list_entities ( self ) -> Generator [ tuple [ str , \"MockedEntity\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) yield self . path , self for v in self . children . values (): for c in v . values (): yield from c . list_entities () def list_data_points ( self , skip_default ) -> Generator [ tuple [ str , \"MockedDataPoint\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) for k , v in dps . items (): if not skip_default or v . prototype . default is None : yield concat_path ( self . path , dp_type_name , k ), v for v in self . children . values (): for c in v . values (): yield from c . list_data_points ( skip_default ) def gen_data_point ( self , dp_name : str , change_attrs = False , use_default = True ) -> dict : self . generate ( regenerate = False , include_data_points = False , change_attrs = change_attrs , use_default = use_default , ) dp = None for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) if dp_name in dps : dp = dps [ dp_name ] break if dp is None : raise KeyError ( f \"Cannot find data points { dp_name } in { self . prototype . unique_name } \" ) return dp . generate ( change_attrs = change_attrs , use_default = use_default ) . dict () def get_child ( self , path : str ) -> \"MockedEntity\" : if path : c_type , _ , path = path . partition ( PATH_SEP ) c_uuid , _ , path = path . partition ( PATH_SEP ) return self . children [ c_type ][ c_uuid ] . get_child ( path ) else : return self","title":"MockedEntity"},{"location":"reference/core/#tiro.core.mock.MockedEntity.dict","text":"Generate a complete tree starting from current entity Source code in tiro/core/mock.py def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res","title":"dict()"},{"location":"reference/core/#tiro.core.mock.Mocker","text":"Source code in tiro/core/mock.py class Mocker : def __init__ ( self , entity : Optional [ Entity ] = None , reference : Optional [ Path | dict ] = None ): if isinstance ( reference , Path ): reference = yaml . safe_load ( reference . open ()) self . entity : MockedEntity = MockedEntity ( entity_type = None , prototype = entity , reference = Reference ( reference ) ) self . entity_cache : Optional [ dict [ str , MockedEntity ]] = None def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) def gen_data_point ( self , path : str , change_attr : bool = False , use_default : bool = True ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path , _ , dp_name = path . rpartition ( PATH_SEP ) path , _ , _ = path . rpartition ( PATH_SEP ) return self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) def gen_value_by_uuid ( self , uuid : str , change_attr : bool = False , use_default : bool = True , value_only : bool = False , ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path = self . entity . reference . search_by_uuid ( uuid ) if path is not None : path , _ , dp_name = path . rpartition ( PATH_SEP ) dp = self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) if value_only : return dp [ \"value\" ] else : return dp else : raise KeyError def list_entities ( self ) -> list [ str ]: return [ k for k , _ in self . entity . list_entities ()] def list_data_points ( self , skip_default = True ) -> list [ str ]: return [ k for k , _ in self . entity . list_data_points ( skip_default = skip_default )] def list_uuids ( self ) -> list [ str ]: return self . entity . reference . list_uuids ()","title":"Mocker"},{"location":"reference/core/#tiro.core.mock.Mocker.dict","text":"Generate a complete dictionary for the tree starting from the given entity. Source code in tiro/core/mock.py def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , )","title":"dict()"},{"location":"reference/core/#tiro.core.mock.Mocker.json","text":"Generate a complete dictionary for the tree starting from the entity and return the coded json string. Source code in tiro/core/mock.py def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs )","title":"json()"},{"location":"reference/core/#tiro.core.model","text":"","title":"model"},{"location":"reference/core/#tiro.core.model.Alias","text":"Alias for a data point info Source code in tiro/core/model.py class Alias : \"\"\"Alias for a data point info\"\"\" def __init__ ( self , origin : str ): self . origin = origin","title":"Alias"},{"location":"reference/core/#tiro.core.model.Attribute","text":"Data point that seldom changes Source code in tiro/core/model.py class Attribute ( DataPointInfo ): \"\"\"Data point that seldom changes\"\"\" pass","title":"Attribute"},{"location":"reference/core/#tiro.core.model.DataPoint","text":"Base Pydantic Model to representing a data point Source code in tiro/core/model.py class DataPoint ( GenericModel , Generic [ DPT ]): \"\"\"Base Pydantic Model to representing a data point\"\"\" value : DPT timestamp : datetime _unit : Optional [ str ] = None class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"DataPoint\" ]) -> None : schema [ \"unit\" ] = model . _unit for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None )","title":"DataPoint"},{"location":"reference/core/#tiro.core.model.DataPointInfo","text":"Source code in tiro/core/model.py class DataPointInfo : SUB_CLASSES = set () SUB_CLASS_NAMES = set () def __init__ ( self , type : Any , unit : Optional [ str ] = None , faker : Optional [ Callable ] = None , time_var : Optional [ timedelta ] = timedelta ( seconds = 0 ), default = None , ): self . type = type self . unit = unit self . faker = faker self . time_var = time_var self . default = default def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) def default_object ( self , cls : Optional [ Type [ DataPoint ]] = None ) -> Optional [ DataPoint | dict ]: if self . default is None : return None else : current_time = datetime . utcnow () . isoformat () if cls : return cls ( value = self . default , timestamp = current_time , _unit = self . unit ) else : return dict ( value = self . default , timestamp = current_time , unit = self . unit )","title":"DataPointInfo"},{"location":"reference/core/#tiro.core.model.DataPointInfo.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ )","title":"__init_subclass__()"},{"location":"reference/core/#tiro.core.model.Entity","text":"Source code in tiro/core/model.py class Entity : class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"Entity\" ]) -> None : schema . pop ( \"title\" , None ) for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) data_point_info : dict [ str , DataPointInfo ] = {} def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} def __init__ ( self , parent : Optional [ \"Entity\" ] = None , ): self . children : dict [ str , Entity ] = {} self . parent : Optional [ Entity ] = parent self . _used_data_points : set [ str ] = set () self . uses = set () @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) @property def name ( self ) -> str : return self . __class__ . __name__ @property def unique_name ( self ) -> str : if self . parent and \"_\" not in self . name : unique_name = f \" { self . parent . unique_name } _ { self . __class__ . __name__ } \" else : unique_name = self . name return unique_name def _parse_use_yaml ( self , d , prefix = \"\" ) -> Generator [ str , None , None ]: if isinstance ( d , list ): for item in d : yield from self . _parse_use_yaml ( item , prefix = prefix ) elif isinstance ( d , dict ): for k , v in d . items (): yield from self . _parse_use_yaml ( v , prefix = concat_path ( prefix , k )) elif isinstance ( d , str ): yield concat_path ( prefix , d ) def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self @classmethod def update_defaults ( cls , ** defaults ): for key , value in defaults . items (): cls . data_point_info [ key ] . default = value def _create_date_points_model ( self , dp_category : Type [ DataPoint ], hide_dp_values : bool ) -> tuple [ Optional [ Type [ BaseModel ]], bool ]: \"\"\"Dynamically generate Pydantic model for all data points in the entity.\"\"\" info = { k : v for k , v in self . data_point_info . items () if isinstance ( v , dp_category ) and k in self . _used_data_points } if not info : return None , False sub_models : dict [ str , tuple [ type , Any ]] = {} is_optional = True for dp_name , dp_info in info . items (): dp_model_name = f \" { self . name } _ { dp_name } \" if hide_dp_values : dp_type = dict else : model_cache = self . __class__ . cached_data_point_model if dp_model_name not in model_cache : model_cache [ dp_model_name ] = type ( f \" { self . name } _ { dp_name } \" , ( DataPoint [ dp_info . type ],), dict ( _unit = dp_info . unit ), ) dp_type = model_cache [ dp_model_name ] if dp_info . default is not None and not hide_dp_values : sub_models [ camel_to_snake ( dp_name )] = Optional [ dp_type ], dp_info . default_object ( dp_type ) else : is_optional = False sub_models [ camel_to_snake ( dp_name )] = dp_type , ... return ( create_model ( f \" { self . unique_name } _ { dp_category . __name__ } \" , ** sub_models ), is_optional , ) def _create_entities_model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ dict [ str , tuple [ type , Any ]], bool ]: \"\"\"Dynamically generate Pydantic model for the entity type.\"\"\" fields = {} is_optional = True for name , ins in self . children . items (): sub_model_list_values , sub_is_optional = ins . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) child_info = self . child_info [ name ] if child_info . ids : sub_model_list = dict [ Literal [ tuple ( child_info . ids )], sub_model_list_values ] else : sub_model_list = dict [ str , sub_model_list_values ] if sub_is_optional and not require_all_children : fields [ camel_to_snake ( name )] = Optional [ sub_model_list ], {} else : is_optional = False fields [ camel_to_snake ( name )] = sub_model_list , ... return fields , is_optional def _model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ Type [ BaseModel ], bool ]: \"\"\"Generate a complete Pydantic model for a model tree staring from current entity.\"\"\" fields , is_optional = self . _create_entities_model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) for dp_category in DataPointInfo . SUB_CLASSES : dp_model , sub_is_optional = self . _create_date_points_model ( dp_category , hide_dp_values = hide_dp_values ) if dp_model : if sub_is_optional : fields |= { camel_to_snake ( dp_category . __name__ ): ( Optional [ dp_model ], {}) } else : fields |= { camel_to_snake ( dp_category . __name__ ): ( dp_model , ... )} is_optional &= sub_is_optional return create_model ( self . unique_name , config = self . Config , ** fields ), is_optional def model ( self , hide_dp_values : bool = False , require_all_children : bool = True ): return self . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children )[ 0 ] def __getattr__ ( self , item : str ) -> RequireHelper : return RequireHelper ( item , self ) def data_points ( self , used_only : bool = True ) -> list [ str ]: if used_only : return list ( self . _used_data_points ) else : return list ( self . data_point_info . keys ()) def query_data_point_info ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . query_data_point_info ( path [ 1 :]) elif path [ 0 ] in self . data_point_info : return self . data_point_info [ path [ 0 ]] def default_values ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . default_values ( path [ 1 :]) else : return {} else : res = {} for k in self . _used_data_points : dp_info = self . data_point_info [ k ] if dp_info . default : res [ k ] = dp_info . default_object () | dict ( type = dp_info . __class__ . __name__ ) return res @classmethod def use_selection_model ( cls , name_prefix = \"\" ): telemetry_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Telemetry ) ) attribute_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Attribute ) ) name = f \" { name_prefix } . { cls . __name__ } \" . strip ( \".\" ) model_kwargs = dict () if telemetry_names : model_kwargs [ \"telemetry\" ] = Optional [ list [ Literal [ telemetry_names ]]], Field ( None , unique_items = True ) if attribute_names : model_kwargs [ \"attribute\" ] = Optional [ list [ Literal [ attribute_names ]]], Field ( None , unique_items = True ) for k , v in cls . child_info . items (): model_kwargs [ k ] = Optional [ v . cls . use_selection_model ( name_prefix = name ) ], Field ( None ) return create_model ( name , ** model_kwargs ) def all_required_paths ( self , prefix = None ) -> Generator [ str , None , None ]: prefix = prefix or \"\" for name , child in self . children . items (): yield from child . all_required_paths ( concat_path ( prefix , name )) for dp in self . _used_data_points : yield concat_path ( prefix , dp ) def all_required_edges ( self , self_name = None ) -> Generator [ tuple [ str , str , str ], None , None ]: self_name = self_name or self . name for child_name , child in self . children . items (): yield \"is_parent_of\" , self_name , child_name yield from child . all_required_edges ( child_name ) for dp_name in self . _used_data_points : yield \"has_data_point\" , self_name , self . data_point_info [ dp_name ] . __class__ . __name__ def match_data_points ( self , pattern_or_uses : str | dict | Path , paths : list [ str ] = None ) -> Iterable [ str ]: if isinstance ( pattern_or_uses , str ): return filter ( partial ( self . path_match , pattern_or_uses ), paths or self . all_required_paths (), ) else : valid_paths = set ( decouple_uses ( pattern_or_uses )) return filter ( valid_paths . __contains__ , paths or self . all_required_paths ()) @staticmethod def path_match ( pattern : str , path : str ) -> bool : if pattern is not None : return bool ( re . fullmatch ( format_regex ( pattern ), path )) else : return True @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) def to_compact ( self , data ): res = { k : { c : self . children [ k ] . to_compact ( cv ) for c , cv in v . items ()} for k , v in data . items () if k in self . children } for dp_type in DataPointInfo . SUB_CLASS_NAMES : if dp_type in data : res |= { k : v [ \"value\" ] for k , v in data [ dp_type ] . items ()} return res","title":"Entity"},{"location":"reference/core/#tiro.core.model.Entity.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {}","title":"__init_subclass__()"},{"location":"reference/core/#tiro.core.model.Entity.create","text":"Dynamically create the entity class according to the entity name defined in an asset library Source code in tiro/core/model.py @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann ))","title":"create()"},{"location":"reference/core/#tiro.core.model.Entity.create_from_define_string","text":"Dynamically create the entity from a dictionary containing model infos Source code in tiro/core/model.py @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args )","title":"create_from_define_string()"},{"location":"reference/core/#tiro.core.model.Entity.many","text":"Return a list of the entities with the same type Source code in tiro/core/model.py @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs )","title":"many()"},{"location":"reference/core/#tiro.core.model.Entity.requires","text":"Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. Source code in tiro/core/model.py def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self","title":"requires()"},{"location":"reference/core/#tiro.core.model.EntityList","text":"Holding the information of a list of entity with the same type Source code in tiro/core/model.py class EntityList : \"\"\"Holding the information of a list of entity with the same type\"\"\" def __init__ ( self , cls : Type [ \"Entity\" ], faking_number : Optional [ Callable | int ] = None , ids : Optional [ list [ str ]] = None , ): self . cls = cls self . ids = ids if self . ids is not None : if faking_number is None : faking_number = len ( self . ids ) elif isinstance ( faking_number , int ) and faking_number > len ( self . ids ): raise RuntimeError ( \"When ids is provided, faking_number must be less than the length of ids.\" ) if isinstance ( faking_number , int ): self . number_faker = lambda : faking_number else : self . number_faker = faking_number def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent )","title":"EntityList"},{"location":"reference/core/#tiro.core.model.EntityList.new_entity","text":"Generate an entity instance Source code in tiro/core/model.py def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent )","title":"new_entity()"},{"location":"reference/core/#tiro.core.model.RequireHelper","text":"Helper class for requiring children or data points Source code in tiro/core/model.py class RequireHelper : \"\"\"Helper class for requiring children or data points\"\"\" def __init__ ( self , component : str , parent : Union [ \"RequireHelper\" , \"Entity\" ]): self . component = component self . parent = parent @property def path ( self ) -> str : if isinstance ( self . parent , RequireHelper ): return f \" { self . parent . path } . { self . component } \" else : return self . component @property def origin ( self ) -> Union [ \"RequireHelper\" , \"Entity\" ]: if isinstance ( self . parent , RequireHelper ): return self . parent . origin else : return self . parent def __getattr__ ( self , item ): return RequireHelper ( item , self ) def use ( self ) -> \"Entity\" : return self . origin . requires ( self . path )","title":"RequireHelper"},{"location":"reference/core/#tiro.core.model.Telemetry","text":"Data point that dynamically changes. Source code in tiro/core/model.py class Telemetry ( DataPointInfo ): \"\"\"Data point that dynamically changes.\"\"\" pass","title":"Telemetry"},{"location":"reference/core/#tiro.core.scenario","text":"","title":"scenario"},{"location":"reference/core/#tiro.core.scenario.Scenario","text":"Source code in tiro/core/scenario.py class Scenario : def __init__ ( self , * entities : Entity | Type [ Entity ], asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** kw_entities : dict [ str , Entity | Type [ Entity ]], ): self . root : Entity = Entity . create ( \"Scenario\" , * entities , base_classes = None , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** kw_entities , )() @classmethod def from_yaml ( cls , scenario_data : Path | str , * uses : Path | str ): if isinstance ( scenario_data , Path ): scenario_data = scenario_data . open () . read () defs = safe_load ( scenario_data ) asset_library_path = defs . get ( f \" { YAML_META_CHAR } asset_library_path\" , None ) asset_library_name = defs . get ( f \" { YAML_META_CHAR } asset_library_name\" , \"tiro.assets\" ) ins = cls ( ** { k : Entity . create_from_define_string ( k , v , prefix = \"Scenario\" , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } ) for use in uses : if isinstance ( use , Path ): use = use . open () . read () ins . requires ( yaml = use ) return ins def __getattr__ ( self , key ): return getattr ( self . root , key ) def mocker ( self , * args , ** kwargs ): return Mocker ( self . root , * args , ** kwargs ) def validator ( self , * args , ** kwargs ): return Validator ( self . root , * args , ** kwargs ) @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) @staticmethod def data_point_path_to_path ( path : str | list [ str ]) -> str : path = split_path ( path ) return f \" { PATH_SEP . join ( path [ i ] for i in range ( 0 , len ( path ) - 2 , 2 )) }{ PATH_SEP }{ path [ - 1 ] } \" @classmethod def data_point_path_to_tags ( cls , path : str | list [ str ], tags = None ) -> dict : tags = tags or dict ( path = cls . data_point_path_to_path ( path ), asset_path = path ) path = split_path ( path ) component = path . pop ( 0 ) if component in DataPointInfo . SUB_CLASS_NAMES : tags |= dict ( type = component , field = path [ - 1 ]) else : uuid = path . pop ( 0 ) tags |= { component : uuid } cls . data_point_path_to_tags ( path , tags ) return tags def guess_missing_paths ( self , existing_paths : Optional [ list [ str ]] = None , pattern_or_uses : Optional [ str | dict | Path ] = None , ): validator = self . validator ( validate_path_only = True , require_all_children = False ) if existing_paths : for path in existing_paths : validator . collect ( path , value = {}) res = validator . validate () if isinstance ( pattern_or_uses , dict ) or isinstance ( pattern_or_uses , Path ): valid_paths = set ( decouple_uses ( pattern_or_uses )) else : valid_paths = None while not res . valid : for error in res . exception . errors (): missing_path = PATH_SEP . join ( error [ \"loc\" ]) validator . collect ( missing_path , value = {}) path = self . data_point_path_to_path ( missing_path ) if valid_paths is None : path_is_required = self . path_match ( pattern_or_uses , path ) else : path_is_required = path in valid_paths if path_is_required and self . query_data_point_info ( path ): yield missing_path res = validator . validate ()","title":"Scenario"},{"location":"reference/core/#tiro.core.scenario.Scenario.decompose_data","text":"Decompose a dict to separate data points. Source code in tiro/core/scenario.py @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info )","title":"decompose_data()"},{"location":"reference/core/#tiro.core.validate","text":"","title":"validate"},{"location":"reference/core/#tiro.core.validate.ValidationResult","text":"ValidationResult(start: datetime.datetime, end: datetime.datetime, valid: bool, exception: Union[jsonschema.exceptions.ValidationError, pydantic.error_wrappers.ValidationError, NoneType]) Source code in tiro/core/validate.py @dataclass class ValidationResult : start : datetime end : datetime valid : bool exception : Optional [ JSONSchemaValidatorError | PydanticValidationError ] def __str__ ( self ): msg = f \"Validation Period: { self . start } -- { self . end } \\n \" if self . valid : msg += \"Successful!\" else : msg += f \"Failed! \\n { str ( self . exception ) } \" return msg def json ( self ) -> str : return json . dumps ( self . info ()) def info ( self ) -> dict : return dict ( start = self . start . isoformat (), end = self . end . isoformat (), valid = self . valid , exception = self . serialise_exception (), ) def serialise_exception ( self ) -> Optional [ dict ]: if self . exception is None : return None elif isinstance ( self . exception , PydanticValidationError ): return self . exception . errors () elif isinstance ( self . exception , JSONSchemaValidatorError ): return dict ( message = self . exception . message , path = self . exception . json_path , description = str ( self . exception ), )","title":"ValidationResult"},{"location":"reference/core/#tiro.core.validate.Validator","text":"Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. Source code in tiro/core/validate.py class Validator : \"\"\" Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. \"\"\" def __init__ ( self , entity : Entity = None , schema : dict = None , retention : int = 0 , log : bool = True , log_size : int = 100 , validate_path_only : bool = False , require_all_children : bool = True , ): if entity : self . model : Type [ BaseModel ] = entity . model ( hide_dp_values = validate_path_only , require_all_children = require_all_children , ) self . schema = None else : self . model = None self . schema = schema self . _data = {} self . retention : Optional [ timedelta ] = timedelta ( seconds = retention if retention > 0 else 1e9 ) self . data_create_time : datetime = datetime . now () self . log : deque [ ValidationResult ] = deque ( maxlen = log_size if log else 1 ) self . _collect_count = 0 def reset_data ( self ) -> None : self . _collect_count = 0 self . _data = {} if self . retention : self . data_create_time = datetime . now () def __enter__ ( self ): self . reset_data () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . reset_data () def validate_retention ( self ): if datetime . now () - self . data_create_time > self . retention : self . validate () self . reset_data () def collect ( self , path : str , value : Any ): self . validate_retention () insert_data_point_to_dict ( path , value , self . _data ) self . _collect_count += 1 def validate ( self ) -> ValidationResult : period_start = self . data_create_time period_end = datetime . now () if period_end - period_start > self . retention : period_end = period_start + self . retention try : if self . model : self . model . parse_obj ( self . _data ) elif self . schema : validate ( instance = self . _data , schema = self . schema ) res = ValidationResult ( period_start , period_end , True , None ) except Exception as e : res = ValidationResult ( period_start , period_end , False , e ) if self . log and self . log [ 0 ] . start == period_start : self . log [ 0 ] = res else : self . log . appendleft ( res ) return res def validate_dict ( self , content : dict ): self . _data = content return self . validate () @property def last_validation_start_time ( self ): if self . log : return self . log [ 0 ] . start else : return datetime . min @property def last_result ( self ): if self . log : return self . log [ 0 ] else : return None @property def current_collection_size ( self ): return self . _collect_count","title":"Validator"},{"location":"reference/core/draft/","text":"","title":"draft"},{"location":"reference/core/mock/","text":"MockedEntity ( MockedItem ) \u00b6 Mock data generator for an entity class Source code in tiro/core/mock.py class MockedEntity ( MockedItem ): \"\"\"Mock data generator for an entity class\"\"\" def __init__ ( self , entity_type : Optional [ str ], * args , uuid = None , ** kwargs ): super ( MockedEntity , self ) . __init__ ( * args , ** kwargs ) self . children : dict [ str , dict [ str , MockedEntity ]] = {} # self.uuid: Optional[str] = uuid or str(uuid1()) self . uuid : Optional [ str ] = uuid or self . gen_uuid () self . entity_type : str = entity_type self . _initialised : bool = False self . _path : Optional [ str ] = None for dp_type in DataPointInfo . SUB_CLASSES : setattr ( self , camel_to_snake ( dp_type . __name__ ), {}) ref_dps = self . reference . get_data_points ( self . path ) if ref_dps is not None : ref_dps = set ( ref_dps ) for k in self . prototype . data_points (): v = self . prototype . data_point_info [ k ] dps = getattr ( self , camel_to_snake ( v . __class__ . __name__ )) k = camel_to_snake ( k ) if ref_dps is None or k in ref_dps and k not in dps : dps [ k ] = MockedDataPoint ( prototype = v , name = k , parent = self , reference = self . reference ) def generate ( self , regenerate : bool , include_data_points : bool , change_attrs : bool , use_default : bool , ) -> \"MockedEntity\" : if not self . _initialised or regenerate : self . children = {} for k , v in self . prototype . children . items (): _children = {} entity_type = camel_to_snake ( k ) prototype = self . prototype . child_info [ k ] number = prototype . number_faker () if prototype . ids and number > len ( prototype . ids ): logging . warning ( f \"Faking number ( { number } )is greater the length of predefined IDs ( { len ( prototype . ids ) } .\" f \"Only { len ( prototype . ids ) } instances will be generated.\" ) child_path = ( f \" { self . path }{ PATH_SEP }{ entity_type } \" if self . path else entity_type ) uuids = self . reference . get_children ( child_path ) if uuids is None : if prototype . ids : uuids = prototype . ids else : uuids = [ None for _ in range ( number )] else : uuids = list ( uuids . keys ()) number = len ( uuids ) for uuid in uuids [: number ]: entity = MockedEntity ( entity_type = entity_type , prototype = v , parent = self , reference = self . reference , uuid = uuid , ) _children [ entity . uuid ] = entity . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) self . children [ entity_type ] = _children self . _initialised = True if include_data_points : self . _generate_data_points ( change_attrs = change_attrs or regenerate , use_default = use_default ) return self def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res def _generate_data_points ( self , use_default , ** kwargs ) -> None : for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) v : MockedDataPoint for v in dps . values (): v . generate ( use_default = use_default , ** kwargs ) def search_entity ( self , uuid : str ) -> Optional [ \"MockedEntity\" ]: if uuid == self . uuid : return self else : for v in self . children . values (): for c in v . values (): entity = c . search_entity ( uuid ) if entity : return entity @property def path ( self ) -> str : if self . _path is None : if not self . parent : self . _path = \"\" else : self . _path = concat_path ( self . parent . path , self . entity_type , self . uuid ) return self . _path def list_entities ( self ) -> Generator [ tuple [ str , \"MockedEntity\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) yield self . path , self for v in self . children . values (): for c in v . values (): yield from c . list_entities () def list_data_points ( self , skip_default ) -> Generator [ tuple [ str , \"MockedDataPoint\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) for k , v in dps . items (): if not skip_default or v . prototype . default is None : yield concat_path ( self . path , dp_type_name , k ), v for v in self . children . values (): for c in v . values (): yield from c . list_data_points ( skip_default ) def gen_data_point ( self , dp_name : str , change_attrs = False , use_default = True ) -> dict : self . generate ( regenerate = False , include_data_points = False , change_attrs = change_attrs , use_default = use_default , ) dp = None for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) if dp_name in dps : dp = dps [ dp_name ] break if dp is None : raise KeyError ( f \"Cannot find data points { dp_name } in { self . prototype . unique_name } \" ) return dp . generate ( change_attrs = change_attrs , use_default = use_default ) . dict () def get_child ( self , path : str ) -> \"MockedEntity\" : if path : c_type , _ , path = path . partition ( PATH_SEP ) c_uuid , _ , path = path . partition ( PATH_SEP ) return self . children [ c_type ][ c_uuid ] . get_child ( path ) else : return self dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) \u00b6 Generate a complete tree starting from current entity Source code in tiro/core/mock.py def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res Mocker \u00b6 Source code in tiro/core/mock.py class Mocker : def __init__ ( self , entity : Optional [ Entity ] = None , reference : Optional [ Path | dict ] = None ): if isinstance ( reference , Path ): reference = yaml . safe_load ( reference . open ()) self . entity : MockedEntity = MockedEntity ( entity_type = None , prototype = entity , reference = Reference ( reference ) ) self . entity_cache : Optional [ dict [ str , MockedEntity ]] = None def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) def gen_data_point ( self , path : str , change_attr : bool = False , use_default : bool = True ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path , _ , dp_name = path . rpartition ( PATH_SEP ) path , _ , _ = path . rpartition ( PATH_SEP ) return self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) def gen_value_by_uuid ( self , uuid : str , change_attr : bool = False , use_default : bool = True , value_only : bool = False , ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path = self . entity . reference . search_by_uuid ( uuid ) if path is not None : path , _ , dp_name = path . rpartition ( PATH_SEP ) dp = self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) if value_only : return dp [ \"value\" ] else : return dp else : raise KeyError def list_entities ( self ) -> list [ str ]: return [ k for k , _ in self . entity . list_entities ()] def list_data_points ( self , skip_default = True ) -> list [ str ]: return [ k for k , _ in self . entity . list_data_points ( skip_default = skip_default )] def list_uuids ( self ) -> list [ str ]: return self . entity . reference . list_uuids () dict ( self , regenerate = False , include_data_points = True , change_attrs = False , skip_default = True , use_default = True ) \u00b6 Generate a complete dictionary for the tree starting from the given entity. Source code in tiro/core/mock.py def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) json ( self , regenerate = False , include_data_points = True , change_attrs = False , skip_default = True , use_default = True , ** kwargs ) \u00b6 Generate a complete dictionary for the tree starting from the entity and return the coded json string. Source code in tiro/core/mock.py def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs )","title":"mock"},{"location":"reference/core/mock/#tiro.core.mock.MockedEntity","text":"Mock data generator for an entity class Source code in tiro/core/mock.py class MockedEntity ( MockedItem ): \"\"\"Mock data generator for an entity class\"\"\" def __init__ ( self , entity_type : Optional [ str ], * args , uuid = None , ** kwargs ): super ( MockedEntity , self ) . __init__ ( * args , ** kwargs ) self . children : dict [ str , dict [ str , MockedEntity ]] = {} # self.uuid: Optional[str] = uuid or str(uuid1()) self . uuid : Optional [ str ] = uuid or self . gen_uuid () self . entity_type : str = entity_type self . _initialised : bool = False self . _path : Optional [ str ] = None for dp_type in DataPointInfo . SUB_CLASSES : setattr ( self , camel_to_snake ( dp_type . __name__ ), {}) ref_dps = self . reference . get_data_points ( self . path ) if ref_dps is not None : ref_dps = set ( ref_dps ) for k in self . prototype . data_points (): v = self . prototype . data_point_info [ k ] dps = getattr ( self , camel_to_snake ( v . __class__ . __name__ )) k = camel_to_snake ( k ) if ref_dps is None or k in ref_dps and k not in dps : dps [ k ] = MockedDataPoint ( prototype = v , name = k , parent = self , reference = self . reference ) def generate ( self , regenerate : bool , include_data_points : bool , change_attrs : bool , use_default : bool , ) -> \"MockedEntity\" : if not self . _initialised or regenerate : self . children = {} for k , v in self . prototype . children . items (): _children = {} entity_type = camel_to_snake ( k ) prototype = self . prototype . child_info [ k ] number = prototype . number_faker () if prototype . ids and number > len ( prototype . ids ): logging . warning ( f \"Faking number ( { number } )is greater the length of predefined IDs ( { len ( prototype . ids ) } .\" f \"Only { len ( prototype . ids ) } instances will be generated.\" ) child_path = ( f \" { self . path }{ PATH_SEP }{ entity_type } \" if self . path else entity_type ) uuids = self . reference . get_children ( child_path ) if uuids is None : if prototype . ids : uuids = prototype . ids else : uuids = [ None for _ in range ( number )] else : uuids = list ( uuids . keys ()) number = len ( uuids ) for uuid in uuids [: number ]: entity = MockedEntity ( entity_type = entity_type , prototype = v , parent = self , reference = self . reference , uuid = uuid , ) _children [ entity . uuid ] = entity . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) self . children [ entity_type ] = _children self . _initialised = True if include_data_points : self . _generate_data_points ( change_attrs = change_attrs or regenerate , use_default = use_default ) return self def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res def _generate_data_points ( self , use_default , ** kwargs ) -> None : for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) v : MockedDataPoint for v in dps . values (): v . generate ( use_default = use_default , ** kwargs ) def search_entity ( self , uuid : str ) -> Optional [ \"MockedEntity\" ]: if uuid == self . uuid : return self else : for v in self . children . values (): for c in v . values (): entity = c . search_entity ( uuid ) if entity : return entity @property def path ( self ) -> str : if self . _path is None : if not self . parent : self . _path = \"\" else : self . _path = concat_path ( self . parent . path , self . entity_type , self . uuid ) return self . _path def list_entities ( self ) -> Generator [ tuple [ str , \"MockedEntity\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) yield self . path , self for v in self . children . values (): for c in v . values (): yield from c . list_entities () def list_data_points ( self , skip_default ) -> Generator [ tuple [ str , \"MockedDataPoint\" ], None , None ]: self . generate ( regenerate = False , include_data_points = False , change_attrs = False , use_default = True , ) for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) for k , v in dps . items (): if not skip_default or v . prototype . default is None : yield concat_path ( self . path , dp_type_name , k ), v for v in self . children . values (): for c in v . values (): yield from c . list_data_points ( skip_default ) def gen_data_point ( self , dp_name : str , change_attrs = False , use_default = True ) -> dict : self . generate ( regenerate = False , include_data_points = False , change_attrs = change_attrs , use_default = use_default , ) dp = None for dp_type in DataPointInfo . SUB_CLASSES : dps = getattr ( self , camel_to_snake ( dp_type . __name__ )) if dp_name in dps : dp = dps [ dp_name ] break if dp is None : raise KeyError ( f \"Cannot find data points { dp_name } in { self . prototype . unique_name } \" ) return dp . generate ( change_attrs = change_attrs , use_default = use_default ) . dict () def get_child ( self , path : str ) -> \"MockedEntity\" : if path : c_type , _ , path = path . partition ( PATH_SEP ) c_uuid , _ , path = path . partition ( PATH_SEP ) return self . children [ c_type ][ c_uuid ] . get_child ( path ) else : return self","title":"MockedEntity"},{"location":"reference/core/mock/#tiro.core.mock.MockedEntity.dict","text":"Generate a complete tree starting from current entity Source code in tiro/core/mock.py def dict ( self , regenerate , include_data_points , change_attrs , skip_default , use_default ) -> dict : \"\"\"Generate a complete tree starting from current entity\"\"\" self . generate ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , use_default = use_default , ) res = {} if self . children : for k , v in self . children . items (): _values = {} for uuid , c in v . items (): _sub_value = c . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) if _sub_value : _values |= { uuid : _sub_value } if _values : res |= { k : _values } for dp_type in DataPointInfo . SUB_CLASSES : dp_type_name = camel_to_snake ( dp_type . __name__ ) dps = getattr ( self , dp_type_name ) if dps : if include_data_points : _values = { k : v . dict () for k , v in dps . items () if not skip_default or v . prototype . default is None } if _values : res |= { dp_type_name : _values } else : res |= { dp_type_name : list ( dps . keys ())} return res","title":"dict()"},{"location":"reference/core/mock/#tiro.core.mock.Mocker","text":"Source code in tiro/core/mock.py class Mocker : def __init__ ( self , entity : Optional [ Entity ] = None , reference : Optional [ Path | dict ] = None ): if isinstance ( reference , Path ): reference = yaml . safe_load ( reference . open ()) self . entity : MockedEntity = MockedEntity ( entity_type = None , prototype = entity , reference = Reference ( reference ) ) self . entity_cache : Optional [ dict [ str , MockedEntity ]] = None def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs ) def gen_data_point ( self , path : str , change_attr : bool = False , use_default : bool = True ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path , _ , dp_name = path . rpartition ( PATH_SEP ) path , _ , _ = path . rpartition ( PATH_SEP ) return self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) def gen_value_by_uuid ( self , uuid : str , change_attr : bool = False , use_default : bool = True , value_only : bool = False , ) -> dict : self . entity . generate ( regenerate = False , include_data_points = False , change_attrs = change_attr , use_default = use_default , ) path = self . entity . reference . search_by_uuid ( uuid ) if path is not None : path , _ , dp_name = path . rpartition ( PATH_SEP ) dp = self . entity . get_child ( path ) . gen_data_point ( dp_name , change_attrs = change_attr , use_default = use_default ) if value_only : return dp [ \"value\" ] else : return dp else : raise KeyError def list_entities ( self ) -> list [ str ]: return [ k for k , _ in self . entity . list_entities ()] def list_data_points ( self , skip_default = True ) -> list [ str ]: return [ k for k , _ in self . entity . list_data_points ( skip_default = skip_default )] def list_uuids ( self ) -> list [ str ]: return self . entity . reference . list_uuids ()","title":"Mocker"},{"location":"reference/core/mock/#tiro.core.mock.Mocker.dict","text":"Generate a complete dictionary for the tree starting from the given entity. Source code in tiro/core/mock.py def dict ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ) -> dict : \"\"\"Generate a complete dictionary for the tree starting from the given entity.\"\"\" if regenerate : self . entity_cache = None return self . entity . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , )","title":"dict()"},{"location":"reference/core/mock/#tiro.core.mock.Mocker.json","text":"Generate a complete dictionary for the tree starting from the entity and return the coded json string. Source code in tiro/core/mock.py def json ( self , regenerate : bool = False , include_data_points : bool = True , change_attrs : bool = False , skip_default : bool = True , use_default : bool = True , ** kwargs , ) -> str : \"\"\"Generate a complete dictionary for the tree starting from the entity and return the coded json string.\"\"\" d = self . dict ( regenerate = regenerate , include_data_points = include_data_points , change_attrs = change_attrs , skip_default = skip_default , use_default = use_default , ) return json . dumps ( d , ** kwargs )","title":"json()"},{"location":"reference/core/model/","text":"Alias \u00b6 Alias for a data point info Source code in tiro/core/model.py class Alias : \"\"\"Alias for a data point info\"\"\" def __init__ ( self , origin : str ): self . origin = origin Attribute ( DataPointInfo ) \u00b6 Data point that seldom changes Source code in tiro/core/model.py class Attribute ( DataPointInfo ): \"\"\"Data point that seldom changes\"\"\" pass DataPoint ( GenericModel , Generic ) pydantic-model \u00b6 Base Pydantic Model to representing a data point Source code in tiro/core/model.py class DataPoint ( GenericModel , Generic [ DPT ]): \"\"\"Base Pydantic Model to representing a data point\"\"\" value : DPT timestamp : datetime _unit : Optional [ str ] = None class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"DataPoint\" ]) -> None : schema [ \"unit\" ] = model . _unit for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) DataPointInfo \u00b6 Source code in tiro/core/model.py class DataPointInfo : SUB_CLASSES = set () SUB_CLASS_NAMES = set () def __init__ ( self , type : Any , unit : Optional [ str ] = None , faker : Optional [ Callable ] = None , time_var : Optional [ timedelta ] = timedelta ( seconds = 0 ), default = None , ): self . type = type self . unit = unit self . faker = faker self . time_var = time_var self . default = default def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) def default_object ( self , cls : Optional [ Type [ DataPoint ]] = None ) -> Optional [ DataPoint | dict ]: if self . default is None : return None else : current_time = datetime . utcnow () . isoformat () if cls : return cls ( value = self . default , timestamp = current_time , _unit = self . unit ) else : return dict ( value = self . default , timestamp = current_time , unit = self . unit ) __init_subclass__ ( ** kwargs ) classmethod special \u00b6 This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) Entity \u00b6 Source code in tiro/core/model.py class Entity : class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"Entity\" ]) -> None : schema . pop ( \"title\" , None ) for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) data_point_info : dict [ str , DataPointInfo ] = {} def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} def __init__ ( self , parent : Optional [ \"Entity\" ] = None , ): self . children : dict [ str , Entity ] = {} self . parent : Optional [ Entity ] = parent self . _used_data_points : set [ str ] = set () self . uses = set () @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) @property def name ( self ) -> str : return self . __class__ . __name__ @property def unique_name ( self ) -> str : if self . parent and \"_\" not in self . name : unique_name = f \" { self . parent . unique_name } _ { self . __class__ . __name__ } \" else : unique_name = self . name return unique_name def _parse_use_yaml ( self , d , prefix = \"\" ) -> Generator [ str , None , None ]: if isinstance ( d , list ): for item in d : yield from self . _parse_use_yaml ( item , prefix = prefix ) elif isinstance ( d , dict ): for k , v in d . items (): yield from self . _parse_use_yaml ( v , prefix = concat_path ( prefix , k )) elif isinstance ( d , str ): yield concat_path ( prefix , d ) def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self @classmethod def update_defaults ( cls , ** defaults ): for key , value in defaults . items (): cls . data_point_info [ key ] . default = value def _create_date_points_model ( self , dp_category : Type [ DataPoint ], hide_dp_values : bool ) -> tuple [ Optional [ Type [ BaseModel ]], bool ]: \"\"\"Dynamically generate Pydantic model for all data points in the entity.\"\"\" info = { k : v for k , v in self . data_point_info . items () if isinstance ( v , dp_category ) and k in self . _used_data_points } if not info : return None , False sub_models : dict [ str , tuple [ type , Any ]] = {} is_optional = True for dp_name , dp_info in info . items (): dp_model_name = f \" { self . name } _ { dp_name } \" if hide_dp_values : dp_type = dict else : model_cache = self . __class__ . cached_data_point_model if dp_model_name not in model_cache : model_cache [ dp_model_name ] = type ( f \" { self . name } _ { dp_name } \" , ( DataPoint [ dp_info . type ],), dict ( _unit = dp_info . unit ), ) dp_type = model_cache [ dp_model_name ] if dp_info . default is not None and not hide_dp_values : sub_models [ camel_to_snake ( dp_name )] = Optional [ dp_type ], dp_info . default_object ( dp_type ) else : is_optional = False sub_models [ camel_to_snake ( dp_name )] = dp_type , ... return ( create_model ( f \" { self . unique_name } _ { dp_category . __name__ } \" , ** sub_models ), is_optional , ) def _create_entities_model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ dict [ str , tuple [ type , Any ]], bool ]: \"\"\"Dynamically generate Pydantic model for the entity type.\"\"\" fields = {} is_optional = True for name , ins in self . children . items (): sub_model_list_values , sub_is_optional = ins . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) child_info = self . child_info [ name ] if child_info . ids : sub_model_list = dict [ Literal [ tuple ( child_info . ids )], sub_model_list_values ] else : sub_model_list = dict [ str , sub_model_list_values ] if sub_is_optional and not require_all_children : fields [ camel_to_snake ( name )] = Optional [ sub_model_list ], {} else : is_optional = False fields [ camel_to_snake ( name )] = sub_model_list , ... return fields , is_optional def _model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ Type [ BaseModel ], bool ]: \"\"\"Generate a complete Pydantic model for a model tree staring from current entity.\"\"\" fields , is_optional = self . _create_entities_model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) for dp_category in DataPointInfo . SUB_CLASSES : dp_model , sub_is_optional = self . _create_date_points_model ( dp_category , hide_dp_values = hide_dp_values ) if dp_model : if sub_is_optional : fields |= { camel_to_snake ( dp_category . __name__ ): ( Optional [ dp_model ], {}) } else : fields |= { camel_to_snake ( dp_category . __name__ ): ( dp_model , ... )} is_optional &= sub_is_optional return create_model ( self . unique_name , config = self . Config , ** fields ), is_optional def model ( self , hide_dp_values : bool = False , require_all_children : bool = True ): return self . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children )[ 0 ] def __getattr__ ( self , item : str ) -> RequireHelper : return RequireHelper ( item , self ) def data_points ( self , used_only : bool = True ) -> list [ str ]: if used_only : return list ( self . _used_data_points ) else : return list ( self . data_point_info . keys ()) def query_data_point_info ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . query_data_point_info ( path [ 1 :]) elif path [ 0 ] in self . data_point_info : return self . data_point_info [ path [ 0 ]] def default_values ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . default_values ( path [ 1 :]) else : return {} else : res = {} for k in self . _used_data_points : dp_info = self . data_point_info [ k ] if dp_info . default : res [ k ] = dp_info . default_object () | dict ( type = dp_info . __class__ . __name__ ) return res @classmethod def use_selection_model ( cls , name_prefix = \"\" ): telemetry_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Telemetry ) ) attribute_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Attribute ) ) name = f \" { name_prefix } . { cls . __name__ } \" . strip ( \".\" ) model_kwargs = dict () if telemetry_names : model_kwargs [ \"telemetry\" ] = Optional [ list [ Literal [ telemetry_names ]]], Field ( None , unique_items = True ) if attribute_names : model_kwargs [ \"attribute\" ] = Optional [ list [ Literal [ attribute_names ]]], Field ( None , unique_items = True ) for k , v in cls . child_info . items (): model_kwargs [ k ] = Optional [ v . cls . use_selection_model ( name_prefix = name ) ], Field ( None ) return create_model ( name , ** model_kwargs ) def all_required_paths ( self , prefix = None ) -> Generator [ str , None , None ]: prefix = prefix or \"\" for name , child in self . children . items (): yield from child . all_required_paths ( concat_path ( prefix , name )) for dp in self . _used_data_points : yield concat_path ( prefix , dp ) def all_required_edges ( self , self_name = None ) -> Generator [ tuple [ str , str , str ], None , None ]: self_name = self_name or self . name for child_name , child in self . children . items (): yield \"is_parent_of\" , self_name , child_name yield from child . all_required_edges ( child_name ) for dp_name in self . _used_data_points : yield \"has_data_point\" , self_name , self . data_point_info [ dp_name ] . __class__ . __name__ def match_data_points ( self , pattern_or_uses : str | dict | Path , paths : list [ str ] = None ) -> Iterable [ str ]: if isinstance ( pattern_or_uses , str ): return filter ( partial ( self . path_match , pattern_or_uses ), paths or self . all_required_paths (), ) else : valid_paths = set ( decouple_uses ( pattern_or_uses )) return filter ( valid_paths . __contains__ , paths or self . all_required_paths ()) @staticmethod def path_match ( pattern : str , path : str ) -> bool : if pattern is not None : return bool ( re . fullmatch ( format_regex ( pattern ), path )) else : return True @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) def to_compact ( self , data ): res = { k : { c : self . children [ k ] . to_compact ( cv ) for c , cv in v . items ()} for k , v in data . items () if k in self . children } for dp_type in DataPointInfo . SUB_CLASS_NAMES : if dp_type in data : res |= { k : v [ \"value\" ] for k , v in data [ dp_type ] . items ()} return res __init_subclass__ ( ** kwargs ) classmethod special \u00b6 This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} create ( name , * entities , * , base_classes = None , asset_library_path = None , asset_library_name = 'tiro.assets' , ** entity_dict ) classmethod \u00b6 Dynamically create the entity class according to the entity name defined in an asset library Source code in tiro/core/model.py @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) create_from_define_string ( name , defs , prefix = '' , asset_library_path = None , asset_library_name = 'tiro.assets' ) classmethod \u00b6 Dynamically create the entity from a dictionary containing model infos Source code in tiro/core/model.py @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) many ( * args , ** kwargs ) classmethod \u00b6 Return a list of the entities with the same type Source code in tiro/core/model.py @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) requires ( self , * paths , * , yaml = None ) \u00b6 Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. Source code in tiro/core/model.py def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self EntityList \u00b6 Holding the information of a list of entity with the same type Source code in tiro/core/model.py class EntityList : \"\"\"Holding the information of a list of entity with the same type\"\"\" def __init__ ( self , cls : Type [ \"Entity\" ], faking_number : Optional [ Callable | int ] = None , ids : Optional [ list [ str ]] = None , ): self . cls = cls self . ids = ids if self . ids is not None : if faking_number is None : faking_number = len ( self . ids ) elif isinstance ( faking_number , int ) and faking_number > len ( self . ids ): raise RuntimeError ( \"When ids is provided, faking_number must be less than the length of ids.\" ) if isinstance ( faking_number , int ): self . number_faker = lambda : faking_number else : self . number_faker = faking_number def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent ) new_entity ( self , parent = None ) \u00b6 Generate an entity instance Source code in tiro/core/model.py def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent ) RequireHelper \u00b6 Helper class for requiring children or data points Source code in tiro/core/model.py class RequireHelper : \"\"\"Helper class for requiring children or data points\"\"\" def __init__ ( self , component : str , parent : Union [ \"RequireHelper\" , \"Entity\" ]): self . component = component self . parent = parent @property def path ( self ) -> str : if isinstance ( self . parent , RequireHelper ): return f \" { self . parent . path } . { self . component } \" else : return self . component @property def origin ( self ) -> Union [ \"RequireHelper\" , \"Entity\" ]: if isinstance ( self . parent , RequireHelper ): return self . parent . origin else : return self . parent def __getattr__ ( self , item ): return RequireHelper ( item , self ) def use ( self ) -> \"Entity\" : return self . origin . requires ( self . path ) Telemetry ( DataPointInfo ) \u00b6 Data point that dynamically changes. Source code in tiro/core/model.py class Telemetry ( DataPointInfo ): \"\"\"Data point that dynamically changes.\"\"\" pass","title":"model"},{"location":"reference/core/model/#tiro.core.model.Alias","text":"Alias for a data point info Source code in tiro/core/model.py class Alias : \"\"\"Alias for a data point info\"\"\" def __init__ ( self , origin : str ): self . origin = origin","title":"Alias"},{"location":"reference/core/model/#tiro.core.model.Attribute","text":"Data point that seldom changes Source code in tiro/core/model.py class Attribute ( DataPointInfo ): \"\"\"Data point that seldom changes\"\"\" pass","title":"Attribute"},{"location":"reference/core/model/#tiro.core.model.DataPoint","text":"Base Pydantic Model to representing a data point Source code in tiro/core/model.py class DataPoint ( GenericModel , Generic [ DPT ]): \"\"\"Base Pydantic Model to representing a data point\"\"\" value : DPT timestamp : datetime _unit : Optional [ str ] = None class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"DataPoint\" ]) -> None : schema [ \"unit\" ] = model . _unit for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None )","title":"DataPoint"},{"location":"reference/core/model/#tiro.core.model.DataPointInfo","text":"Source code in tiro/core/model.py class DataPointInfo : SUB_CLASSES = set () SUB_CLASS_NAMES = set () def __init__ ( self , type : Any , unit : Optional [ str ] = None , faker : Optional [ Callable ] = None , time_var : Optional [ timedelta ] = timedelta ( seconds = 0 ), default = None , ): self . type = type self . unit = unit self . faker = faker self . time_var = time_var self . default = default def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ ) def default_object ( self , cls : Optional [ Type [ DataPoint ]] = None ) -> Optional [ DataPoint | dict ]: if self . default is None : return None else : current_time = datetime . utcnow () . isoformat () if cls : return cls ( value = self . default , timestamp = current_time , _unit = self . unit ) else : return dict ( value = self . default , timestamp = current_time , unit = self . unit )","title":"DataPointInfo"},{"location":"reference/core/model/#tiro.core.model.DataPointInfo.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): cls . SUB_CLASSES . add ( cls ) cls . SUB_CLASS_NAMES . add ( cls . __name__ )","title":"__init_subclass__()"},{"location":"reference/core/model/#tiro.core.model.Entity","text":"Source code in tiro/core/model.py class Entity : class Config : @staticmethod def schema_extra ( schema : dict [ str , Any ], model : Type [ \"Entity\" ]) -> None : schema . pop ( \"title\" , None ) for p in schema [ \"properties\" ] . values (): p . pop ( \"title\" , None ) data_point_info : dict [ str , DataPointInfo ] = {} def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {} def __init__ ( self , parent : Optional [ \"Entity\" ] = None , ): self . children : dict [ str , Entity ] = {} self . parent : Optional [ Entity ] = parent self . _used_data_points : set [ str ] = set () self . uses = set () @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs ) @property def name ( self ) -> str : return self . __class__ . __name__ @property def unique_name ( self ) -> str : if self . parent and \"_\" not in self . name : unique_name = f \" { self . parent . unique_name } _ { self . __class__ . __name__ } \" else : unique_name = self . name return unique_name def _parse_use_yaml ( self , d , prefix = \"\" ) -> Generator [ str , None , None ]: if isinstance ( d , list ): for item in d : yield from self . _parse_use_yaml ( item , prefix = prefix ) elif isinstance ( d , dict ): for k , v in d . items (): yield from self . _parse_use_yaml ( v , prefix = concat_path ( prefix , k )) elif isinstance ( d , str ): yield concat_path ( prefix , d ) def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self @classmethod def update_defaults ( cls , ** defaults ): for key , value in defaults . items (): cls . data_point_info [ key ] . default = value def _create_date_points_model ( self , dp_category : Type [ DataPoint ], hide_dp_values : bool ) -> tuple [ Optional [ Type [ BaseModel ]], bool ]: \"\"\"Dynamically generate Pydantic model for all data points in the entity.\"\"\" info = { k : v for k , v in self . data_point_info . items () if isinstance ( v , dp_category ) and k in self . _used_data_points } if not info : return None , False sub_models : dict [ str , tuple [ type , Any ]] = {} is_optional = True for dp_name , dp_info in info . items (): dp_model_name = f \" { self . name } _ { dp_name } \" if hide_dp_values : dp_type = dict else : model_cache = self . __class__ . cached_data_point_model if dp_model_name not in model_cache : model_cache [ dp_model_name ] = type ( f \" { self . name } _ { dp_name } \" , ( DataPoint [ dp_info . type ],), dict ( _unit = dp_info . unit ), ) dp_type = model_cache [ dp_model_name ] if dp_info . default is not None and not hide_dp_values : sub_models [ camel_to_snake ( dp_name )] = Optional [ dp_type ], dp_info . default_object ( dp_type ) else : is_optional = False sub_models [ camel_to_snake ( dp_name )] = dp_type , ... return ( create_model ( f \" { self . unique_name } _ { dp_category . __name__ } \" , ** sub_models ), is_optional , ) def _create_entities_model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ dict [ str , tuple [ type , Any ]], bool ]: \"\"\"Dynamically generate Pydantic model for the entity type.\"\"\" fields = {} is_optional = True for name , ins in self . children . items (): sub_model_list_values , sub_is_optional = ins . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) child_info = self . child_info [ name ] if child_info . ids : sub_model_list = dict [ Literal [ tuple ( child_info . ids )], sub_model_list_values ] else : sub_model_list = dict [ str , sub_model_list_values ] if sub_is_optional and not require_all_children : fields [ camel_to_snake ( name )] = Optional [ sub_model_list ], {} else : is_optional = False fields [ camel_to_snake ( name )] = sub_model_list , ... return fields , is_optional def _model ( self , hide_dp_values : bool , require_all_children : bool ) -> tuple [ Type [ BaseModel ], bool ]: \"\"\"Generate a complete Pydantic model for a model tree staring from current entity.\"\"\" fields , is_optional = self . _create_entities_model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children ) for dp_category in DataPointInfo . SUB_CLASSES : dp_model , sub_is_optional = self . _create_date_points_model ( dp_category , hide_dp_values = hide_dp_values ) if dp_model : if sub_is_optional : fields |= { camel_to_snake ( dp_category . __name__ ): ( Optional [ dp_model ], {}) } else : fields |= { camel_to_snake ( dp_category . __name__ ): ( dp_model , ... )} is_optional &= sub_is_optional return create_model ( self . unique_name , config = self . Config , ** fields ), is_optional def model ( self , hide_dp_values : bool = False , require_all_children : bool = True ): return self . _model ( hide_dp_values = hide_dp_values , require_all_children = require_all_children )[ 0 ] def __getattr__ ( self , item : str ) -> RequireHelper : return RequireHelper ( item , self ) def data_points ( self , used_only : bool = True ) -> list [ str ]: if used_only : return list ( self . _used_data_points ) else : return list ( self . data_point_info . keys ()) def query_data_point_info ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . query_data_point_info ( path [ 1 :]) elif path [ 0 ] in self . data_point_info : return self . data_point_info [ path [ 0 ]] def default_values ( self , path : str | list [ str ]): path = split_path ( path ) if path : if path [ 0 ] in self . children . keys (): return self . children [ path [ 0 ]] . default_values ( path [ 1 :]) else : return {} else : res = {} for k in self . _used_data_points : dp_info = self . data_point_info [ k ] if dp_info . default : res [ k ] = dp_info . default_object () | dict ( type = dp_info . __class__ . __name__ ) return res @classmethod def use_selection_model ( cls , name_prefix = \"\" ): telemetry_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Telemetry ) ) attribute_names = tuple ( k for k , v in cls . data_point_info . items () if isinstance ( v , Attribute ) ) name = f \" { name_prefix } . { cls . __name__ } \" . strip ( \".\" ) model_kwargs = dict () if telemetry_names : model_kwargs [ \"telemetry\" ] = Optional [ list [ Literal [ telemetry_names ]]], Field ( None , unique_items = True ) if attribute_names : model_kwargs [ \"attribute\" ] = Optional [ list [ Literal [ attribute_names ]]], Field ( None , unique_items = True ) for k , v in cls . child_info . items (): model_kwargs [ k ] = Optional [ v . cls . use_selection_model ( name_prefix = name ) ], Field ( None ) return create_model ( name , ** model_kwargs ) def all_required_paths ( self , prefix = None ) -> Generator [ str , None , None ]: prefix = prefix or \"\" for name , child in self . children . items (): yield from child . all_required_paths ( concat_path ( prefix , name )) for dp in self . _used_data_points : yield concat_path ( prefix , dp ) def all_required_edges ( self , self_name = None ) -> Generator [ tuple [ str , str , str ], None , None ]: self_name = self_name or self . name for child_name , child in self . children . items (): yield \"is_parent_of\" , self_name , child_name yield from child . all_required_edges ( child_name ) for dp_name in self . _used_data_points : yield \"has_data_point\" , self_name , self . data_point_info [ dp_name ] . __class__ . __name__ def match_data_points ( self , pattern_or_uses : str | dict | Path , paths : list [ str ] = None ) -> Iterable [ str ]: if isinstance ( pattern_or_uses , str ): return filter ( partial ( self . path_match , pattern_or_uses ), paths or self . all_required_paths (), ) else : valid_paths = set ( decouple_uses ( pattern_or_uses )) return filter ( valid_paths . __contains__ , paths or self . all_required_paths ()) @staticmethod def path_match ( pattern : str , path : str ) -> bool : if pattern is not None : return bool ( re . fullmatch ( format_regex ( pattern ), path )) else : return True @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann )) @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args ) def to_compact ( self , data ): res = { k : { c : self . children [ k ] . to_compact ( cv ) for c , cv in v . items ()} for k , v in data . items () if k in self . children } for dp_type in DataPointInfo . SUB_CLASS_NAMES : if dp_type in data : res |= { k : v [ \"value\" ] for k , v in data [ dp_type ] . items ()} return res","title":"Entity"},{"location":"reference/core/model/#tiro.core.model.Entity.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in tiro/core/model.py def __init_subclass__ ( cls , ** kwargs ): super ( Entity , cls ) . __init_subclass__ ( ** kwargs ) cls . data_point_info = {} for c in cls . __mro__ : if issubclass ( c , Entity ): cls . data_point_info |= c . data_point_info cls . child_info : dict [ str , EntityList ] = {} annotations = get_annotations ( cls ) for k , v in annotations . items (): if isinstance ( v , Alias ): v = annotations [ v . origin ] if isinstance ( v , DataPointInfo ): cls . data_point_info [ k ] = v if hasattr ( cls , k ): v . default = getattr ( cls , k ) delattr ( cls , k ) elif isinstance ( v , EntityList ): cls . child_info [ k ] = v if hasattr ( cls , k ): delattr ( cls , k ) cls . cached_data_point_model = {}","title":"__init_subclass__()"},{"location":"reference/core/model/#tiro.core.model.Entity.create","text":"Dynamically create the entity class according to the entity name defined in an asset library Source code in tiro/core/model.py @classmethod def create ( cls , name : str , * entities : Union [ \"Entity\" , Type [ \"Entity\" ]], base_classes : Optional [ list [ str ]] = None , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** entity_dict : dict [ str , Union [ \"Entity\" , Type [ \"Entity\" ]]], ) -> Type [ \"Entity\" ]: \"\"\"Dynamically create the entity class according to the entity name defined in an asset library\"\"\" if base_classes : if asset_library_path and asset_library_path not in sys . path : sys . path . insert ( 0 , asset_library_path ) bases = [] for base_class in base_classes : base_class , _ , base_name = base_class . rpartition ( \".\" ) if not base_class : base = getattr ( import_module ( asset_library_name ), base_name ) else : base = getattr ( import_module ( f \" { asset_library_name } . { base_class } \" ), base_name ) bases . append ( base ) bases = tuple ( bases ) else : bases = ( Entity ,) ann = {} for item in entities : if isinstance ( item , type ) and issubclass ( item , Entity ): item = item . many ( faking_number = 1 ) ann [ item . cls . __name__ ] = item for k , v in entity_dict . items (): if isinstance ( v , type ) and issubclass ( v , Entity ): v = v . many ( faking_number = 1 ) ann [ k ] = v return type ( name , bases , dict ( __annotations__ = ann ))","title":"create()"},{"location":"reference/core/model/#tiro.core.model.Entity.create_from_define_string","text":"Dynamically create the entity from a dictionary containing model infos Source code in tiro/core/model.py @classmethod def create_from_define_string ( cls , name : str , defs : dict , prefix : str = \"\" , asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ) -> EntityList : \"\"\"Dynamically create the entity from a dictionary containing model infos\"\"\" if prefix : name = f \" { prefix } _ { name } \" children = { k : cls . create_from_define_string ( k , v , prefix = name , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } base_classes = defs [ f \" { YAML_META_CHAR } type\" ] if not isinstance ( base_classes , list ): base_classes = [ base_classes ] entity = cls . create ( name , base_classes = base_classes , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** children , ) list_args = {} if f \" { YAML_META_CHAR } number\" in defs : number = defs [ f \" { YAML_META_CHAR } number\" ] if isinstance ( number , str ) and \"-\" in number : min_num , max_num = number . split ( \"-\" ) list_args [ \"faking_number\" ] = partial ( randint , int ( min_num ), int ( max_num ) ) else : list_args [ \"faking_number\" ] = int ( number ) else : list_args [ \"faking_number\" ] = 1 if f \" { YAML_META_CHAR } ids\" in defs : list_args [ \"ids\" ] = defs [ f \" { YAML_META_CHAR } ids\" ] if f \" { YAML_META_CHAR } defaults\" in defs : entity . update_defaults ( ** defs [ f \" { YAML_META_CHAR } defaults\" ]) return entity . many ( ** list_args )","title":"create_from_define_string()"},{"location":"reference/core/model/#tiro.core.model.Entity.many","text":"Return a list of the entities with the same type Source code in tiro/core/model.py @classmethod def many ( cls , * args , ** kwargs ) -> EntityList : \"\"\"Return a list of the entities with the same type\"\"\" return EntityList ( cls , * args , ** kwargs )","title":"many()"},{"location":"reference/core/model/#tiro.core.model.Entity.requires","text":"Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. Source code in tiro/core/model.py def requires ( self , * paths : str , yaml : str | Path = None ) -> \"Entity\" : \"\"\" Mark a series of data points are required in a use case. A path is a string like Scenario.Room.Server.CPUTemperature. Alternatively, a yaml string or path can be provided. \"\"\" paths = list ( paths ) if yaml : if isinstance ( yaml , Path ): yaml = yaml . open () . read () paths . extend ( self . _parse_use_yaml ( safe_load ( yaml ))) for path in paths : self . uses . add ( path ) if PATH_SEP in path : entity , _ , path = path . partition ( PATH_SEP ) if entity not in self . children : self . children [ entity ] = self . child_info [ entity ] . new_entity ( self ) self . children [ entity ] . requires ( path ) else : if path in self . data_point_info : self . _used_data_points . add ( path ) elif path in self . child_info and path not in self . children : self . children [ path ] = self . child_info [ path ] . new_entity ( self ) return self","title":"requires()"},{"location":"reference/core/model/#tiro.core.model.EntityList","text":"Holding the information of a list of entity with the same type Source code in tiro/core/model.py class EntityList : \"\"\"Holding the information of a list of entity with the same type\"\"\" def __init__ ( self , cls : Type [ \"Entity\" ], faking_number : Optional [ Callable | int ] = None , ids : Optional [ list [ str ]] = None , ): self . cls = cls self . ids = ids if self . ids is not None : if faking_number is None : faking_number = len ( self . ids ) elif isinstance ( faking_number , int ) and faking_number > len ( self . ids ): raise RuntimeError ( \"When ids is provided, faking_number must be less than the length of ids.\" ) if isinstance ( faking_number , int ): self . number_faker = lambda : faking_number else : self . number_faker = faking_number def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent )","title":"EntityList"},{"location":"reference/core/model/#tiro.core.model.EntityList.new_entity","text":"Generate an entity instance Source code in tiro/core/model.py def new_entity ( self , parent : Optional [ \"Entity\" ] = None ) -> \"Entity\" : \"\"\"Generate an entity instance\"\"\" return self . cls ( parent )","title":"new_entity()"},{"location":"reference/core/model/#tiro.core.model.RequireHelper","text":"Helper class for requiring children or data points Source code in tiro/core/model.py class RequireHelper : \"\"\"Helper class for requiring children or data points\"\"\" def __init__ ( self , component : str , parent : Union [ \"RequireHelper\" , \"Entity\" ]): self . component = component self . parent = parent @property def path ( self ) -> str : if isinstance ( self . parent , RequireHelper ): return f \" { self . parent . path } . { self . component } \" else : return self . component @property def origin ( self ) -> Union [ \"RequireHelper\" , \"Entity\" ]: if isinstance ( self . parent , RequireHelper ): return self . parent . origin else : return self . parent def __getattr__ ( self , item ): return RequireHelper ( item , self ) def use ( self ) -> \"Entity\" : return self . origin . requires ( self . path )","title":"RequireHelper"},{"location":"reference/core/model/#tiro.core.model.Telemetry","text":"Data point that dynamically changes. Source code in tiro/core/model.py class Telemetry ( DataPointInfo ): \"\"\"Data point that dynamically changes.\"\"\" pass","title":"Telemetry"},{"location":"reference/core/scenario/","text":"Scenario \u00b6 Source code in tiro/core/scenario.py class Scenario : def __init__ ( self , * entities : Entity | Type [ Entity ], asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** kw_entities : dict [ str , Entity | Type [ Entity ]], ): self . root : Entity = Entity . create ( \"Scenario\" , * entities , base_classes = None , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** kw_entities , )() @classmethod def from_yaml ( cls , scenario_data : Path | str , * uses : Path | str ): if isinstance ( scenario_data , Path ): scenario_data = scenario_data . open () . read () defs = safe_load ( scenario_data ) asset_library_path = defs . get ( f \" { YAML_META_CHAR } asset_library_path\" , None ) asset_library_name = defs . get ( f \" { YAML_META_CHAR } asset_library_name\" , \"tiro.assets\" ) ins = cls ( ** { k : Entity . create_from_define_string ( k , v , prefix = \"Scenario\" , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } ) for use in uses : if isinstance ( use , Path ): use = use . open () . read () ins . requires ( yaml = use ) return ins def __getattr__ ( self , key ): return getattr ( self . root , key ) def mocker ( self , * args , ** kwargs ): return Mocker ( self . root , * args , ** kwargs ) def validator ( self , * args , ** kwargs ): return Validator ( self . root , * args , ** kwargs ) @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) @staticmethod def data_point_path_to_path ( path : str | list [ str ]) -> str : path = split_path ( path ) return f \" { PATH_SEP . join ( path [ i ] for i in range ( 0 , len ( path ) - 2 , 2 )) }{ PATH_SEP }{ path [ - 1 ] } \" @classmethod def data_point_path_to_tags ( cls , path : str | list [ str ], tags = None ) -> dict : tags = tags or dict ( path = cls . data_point_path_to_path ( path ), asset_path = path ) path = split_path ( path ) component = path . pop ( 0 ) if component in DataPointInfo . SUB_CLASS_NAMES : tags |= dict ( type = component , field = path [ - 1 ]) else : uuid = path . pop ( 0 ) tags |= { component : uuid } cls . data_point_path_to_tags ( path , tags ) return tags def guess_missing_paths ( self , existing_paths : Optional [ list [ str ]] = None , pattern_or_uses : Optional [ str | dict | Path ] = None , ): validator = self . validator ( validate_path_only = True , require_all_children = False ) if existing_paths : for path in existing_paths : validator . collect ( path , value = {}) res = validator . validate () if isinstance ( pattern_or_uses , dict ) or isinstance ( pattern_or_uses , Path ): valid_paths = set ( decouple_uses ( pattern_or_uses )) else : valid_paths = None while not res . valid : for error in res . exception . errors (): missing_path = PATH_SEP . join ( error [ \"loc\" ]) validator . collect ( missing_path , value = {}) path = self . data_point_path_to_path ( missing_path ) if valid_paths is None : path_is_required = self . path_match ( pattern_or_uses , path ) else : path_is_required = path in valid_paths if path_is_required and self . query_data_point_info ( path ): yield missing_path res = validator . validate () decompose_data ( path , value , info = None ) classmethod \u00b6 Decompose a dict to separate data points. Source code in tiro/core/scenario.py @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info )","title":"scenario"},{"location":"reference/core/scenario/#tiro.core.scenario.Scenario","text":"Source code in tiro/core/scenario.py class Scenario : def __init__ ( self , * entities : Entity | Type [ Entity ], asset_library_path : Optional [ str ] = None , asset_library_name : str = \"tiro.assets\" , ** kw_entities : dict [ str , Entity | Type [ Entity ]], ): self . root : Entity = Entity . create ( \"Scenario\" , * entities , base_classes = None , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ** kw_entities , )() @classmethod def from_yaml ( cls , scenario_data : Path | str , * uses : Path | str ): if isinstance ( scenario_data , Path ): scenario_data = scenario_data . open () . read () defs = safe_load ( scenario_data ) asset_library_path = defs . get ( f \" { YAML_META_CHAR } asset_library_path\" , None ) asset_library_name = defs . get ( f \" { YAML_META_CHAR } asset_library_name\" , \"tiro.assets\" ) ins = cls ( ** { k : Entity . create_from_define_string ( k , v , prefix = \"Scenario\" , asset_library_path = asset_library_path , asset_library_name = asset_library_name , ) for k , v in defs . items () if not k . startswith ( YAML_META_CHAR ) } ) for use in uses : if isinstance ( use , Path ): use = use . open () . read () ins . requires ( yaml = use ) return ins def __getattr__ ( self , key ): return getattr ( self . root , key ) def mocker ( self , * args , ** kwargs ): return Mocker ( self . root , * args , ** kwargs ) def validator ( self , * args , ** kwargs ): return Validator ( self . root , * args , ** kwargs ) @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info ) @staticmethod def data_point_path_to_path ( path : str | list [ str ]) -> str : path = split_path ( path ) return f \" { PATH_SEP . join ( path [ i ] for i in range ( 0 , len ( path ) - 2 , 2 )) }{ PATH_SEP }{ path [ - 1 ] } \" @classmethod def data_point_path_to_tags ( cls , path : str | list [ str ], tags = None ) -> dict : tags = tags or dict ( path = cls . data_point_path_to_path ( path ), asset_path = path ) path = split_path ( path ) component = path . pop ( 0 ) if component in DataPointInfo . SUB_CLASS_NAMES : tags |= dict ( type = component , field = path [ - 1 ]) else : uuid = path . pop ( 0 ) tags |= { component : uuid } cls . data_point_path_to_tags ( path , tags ) return tags def guess_missing_paths ( self , existing_paths : Optional [ list [ str ]] = None , pattern_or_uses : Optional [ str | dict | Path ] = None , ): validator = self . validator ( validate_path_only = True , require_all_children = False ) if existing_paths : for path in existing_paths : validator . collect ( path , value = {}) res = validator . validate () if isinstance ( pattern_or_uses , dict ) or isinstance ( pattern_or_uses , Path ): valid_paths = set ( decouple_uses ( pattern_or_uses )) else : valid_paths = None while not res . valid : for error in res . exception . errors (): missing_path = PATH_SEP . join ( error [ \"loc\" ]) validator . collect ( missing_path , value = {}) path = self . data_point_path_to_path ( missing_path ) if valid_paths is None : path_is_required = self . path_match ( pattern_or_uses , path ) else : path_is_required = path in valid_paths if path_is_required and self . query_data_point_info ( path ): yield missing_path res = validator . validate ()","title":"Scenario"},{"location":"reference/core/scenario/#tiro.core.scenario.Scenario.decompose_data","text":"Decompose a dict to separate data points. Source code in tiro/core/scenario.py @classmethod def decompose_data ( cls , path : str | list [ str ], value : dict , info = None ) -> Generator [ dict , None , None ]: \"\"\"Decompose a dict to separate data points.\"\"\" path = split_path ( path ) info = info or dict ( path = \"\" , asset_path = \"\" ) pre_path = info [ \"path\" ] pre_asset_path = info [ \"asset_path\" ] len_prefix = len ( path ) data_point_types = DataPointInfo . SUB_CLASS_NAMES if len_prefix == 0 : for k , v in value . items (): if k in data_point_types : for sub_k , sub_v in v . items (): yield info | dict ( type = k , field = sub_k , path = concat_path ( pre_path , snake_to_camel ( sub_k )), ) | ( sub_v if isinstance ( sub_v , dict ) else dict ( value = sub_v )) elif \"type\" in info : yield info | dict ( field = k ) else : for sub_k , sub_v in v . items (): _info = ( info | { k : sub_k } | dict ( path = concat_path ( pre_path , snake_to_camel ( k )), asset_path = concat_path ( pre_asset_path , snake_to_camel ( k ), sub_k ), ) ) yield from cls . decompose_data ( path , sub_v , _info ) elif len_prefix == 1 : for name in data_point_types : if path [ 0 ] == name : for k , v in value . items (): yield info | dict ( type = name , field = k , path = concat_path ( pre_path , snake_to_camel ( k )), ) | v return for k , v in value . items (): _info = ( info | { snake_to_camel ( path [ 0 ]): k } | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), k ), ) ) yield from cls . decompose_data ( path [ 1 :], v , _info ) else : for name in data_point_types : if path [ 0 ] == name : yield info | dict ( type = name , field = path [ 1 ], path = concat_path ( pre_path , snake_to_camel ( path [ 1 ])), ) | value return _info = ( info | { snake_to_camel ( path [ 0 ]): path [ 1 ]} | dict ( path = concat_path ( pre_path , snake_to_camel ( path [ 0 ])), asset_path = concat_path ( pre_asset_path , snake_to_camel ( path [ 0 ]), path [ 1 ] ), ) ) yield from cls . decompose_data ( path [ 2 :], value , _info )","title":"decompose_data()"},{"location":"reference/core/utils/","text":"","title":"utils"},{"location":"reference/core/validate/","text":"ValidationResult dataclass \u00b6 ValidationResult(start: datetime.datetime, end: datetime.datetime, valid: bool, exception: Union[jsonschema.exceptions.ValidationError, pydantic.error_wrappers.ValidationError, NoneType]) Source code in tiro/core/validate.py @dataclass class ValidationResult : start : datetime end : datetime valid : bool exception : Optional [ JSONSchemaValidatorError | PydanticValidationError ] def __str__ ( self ): msg = f \"Validation Period: { self . start } -- { self . end } \\n \" if self . valid : msg += \"Successful!\" else : msg += f \"Failed! \\n { str ( self . exception ) } \" return msg def json ( self ) -> str : return json . dumps ( self . info ()) def info ( self ) -> dict : return dict ( start = self . start . isoformat (), end = self . end . isoformat (), valid = self . valid , exception = self . serialise_exception (), ) def serialise_exception ( self ) -> Optional [ dict ]: if self . exception is None : return None elif isinstance ( self . exception , PydanticValidationError ): return self . exception . errors () elif isinstance ( self . exception , JSONSchemaValidatorError ): return dict ( message = self . exception . message , path = self . exception . json_path , description = str ( self . exception ), ) Validator \u00b6 Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. Source code in tiro/core/validate.py class Validator : \"\"\" Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. \"\"\" def __init__ ( self , entity : Entity = None , schema : dict = None , retention : int = 0 , log : bool = True , log_size : int = 100 , validate_path_only : bool = False , require_all_children : bool = True , ): if entity : self . model : Type [ BaseModel ] = entity . model ( hide_dp_values = validate_path_only , require_all_children = require_all_children , ) self . schema = None else : self . model = None self . schema = schema self . _data = {} self . retention : Optional [ timedelta ] = timedelta ( seconds = retention if retention > 0 else 1e9 ) self . data_create_time : datetime = datetime . now () self . log : deque [ ValidationResult ] = deque ( maxlen = log_size if log else 1 ) self . _collect_count = 0 def reset_data ( self ) -> None : self . _collect_count = 0 self . _data = {} if self . retention : self . data_create_time = datetime . now () def __enter__ ( self ): self . reset_data () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . reset_data () def validate_retention ( self ): if datetime . now () - self . data_create_time > self . retention : self . validate () self . reset_data () def collect ( self , path : str , value : Any ): self . validate_retention () insert_data_point_to_dict ( path , value , self . _data ) self . _collect_count += 1 def validate ( self ) -> ValidationResult : period_start = self . data_create_time period_end = datetime . now () if period_end - period_start > self . retention : period_end = period_start + self . retention try : if self . model : self . model . parse_obj ( self . _data ) elif self . schema : validate ( instance = self . _data , schema = self . schema ) res = ValidationResult ( period_start , period_end , True , None ) except Exception as e : res = ValidationResult ( period_start , period_end , False , e ) if self . log and self . log [ 0 ] . start == period_start : self . log [ 0 ] = res else : self . log . appendleft ( res ) return res def validate_dict ( self , content : dict ): self . _data = content return self . validate () @property def last_validation_start_time ( self ): if self . log : return self . log [ 0 ] . start else : return datetime . min @property def last_result ( self ): if self . log : return self . log [ 0 ] else : return None @property def current_collection_size ( self ): return self . _collect_count","title":"validate"},{"location":"reference/core/validate/#tiro.core.validate.ValidationResult","text":"ValidationResult(start: datetime.datetime, end: datetime.datetime, valid: bool, exception: Union[jsonschema.exceptions.ValidationError, pydantic.error_wrappers.ValidationError, NoneType]) Source code in tiro/core/validate.py @dataclass class ValidationResult : start : datetime end : datetime valid : bool exception : Optional [ JSONSchemaValidatorError | PydanticValidationError ] def __str__ ( self ): msg = f \"Validation Period: { self . start } -- { self . end } \\n \" if self . valid : msg += \"Successful!\" else : msg += f \"Failed! \\n { str ( self . exception ) } \" return msg def json ( self ) -> str : return json . dumps ( self . info ()) def info ( self ) -> dict : return dict ( start = self . start . isoformat (), end = self . end . isoformat (), valid = self . valid , exception = self . serialise_exception (), ) def serialise_exception ( self ) -> Optional [ dict ]: if self . exception is None : return None elif isinstance ( self . exception , PydanticValidationError ): return self . exception . errors () elif isinstance ( self . exception , JSONSchemaValidatorError ): return dict ( message = self . exception . message , path = self . exception . json_path , description = str ( self . exception ), )","title":"ValidationResult"},{"location":"reference/core/validate/#tiro.core.validate.Validator","text":"Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. Source code in tiro/core/validate.py class Validator : \"\"\" Validator receives data points and validate the JSON combined from all received data points in a short period against the given scenario or JSON schema. \"\"\" def __init__ ( self , entity : Entity = None , schema : dict = None , retention : int = 0 , log : bool = True , log_size : int = 100 , validate_path_only : bool = False , require_all_children : bool = True , ): if entity : self . model : Type [ BaseModel ] = entity . model ( hide_dp_values = validate_path_only , require_all_children = require_all_children , ) self . schema = None else : self . model = None self . schema = schema self . _data = {} self . retention : Optional [ timedelta ] = timedelta ( seconds = retention if retention > 0 else 1e9 ) self . data_create_time : datetime = datetime . now () self . log : deque [ ValidationResult ] = deque ( maxlen = log_size if log else 1 ) self . _collect_count = 0 def reset_data ( self ) -> None : self . _collect_count = 0 self . _data = {} if self . retention : self . data_create_time = datetime . now () def __enter__ ( self ): self . reset_data () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . reset_data () def validate_retention ( self ): if datetime . now () - self . data_create_time > self . retention : self . validate () self . reset_data () def collect ( self , path : str , value : Any ): self . validate_retention () insert_data_point_to_dict ( path , value , self . _data ) self . _collect_count += 1 def validate ( self ) -> ValidationResult : period_start = self . data_create_time period_end = datetime . now () if period_end - period_start > self . retention : period_end = period_start + self . retention try : if self . model : self . model . parse_obj ( self . _data ) elif self . schema : validate ( instance = self . _data , schema = self . schema ) res = ValidationResult ( period_start , period_end , True , None ) except Exception as e : res = ValidationResult ( period_start , period_end , False , e ) if self . log and self . log [ 0 ] . start == period_start : self . log [ 0 ] = res else : self . log . appendleft ( res ) return res def validate_dict ( self , content : dict ): self . _data = content return self . validate () @property def last_validation_start_time ( self ): if self . log : return self . log [ 0 ] . start else : return datetime . min @property def last_result ( self ): if self . log : return self . log [ 0 ] else : return None @property def current_collection_size ( self ): return self . _collect_count","title":"Validator"},{"location":"reference/plugins/","text":"karez special \u00b6 connector \u00b6 ConnectorForMockServer ( RestfulConnectorBase ) \u00b6 Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result fetch_data ( self , client , entities ) async \u00b6 Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result dispatcher \u00b6 DispatcherForMockServer ( DispatcherBase ) \u00b6 Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json () load_entities ( self ) async \u00b6 Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"plugins"},{"location":"reference/plugins/#tiro.plugins.karez","text":"","title":"karez"},{"location":"reference/plugins/#tiro.plugins.karez.connector","text":"","title":"connector"},{"location":"reference/plugins/#tiro.plugins.karez.connector.ConnectorForMockServer","text":"Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"ConnectorForMockServer"},{"location":"reference/plugins/#tiro.plugins.karez.connector.ConnectorForMockServer.fetch_data","text":"Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"fetch_data()"},{"location":"reference/plugins/#tiro.plugins.karez.dispatcher","text":"","title":"dispatcher"},{"location":"reference/plugins/#tiro.plugins.karez.dispatcher.DispatcherForMockServer","text":"Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"DispatcherForMockServer"},{"location":"reference/plugins/#tiro.plugins.karez.dispatcher.DispatcherForMockServer.load_entities","text":"Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"load_entities()"},{"location":"reference/plugins/utinni/","text":"","title":"utinni"},{"location":"reference/plugins/graph/","text":"","title":"graph"},{"location":"reference/plugins/graph/agent/","text":"","title":"agent"},{"location":"reference/plugins/graph/aql/","text":"","title":"aql"},{"location":"reference/plugins/graph/qpath/","text":"","title":"qpath"},{"location":"reference/plugins/karez/","text":"connector \u00b6 ConnectorForMockServer ( RestfulConnectorBase ) \u00b6 Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result fetch_data ( self , client , entities ) async \u00b6 Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result dispatcher \u00b6 DispatcherForMockServer ( DispatcherBase ) \u00b6 Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json () load_entities ( self ) async \u00b6 Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"karez"},{"location":"reference/plugins/karez/#tiro.plugins.karez.connector","text":"","title":"connector"},{"location":"reference/plugins/karez/#tiro.plugins.karez.connector.ConnectorForMockServer","text":"Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"ConnectorForMockServer"},{"location":"reference/plugins/karez/#tiro.plugins.karez.connector.ConnectorForMockServer.fetch_data","text":"Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"fetch_data()"},{"location":"reference/plugins/karez/#tiro.plugins.karez.dispatcher","text":"","title":"dispatcher"},{"location":"reference/plugins/karez/#tiro.plugins.karez.dispatcher.DispatcherForMockServer","text":"Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"DispatcherForMockServer"},{"location":"reference/plugins/karez/#tiro.plugins.karez.dispatcher.DispatcherForMockServer.load_entities","text":"Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"load_entities()"},{"location":"reference/plugins/karez/aggregator/","text":"","title":"aggregator"},{"location":"reference/plugins/karez/connector/","text":"ConnectorForMockServer ( RestfulConnectorBase ) \u00b6 Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result fetch_data ( self , client , entities ) async \u00b6 Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"connector"},{"location":"reference/plugins/karez/connector/#tiro.plugins.karez.connector.ConnectorForMockServer","text":"Source code in tiro/plugins/karez/connector.py class ConnectorForMockServer ( RestfulConnectorBase ): @classmethod def role_description ( cls ): return \"Connector to fetch telemetries and attributes from a Tiro mock server.\" @classmethod def config_entities ( cls ): yield from super ( ConnectorForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"ConnectorForMockServer"},{"location":"reference/plugins/karez/connector/#tiro.plugins.karez.connector.ConnectorForMockServer.fetch_data","text":"Fetch data from external sources. Parameters: Name Type Description Default client AsyncClient a client object that can be used to fetch data, e.g. a https client. Can be None if not needed. required entities a list of entities to be fetched. It can be in any format, e.g. a list of ids, a list of dicts, etc. required Returns: Type Description a list of fetched data. Each item will be passed to the next converter or aggregator. Source code in tiro/plugins/karez/connector.py async def fetch_data ( self , client : httpx . AsyncClient , entities ): result = [] for entity in entities : if self . config . by == \"path\" : r = await client . get ( f \"/points/ { entity } \" ) if r . status_code == httpx . codes . OK : data = dict ( path = entity , result = r . json ()) result . append ( self . update_meta ( data , category = entity . split ( \".\" )[ - 2 ] . lower ()) ) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /points/ { entity } \" ) elif self . config . by == \"uuid\" : r = await client . get ( f \"/values/ { entity } \" ) if r . status_code == httpx . codes . OK : result . append ( dict ( name = entity , value = r . json ())) else : logging . error ( f \" { self . __class__ . __name__ } [ { self . name } ] request error { r . status_code } : /values/ { entity } \" ) else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return result","title":"fetch_data()"},{"location":"reference/plugins/karez/converter/","text":"","title":"converter"},{"location":"reference/plugins/karez/dispatcher/","text":"DispatcherForMockServer ( DispatcherBase ) \u00b6 Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json () load_entities ( self ) async \u00b6 Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"dispatcher"},{"location":"reference/plugins/karez/dispatcher/#tiro.plugins.karez.dispatcher.DispatcherForMockServer","text":"Source code in tiro/plugins/karez/dispatcher.py class DispatcherForMockServer ( DispatcherBase ): @classmethod def role_description ( cls ) -> str : return \"Dispatcher for Tiro Mock server\" @classmethod def config_entities ( cls ): yield from super ( DispatcherForMockServer , cls ) . config_entities () yield OptionalConfigEntity ( \"base_url\" , \"http://localhost:8000\" , \"URL of the Tiro Mock Server\" ) yield OptionalConfigEntity ( \"by\" , \"path\" , \"Access data by path or uuid? (path, uuid)\" ) async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"DispatcherForMockServer"},{"location":"reference/plugins/karez/dispatcher/#tiro.plugins.karez.dispatcher.DispatcherForMockServer.load_entities","text":"Load entities from somewhere. It can be a local file, a database, or a remote API. Returns: Type Description list A list of entities. An entity can be anything that can be passed to a connector, for example, a device ID. Source code in tiro/plugins/karez/dispatcher.py async def load_entities ( self ) -> list : if self . config . by == \"path\" : url = f \" { self . config . base_url } /points/\" elif self . config . by == \"uuid\" : url = f \" { self . config . base_url } /values/\" else : raise ValueError ( f 'Config entity \"by\" can only be \"path\" or \"uuid\"' ) return httpx . get ( url ) . json ()","title":"load_entities()"},{"location":"topics/asset_library/","text":"Asset Library \u00b6 We have created a library of assets common used in data centers. One can directly use the assets in scenario definitions. The library is available in the Tiro asset library repository. The library is currently in a private repository. To access the library, please contact us.","title":"Asset Library"},{"location":"topics/asset_library/#asset-library","text":"We have created a library of assets common used in data centers. One can directly use the assets in scenario definitions. The library is available in the Tiro asset library repository. The library is currently in a private repository. To access the library, please contact us.","title":"Asset Library"},{"location":"topics/data_collection_protocol/","text":"Date Collection Protocol \u00b6 In Get Started , we build a data collection that retrieves data from a mocking service. In real system, we just need to replace the mocking service with the real data collector, aka. the Karez dispatcher and connector roles. The Karez connector can be easily integrated into the workflow as long as it follows the data collection protocol. Data Collection Protocol \u00b6 There are two kinds of data formats that a Karez connector can send to NATS queue: By Path \u00b6 A Path is a string like DataHall.datahall_0.Rack.rack_0.Server.server_0.CPUTemperature , which encodes the path of the data point from the root asset to the data point, separated by . . This can be used to locate the data point in the scenario. The first data format is a JSON object with the following fields: { \"path\" : \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.SupplyTemperature\" , \"result\" :{ \"value\" : 9.31 , \"timestamp\" : \"2022-09-28T11:06:35.050694\" }, \"_karez\" :{ \"category\" : \"telemetry\" } } If a Karez connector sends such data to the NATS queue, it can be directly pipelined to a TiroPreprocessConverter and following tasks. Note The _karez field is reserved for internal use. It is used to indicate the category of the data point, which can be telemetry or attribute . The telemetry category is used for data points that can be continuously updated, like temperature, pressure, etc. The attribute category is used for data points that can only be updated once, like the status of a switch, the status of a door, etc. This filed can be omitted. If omitted, the data point will be treated as a telemetry data point. Py ID \u00b6 The second data format is a simple JSON format with only two fields name and value . This format may be more convenient for some data collectors, like the OPC-UA connector. In realistic scenarios, the data collector may be a third-party system, which may not be able to provide the path of the data point. In this case, the data collector can provide the name of the data point, which is the unique identifier of the data point in the scenario. The value field is the value of the data point. { \"name\" : \"some_uuid\" , \"value\" : 9.31 } If use this case, another TiroUpdateInfoForValueConverter converter needs to be added before TiroPreprocessConverter to add more metadata to the data point. A sample configuration of this converter is as follows: converters : - type : tiro_update_info_for_value reference : scenario/reference.yaml scenario : scenario/scenario.yaml uses : scenario/uses.yaml tz_infos : SGT : Aisa/Singapore next : - tiro_preprocess - update_category_for_validator Note that it needs and additional reference.yaml file to provide the mapping from the name to the path of the data point. Please refer to this section for more information.","title":"Data Collection Protocol"},{"location":"topics/data_collection_protocol/#date-collection-protocol","text":"In Get Started , we build a data collection that retrieves data from a mocking service. In real system, we just need to replace the mocking service with the real data collector, aka. the Karez dispatcher and connector roles. The Karez connector can be easily integrated into the workflow as long as it follows the data collection protocol.","title":"Date Collection Protocol"},{"location":"topics/data_collection_protocol/#data-collection-protocol","text":"There are two kinds of data formats that a Karez connector can send to NATS queue:","title":"Data Collection Protocol"},{"location":"topics/data_collection_protocol/#by-path","text":"A Path is a string like DataHall.datahall_0.Rack.rack_0.Server.server_0.CPUTemperature , which encodes the path of the data point from the root asset to the data point, separated by . . This can be used to locate the data point in the scenario. The first data format is a JSON object with the following fields: { \"path\" : \"DataHall.data_hall_0.CRAC.crac_0.Telemetry.SupplyTemperature\" , \"result\" :{ \"value\" : 9.31 , \"timestamp\" : \"2022-09-28T11:06:35.050694\" }, \"_karez\" :{ \"category\" : \"telemetry\" } } If a Karez connector sends such data to the NATS queue, it can be directly pipelined to a TiroPreprocessConverter and following tasks. Note The _karez field is reserved for internal use. It is used to indicate the category of the data point, which can be telemetry or attribute . The telemetry category is used for data points that can be continuously updated, like temperature, pressure, etc. The attribute category is used for data points that can only be updated once, like the status of a switch, the status of a door, etc. This filed can be omitted. If omitted, the data point will be treated as a telemetry data point.","title":"By Path"},{"location":"topics/data_collection_protocol/#py-id","text":"The second data format is a simple JSON format with only two fields name and value . This format may be more convenient for some data collectors, like the OPC-UA connector. In realistic scenarios, the data collector may be a third-party system, which may not be able to provide the path of the data point. In this case, the data collector can provide the name of the data point, which is the unique identifier of the data point in the scenario. The value field is the value of the data point. { \"name\" : \"some_uuid\" , \"value\" : 9.31 } If use this case, another TiroUpdateInfoForValueConverter converter needs to be added before TiroPreprocessConverter to add more metadata to the data point. A sample configuration of this converter is as follows: converters : - type : tiro_update_info_for_value reference : scenario/reference.yaml scenario : scenario/scenario.yaml uses : scenario/uses.yaml tz_infos : SGT : Aisa/Singapore next : - tiro_preprocess - update_category_for_validator Note that it needs and additional reference.yaml file to provide the mapping from the name to the path of the data point. Please refer to this section for more information.","title":"Py ID"},{"location":"topics/scenario_from_snapshot/","text":"Drafting Scenario from Snapshot \u00b6 In Get Started , we create a scenario from scratch. However, it can still be tedious. Therefore, Tiro also provides tools to help you draft the scenario and uses files from a \"snapshot\" of the system. Snapshot \u00b6 A \"snapshot\" is a csv file that contains the list of data points and their values at a specific time, which may be exported from a system by some external tools. The csv file must include the following columns: data_point : the name of the data point, like \"temperature\" or \"pressure\"; asset : the name of the asset that contains the data point, like \"room1\" or \"rack1\"; asset_type : the type of the asset, like \"Room\" or \"Rack\"; parent_asset : the name of the parent asset, for example, the parent asset of \"rack1\" may be \"room1\", as \"rack1\" is installed in \"room1\"; value : the value of the data point, like \"23.5\" or \"true\". and an optional column uuid , which is the unique identifier of the data point. If the uuid column is not provided, Tiro will generate a UUID for each data point using the asset and data_point columns. Below is the content of an example snapshot file snapshot.csv : snapshot.csv data_point asset asset_type parent_asset value Supply Temperature crac_1 CRAC data_hall 15 Supply Temperature crac_2 CRAC data_hall 17 Supply Temperature crac_3 CRAC data_hall 16.5 Supply Temperature crac_4 CRAC data_hall 15 CPU Temperature server_1 Server rack 80 CPU Temperature server_2 Server rack 60 CPU Temperature server_3 Server rack 67 CPU Temperature server_4 Server rack 55 Active Power rack Rack data_hall 10000 Active Power crac_1 CRAC data_hall 3000 Active Power crac_2 CRAC data_hall 4000 Active Power crac_3 CRAC data_hall 6000 Active Power crac_4 CRAC data_hall 4300 Temperature data_hall DataHall 23 Drafting Scenario \u00b6 With the snapshot file, we can draft the scenario by running the following command: $ tiro draft scenario snapshot.csv -o scenario_draft.yaml The resulting scenario draft file scenario_draft.yaml will be like: scenario_draft.yaml DataHall : $number : 1 $type : DataHall CRAC : $number : 4 $type : CRAC Rack : $number : 1 $type : Rack Server : $number : 4 $type : Server Then, we can edit the scenario draft file to add more details, like correcting the type of the data points and adding the asset library name or path. Drafting Uses File \u00b6 Run the following command to draft the uses file: $ tiro draft uses snapshot.csv -o uses.yaml The resulting uses draft file uses.yaml will be like: uses_draft.yaml - DataHall : - CRAC : - ActivePower - SupplyTemperature - Rack : - ActivePower - Server : - CPUTemperature - Temperature The files can then be distributed to different service or application developers, and each developer can delete the data points that are not needed. Drafting Reference File \u00b6 Additional, to better mimic the real system, Tiro also supports to appoint a reference file when generating mock data. The reference file contains the real asset names and relations, reference values of the data points and some additional information. So, when this file is provided, the mock data generator will try to follow the actual assets and value ranges in the reference file to make the generated data more realistic. Tiro also provides a command to draft the reference file from the snapshot file: $ tiro draft reference snapshot.csv -o reference.yaml reference.yaml tree : DataHall : data_hall : CRAC : crac_1 : DataPoints : - SupplyTemperature - ActivePower crac_2 : DataPoints : - SupplyTemperature - ActivePower crac_3 : DataPoints : - SupplyTemperature - ActivePower crac_4 : DataPoints : - SupplyTemperature - ActivePower DataPoints : - Temperature Rack : rack : DataPoints : - ActivePower Server : server_1 : DataPoints : - CPUTemperature server_2 : DataPoints : - CPUTemperature server_3 : DataPoints : - CPUTemperature server_4 : DataPoints : - CPUTemperature uuid_map : crac_1.ActivePower : DataHall.data_hall.CRAC.crac_1.ActivePower crac_1.SupplyTemperature : DataHall.data_hall.CRAC.crac_1.SupplyTemperature crac_2.ActivePower : DataHall.data_hall.CRAC.crac_2.ActivePower crac_2.SupplyTemperature : DataHall.data_hall.CRAC.crac_2.SupplyTemperature crac_3.ActivePower : DataHall.data_hall.CRAC.crac_3.ActivePower crac_3.SupplyTemperature : DataHall.data_hall.CRAC.crac_3.SupplyTemperature crac_4.ActivePower : DataHall.data_hall.CRAC.crac_4.ActivePower crac_4.SupplyTemperature : DataHall.data_hall.CRAC.crac_4.SupplyTemperature data_hall.Temperature : DataHall.data_hall.Temperature rack.ActivePower : DataHall.data_hall.Rack.rack.ActivePower server_1.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_1.CPUTemperature server_2.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_2.CPUTemperature server_3.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_3.CPUTemperature server_4.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_4.CPUTemperature value_range : DataHall.CRAC.ActivePower : max : 6000.0 min : 3000.0 DataHall.CRAC.SupplyTemperature : max : 17.0 min : 15.0 DataHall.Rack.ActivePower : max : 10000.0 min : 10000.0 DataHall.Rack.Server.CPUTemperature : max : 80.0 min : 55.0 DataHall.Temperature : max : 23.0 min : 23.0 Having the reference file, we can generate more realistic mock data with the following command: $ tiro mock serve scenario.yaml use-srv1.yaml use-srv2.yaml -r reference.yaml","title":"Scenario from Snapshot"},{"location":"topics/scenario_from_snapshot/#drafting-scenario-from-snapshot","text":"In Get Started , we create a scenario from scratch. However, it can still be tedious. Therefore, Tiro also provides tools to help you draft the scenario and uses files from a \"snapshot\" of the system.","title":"Drafting Scenario from Snapshot"},{"location":"topics/scenario_from_snapshot/#snapshot","text":"A \"snapshot\" is a csv file that contains the list of data points and their values at a specific time, which may be exported from a system by some external tools. The csv file must include the following columns: data_point : the name of the data point, like \"temperature\" or \"pressure\"; asset : the name of the asset that contains the data point, like \"room1\" or \"rack1\"; asset_type : the type of the asset, like \"Room\" or \"Rack\"; parent_asset : the name of the parent asset, for example, the parent asset of \"rack1\" may be \"room1\", as \"rack1\" is installed in \"room1\"; value : the value of the data point, like \"23.5\" or \"true\". and an optional column uuid , which is the unique identifier of the data point. If the uuid column is not provided, Tiro will generate a UUID for each data point using the asset and data_point columns. Below is the content of an example snapshot file snapshot.csv : snapshot.csv data_point asset asset_type parent_asset value Supply Temperature crac_1 CRAC data_hall 15 Supply Temperature crac_2 CRAC data_hall 17 Supply Temperature crac_3 CRAC data_hall 16.5 Supply Temperature crac_4 CRAC data_hall 15 CPU Temperature server_1 Server rack 80 CPU Temperature server_2 Server rack 60 CPU Temperature server_3 Server rack 67 CPU Temperature server_4 Server rack 55 Active Power rack Rack data_hall 10000 Active Power crac_1 CRAC data_hall 3000 Active Power crac_2 CRAC data_hall 4000 Active Power crac_3 CRAC data_hall 6000 Active Power crac_4 CRAC data_hall 4300 Temperature data_hall DataHall 23","title":"Snapshot"},{"location":"topics/scenario_from_snapshot/#drafting-scenario","text":"With the snapshot file, we can draft the scenario by running the following command: $ tiro draft scenario snapshot.csv -o scenario_draft.yaml The resulting scenario draft file scenario_draft.yaml will be like: scenario_draft.yaml DataHall : $number : 1 $type : DataHall CRAC : $number : 4 $type : CRAC Rack : $number : 1 $type : Rack Server : $number : 4 $type : Server Then, we can edit the scenario draft file to add more details, like correcting the type of the data points and adding the asset library name or path.","title":"Drafting Scenario"},{"location":"topics/scenario_from_snapshot/#drafting-uses-file","text":"Run the following command to draft the uses file: $ tiro draft uses snapshot.csv -o uses.yaml The resulting uses draft file uses.yaml will be like: uses_draft.yaml - DataHall : - CRAC : - ActivePower - SupplyTemperature - Rack : - ActivePower - Server : - CPUTemperature - Temperature The files can then be distributed to different service or application developers, and each developer can delete the data points that are not needed.","title":"Drafting Uses File"},{"location":"topics/scenario_from_snapshot/#drafting-reference-file","text":"Additional, to better mimic the real system, Tiro also supports to appoint a reference file when generating mock data. The reference file contains the real asset names and relations, reference values of the data points and some additional information. So, when this file is provided, the mock data generator will try to follow the actual assets and value ranges in the reference file to make the generated data more realistic. Tiro also provides a command to draft the reference file from the snapshot file: $ tiro draft reference snapshot.csv -o reference.yaml reference.yaml tree : DataHall : data_hall : CRAC : crac_1 : DataPoints : - SupplyTemperature - ActivePower crac_2 : DataPoints : - SupplyTemperature - ActivePower crac_3 : DataPoints : - SupplyTemperature - ActivePower crac_4 : DataPoints : - SupplyTemperature - ActivePower DataPoints : - Temperature Rack : rack : DataPoints : - ActivePower Server : server_1 : DataPoints : - CPUTemperature server_2 : DataPoints : - CPUTemperature server_3 : DataPoints : - CPUTemperature server_4 : DataPoints : - CPUTemperature uuid_map : crac_1.ActivePower : DataHall.data_hall.CRAC.crac_1.ActivePower crac_1.SupplyTemperature : DataHall.data_hall.CRAC.crac_1.SupplyTemperature crac_2.ActivePower : DataHall.data_hall.CRAC.crac_2.ActivePower crac_2.SupplyTemperature : DataHall.data_hall.CRAC.crac_2.SupplyTemperature crac_3.ActivePower : DataHall.data_hall.CRAC.crac_3.ActivePower crac_3.SupplyTemperature : DataHall.data_hall.CRAC.crac_3.SupplyTemperature crac_4.ActivePower : DataHall.data_hall.CRAC.crac_4.ActivePower crac_4.SupplyTemperature : DataHall.data_hall.CRAC.crac_4.SupplyTemperature data_hall.Temperature : DataHall.data_hall.Temperature rack.ActivePower : DataHall.data_hall.Rack.rack.ActivePower server_1.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_1.CPUTemperature server_2.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_2.CPUTemperature server_3.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_3.CPUTemperature server_4.CPUTemperature : DataHall.data_hall.Rack.rack.Server.server_4.CPUTemperature value_range : DataHall.CRAC.ActivePower : max : 6000.0 min : 3000.0 DataHall.CRAC.SupplyTemperature : max : 17.0 min : 15.0 DataHall.Rack.ActivePower : max : 10000.0 min : 10000.0 DataHall.Rack.Server.CPUTemperature : max : 80.0 min : 55.0 DataHall.Temperature : max : 23.0 min : 23.0 Having the reference file, we can generate more realistic mock data with the following command: $ tiro mock serve scenario.yaml use-srv1.yaml use-srv2.yaml -r reference.yaml","title":"Drafting Reference File"},{"location":"topics/utinni_querying_guide/","text":"Query Guide \u00b6 This guide explains how to query data from using Tiro-provided Utinni data pump in more details. Query String \u00b6 The most common way to query data is to use the query string. The query string is a string to match the \"type path\" of the data point. The \"type path\" is the path to the data point in the scenario tree, but includes only the asset type names but no asset names. For example, the \"type path\" of the data point DataHall.data_hall.CRAC.crac_1.Telemetry.SupplyTemperature is DataHall.CRAC.SupplyTemperature . A query string is basically a regular expression. However, because . is a special character in regular expression, we use % to represent . in the query string. For example, the query string DataHall%CRAC%SupplyTemperature matches the data point DataHall.CRAC.SupplyTemperature . Also, we introduce %% to match any number of characters including . . For example, the query string DataHall%%Temperature matches the data point DataHall.CRAC.SupplyTemperature and DataHall.Rack.CPUTemperature . The table definition of the Tiro Pump has a unified interface context . tiro_table ( query_str , type , ** kwargs ) where query_str is the query string, and type can be historian or status . Querying Historian Data \u00b6 When passing type=\"historian\" to the table constructor, the table will query historian data. The table will always be \"3-D\" table, that is, the value is a dictionary of pandas.DataFreame indexed by the timestamp. The keys of the dictionary are the data point names. The columns of the pandas.DataFreame are the data point names. For example, the following query will return a dictionary of two tabels. >>> context . bind ( start =- timedelta ( hours = 1 ), step = timedelta ( minutes = 30 )) >>> context . tiro_table ( \" %% CRAC %% Temperature\" , type = \"historian\" ) . value { 'ReturnTemperature': DataHall.data_hall_0.CRAC.crac_0 2022-09-28 12:30:00+08:00 NaN 2022-09-28 13:00:00+08:00 46.85 2022-09-28 13:30:00+08:00 43.84, 'SupplyTemperature': DataHall.data_hall_0.CRAC.crac_0 2022-09-28 12:30:00+08:00 NaN 2022-09-28 13:00:00+08:00 45.52 2022-09-28 13:30:00+08:00 1.42 } There is an additional parameter column can be passed to the table constructor, which can be \"asset_path\" or a asset type like \"Rack\" . By default, it is asset_path , which means the full path of the data point, like DataHall.data_hall.CRAC.crac_1.Telemetry.SupplyTemperature . In this case the returned dataframe's column will be the full path of the data point. Otherwise, if column is set to an asset type, the name of the asset of the appointed type which relates to the data point will be used as the column name. For example, if column is set to \"Rack\" , and the query string is %%Rack%ActivePower , the returned data frame will be a dataframe whose column is different rack names, and the value is corresponding active powers When passing the column parameter, if there are multiple columns which have the same name, they will be aggregated, and the aggregation function and additional keyword arguments can be passed to the table constructor by asset_agg_fn and asser_agg_fn_kwargs , respectively. For example, the following query will return total flow rate of each rack. The default aggregation function is mean , and the default aggregation function keyword arguments is {} . >>> context . bind ( start =- timedelta ( hours = 1 ), step = timedelta ( minutes = 30 )) >>> context . tiro_table ( >>> \" %% Rack %% FlowRate\" , >>> type = \"historian\" , >>> column = \"Rack\" , >>> asset_agg_fn = \"sum\" , >>> )[ \"FlowRate\" ] . value rack_0 rack_1 2022-09-28 12:30:00+08:00 0.00 0.00 2022-09-28 13:00:00+08:00 2150.16 2649.47 2022-09-28 13:30:00+08:00 1922.42 1848.53 Querying Status Data \u00b6 When passing type=\"status\" to the table constructor, the table will query status data. Additional parameter as_data_frame can be passed to the table constructor, which can be True or False . By default, it is False . In this case, the returned table will contain an embedded table. If as_data_frame is set to True , Tiro will try to convert the embedded table to a pandas.DataFreame . >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = False , >>> ) . value { 'DataHall': { 'data_hall_0': { 'CRAC': { 'crac_0': { 'Telemetry': {'ActivePower': {'timestamp': 1664344487.591439, 'unit': None, 'value': 665.63}} } }, 'Rack': { 'rack_0': { 'Telemetry': {'ActivePower': {'timestamp': 1664344487.941468, 'unit': None, 'value': 648.4}} }, 'rack_1': { 'Telemetry': {'ActivePower': {'timestamp': 1664344487.672051, 'unit': None, 'value': 575.49}} } } } } } >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = True , >>> )[ \"ActivePower\" ] . value { 'ActivePower': path DataHall \\ asset_path DataHall.data_hall_0.CRAC.crac_0 DataHall.CRAC.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_0 DataHall.Rack.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_1 DataHall.Rack.ActivePower data_hall_0 CRAC type timestamp unit \\ asset_path DataHall.data_hall_0.CRAC.crac_0 crac_0 Telemetry 1.664345e+09 None DataHall.data_hall_0.Rack.rack_0 NaN Telemetry 1.664345e+09 None DataHall.data_hall_0.Rack.rack_1 NaN Telemetry 1.664345e+09 None value Rack asset_path DataHall.data_hall_0.CRAC.crac_0 115.49 NaN DataHall.data_hall_0.Rack.rack_0 656.66 rack_0 DataHall.data_hall_0.Rack.rack_1 198.19 rack_1 } There is also a parameter value_only can be passed to the table constructor, which can be True or False . By default, it is False . If it is True , the timestamp and unit information will be removed from the returned table. >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = False , >>> value_only = True , >>> ) . value { 'DataHall': { 'data_hall_0': { 'CRAC': {'crac_0': {'Telemetry': {'ActivePower': 469.97}}}, 'Rack': { 'rack_0': {'Telemetry': {'ActivePower': 926.17}}, 'rack_1': {'Telemetry': {'ActivePower': 924.3}} } } } } >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = True , >>> value_only = True >>> )[ \"ActivePower\" ] . value path DataHall \\ asset_path DataHall.data_hall_0.CRAC.crac_0 DataHall.CRAC.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_0 DataHall.Rack.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_1 DataHall.Rack.ActivePower data_hall_0 CRAC type value Rack asset_path DataHall.data_hall_0.CRAC.crac_0 crac_0 Telemetry 51.80 NaN DataHall.data_hall_0.Rack.rack_0 NaN Telemetry 698.57 rack_0 DataHall.data_hall_0.Rack.rack_1 NaN Telemetry 109.14 rack_1 Also, when as_dataframe=True , there is an additional parameter include_tags can be passed to the table constructor. By default, it is all , which means the return dataframe will include as much information as possible, as shown in above example. If include_tags is set to an asset type or a list of asset types, the columns will be filtered to only include the specified asset types. For example, >>> context . tiro_table ( >>> \" %% FlowRate\" , >>> type = \"status\" , >>> as_dataframe = True , >>> include_tags = [ \"Rack\" , \"Server\" ], >>> value_only = True >>> )[ \"FlowRate\" ] . value Rack Server value 0 rack_0 server_0 527.90 1 rack_0 server_1 597.79 2 rack_0 server_2 587.88 3 rack_0 server_3 863.72 4 rack_1 server_3 863.72 5 rack_1 server_4 590.90 6 rack_1 server_5 865.13 7 rack_1 server_6 357.40 8 rack_1 server_7 125.10 More Complex Status Query \u00b6 When querying status data, other than using the simple query string, one can also pass in a more complex dictionary (or a yaml file containing the dictionary) to achieve more complex query. Here is an example. >>> query = \"\"\" >>> DataHall: >>> Rack: >>> $name_match: _1 >>> Server: >>> FlowRate: >>> $lt: 400 >>> CRAC: >>> ActivePower: >>> _ReturnTemperature: # >>> $lt: 15 >>> $gt: 25 >>> \"\"\" >>> query_dict = yaml . safe_load ( query ) >>> context . tiro_table ( >>> yaml . safe_load ( query ), >>> type = \"status\" , >>> value_only = True >>> ) . value { 'DataHall': { 'data_hall_0': { 'Rack': { 'rack_1': { 'Server': { 'server_3': {'Telemetry': {'FlowRate': 468.75}}, 'server_5': {'Telemetry': {'FlowRate': 892.7}}, 'server_6': {'Telemetry': {'FlowRate': 903.21}}, 'server_7': {'Telemetry': {'FlowRate': 879.9}} } } }, 'CRAC': {'crac_0': {'Telemetry': {'ActivePower': 732.24}}} } } } The above example queries the servers' current flow rates and CRACs' active powers at the same time, but Only considers the servers in the racks whose names contain _1 (in our case, only \"rack_1\"); Only considers the servers whose flow rates are less than 400; Only considers the CRACs whose return temperatures are between 15 and 25. Note Available filters: $name_match : match the asset name with the given regex pattern. $lt , $le , $gt , $ge : compare the value with the given number. $match , $not_match : match the string value with the given regex pattern. The data point name starts with _ will not be included in the returned data, but they can be used to filter the data points.","title":"Utinni Querying Guide"},{"location":"topics/utinni_querying_guide/#query-guide","text":"This guide explains how to query data from using Tiro-provided Utinni data pump in more details.","title":"Query Guide"},{"location":"topics/utinni_querying_guide/#query-string","text":"The most common way to query data is to use the query string. The query string is a string to match the \"type path\" of the data point. The \"type path\" is the path to the data point in the scenario tree, but includes only the asset type names but no asset names. For example, the \"type path\" of the data point DataHall.data_hall.CRAC.crac_1.Telemetry.SupplyTemperature is DataHall.CRAC.SupplyTemperature . A query string is basically a regular expression. However, because . is a special character in regular expression, we use % to represent . in the query string. For example, the query string DataHall%CRAC%SupplyTemperature matches the data point DataHall.CRAC.SupplyTemperature . Also, we introduce %% to match any number of characters including . . For example, the query string DataHall%%Temperature matches the data point DataHall.CRAC.SupplyTemperature and DataHall.Rack.CPUTemperature . The table definition of the Tiro Pump has a unified interface context . tiro_table ( query_str , type , ** kwargs ) where query_str is the query string, and type can be historian or status .","title":"Query String"},{"location":"topics/utinni_querying_guide/#querying-historian-data","text":"When passing type=\"historian\" to the table constructor, the table will query historian data. The table will always be \"3-D\" table, that is, the value is a dictionary of pandas.DataFreame indexed by the timestamp. The keys of the dictionary are the data point names. The columns of the pandas.DataFreame are the data point names. For example, the following query will return a dictionary of two tabels. >>> context . bind ( start =- timedelta ( hours = 1 ), step = timedelta ( minutes = 30 )) >>> context . tiro_table ( \" %% CRAC %% Temperature\" , type = \"historian\" ) . value { 'ReturnTemperature': DataHall.data_hall_0.CRAC.crac_0 2022-09-28 12:30:00+08:00 NaN 2022-09-28 13:00:00+08:00 46.85 2022-09-28 13:30:00+08:00 43.84, 'SupplyTemperature': DataHall.data_hall_0.CRAC.crac_0 2022-09-28 12:30:00+08:00 NaN 2022-09-28 13:00:00+08:00 45.52 2022-09-28 13:30:00+08:00 1.42 } There is an additional parameter column can be passed to the table constructor, which can be \"asset_path\" or a asset type like \"Rack\" . By default, it is asset_path , which means the full path of the data point, like DataHall.data_hall.CRAC.crac_1.Telemetry.SupplyTemperature . In this case the returned dataframe's column will be the full path of the data point. Otherwise, if column is set to an asset type, the name of the asset of the appointed type which relates to the data point will be used as the column name. For example, if column is set to \"Rack\" , and the query string is %%Rack%ActivePower , the returned data frame will be a dataframe whose column is different rack names, and the value is corresponding active powers When passing the column parameter, if there are multiple columns which have the same name, they will be aggregated, and the aggregation function and additional keyword arguments can be passed to the table constructor by asset_agg_fn and asser_agg_fn_kwargs , respectively. For example, the following query will return total flow rate of each rack. The default aggregation function is mean , and the default aggregation function keyword arguments is {} . >>> context . bind ( start =- timedelta ( hours = 1 ), step = timedelta ( minutes = 30 )) >>> context . tiro_table ( >>> \" %% Rack %% FlowRate\" , >>> type = \"historian\" , >>> column = \"Rack\" , >>> asset_agg_fn = \"sum\" , >>> )[ \"FlowRate\" ] . value rack_0 rack_1 2022-09-28 12:30:00+08:00 0.00 0.00 2022-09-28 13:00:00+08:00 2150.16 2649.47 2022-09-28 13:30:00+08:00 1922.42 1848.53","title":"Querying Historian Data"},{"location":"topics/utinni_querying_guide/#querying-status-data","text":"When passing type=\"status\" to the table constructor, the table will query status data. Additional parameter as_data_frame can be passed to the table constructor, which can be True or False . By default, it is False . In this case, the returned table will contain an embedded table. If as_data_frame is set to True , Tiro will try to convert the embedded table to a pandas.DataFreame . >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = False , >>> ) . value { 'DataHall': { 'data_hall_0': { 'CRAC': { 'crac_0': { 'Telemetry': {'ActivePower': {'timestamp': 1664344487.591439, 'unit': None, 'value': 665.63}} } }, 'Rack': { 'rack_0': { 'Telemetry': {'ActivePower': {'timestamp': 1664344487.941468, 'unit': None, 'value': 648.4}} }, 'rack_1': { 'Telemetry': {'ActivePower': {'timestamp': 1664344487.672051, 'unit': None, 'value': 575.49}} } } } } } >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = True , >>> )[ \"ActivePower\" ] . value { 'ActivePower': path DataHall \\ asset_path DataHall.data_hall_0.CRAC.crac_0 DataHall.CRAC.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_0 DataHall.Rack.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_1 DataHall.Rack.ActivePower data_hall_0 CRAC type timestamp unit \\ asset_path DataHall.data_hall_0.CRAC.crac_0 crac_0 Telemetry 1.664345e+09 None DataHall.data_hall_0.Rack.rack_0 NaN Telemetry 1.664345e+09 None DataHall.data_hall_0.Rack.rack_1 NaN Telemetry 1.664345e+09 None value Rack asset_path DataHall.data_hall_0.CRAC.crac_0 115.49 NaN DataHall.data_hall_0.Rack.rack_0 656.66 rack_0 DataHall.data_hall_0.Rack.rack_1 198.19 rack_1 } There is also a parameter value_only can be passed to the table constructor, which can be True or False . By default, it is False . If it is True , the timestamp and unit information will be removed from the returned table. >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = False , >>> value_only = True , >>> ) . value { 'DataHall': { 'data_hall_0': { 'CRAC': {'crac_0': {'Telemetry': {'ActivePower': 469.97}}}, 'Rack': { 'rack_0': {'Telemetry': {'ActivePower': 926.17}}, 'rack_1': {'Telemetry': {'ActivePower': 924.3}} } } } } >>> context . tiro_table ( >>> \" %% ActivePower\" , >>> type = \"status\" , >>> as_dataframe = True , >>> value_only = True >>> )[ \"ActivePower\" ] . value path DataHall \\ asset_path DataHall.data_hall_0.CRAC.crac_0 DataHall.CRAC.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_0 DataHall.Rack.ActivePower data_hall_0 DataHall.data_hall_0.Rack.rack_1 DataHall.Rack.ActivePower data_hall_0 CRAC type value Rack asset_path DataHall.data_hall_0.CRAC.crac_0 crac_0 Telemetry 51.80 NaN DataHall.data_hall_0.Rack.rack_0 NaN Telemetry 698.57 rack_0 DataHall.data_hall_0.Rack.rack_1 NaN Telemetry 109.14 rack_1 Also, when as_dataframe=True , there is an additional parameter include_tags can be passed to the table constructor. By default, it is all , which means the return dataframe will include as much information as possible, as shown in above example. If include_tags is set to an asset type or a list of asset types, the columns will be filtered to only include the specified asset types. For example, >>> context . tiro_table ( >>> \" %% FlowRate\" , >>> type = \"status\" , >>> as_dataframe = True , >>> include_tags = [ \"Rack\" , \"Server\" ], >>> value_only = True >>> )[ \"FlowRate\" ] . value Rack Server value 0 rack_0 server_0 527.90 1 rack_0 server_1 597.79 2 rack_0 server_2 587.88 3 rack_0 server_3 863.72 4 rack_1 server_3 863.72 5 rack_1 server_4 590.90 6 rack_1 server_5 865.13 7 rack_1 server_6 357.40 8 rack_1 server_7 125.10","title":"Querying Status Data"},{"location":"topics/utinni_querying_guide/#more-complex-status-query","text":"When querying status data, other than using the simple query string, one can also pass in a more complex dictionary (or a yaml file containing the dictionary) to achieve more complex query. Here is an example. >>> query = \"\"\" >>> DataHall: >>> Rack: >>> $name_match: _1 >>> Server: >>> FlowRate: >>> $lt: 400 >>> CRAC: >>> ActivePower: >>> _ReturnTemperature: # >>> $lt: 15 >>> $gt: 25 >>> \"\"\" >>> query_dict = yaml . safe_load ( query ) >>> context . tiro_table ( >>> yaml . safe_load ( query ), >>> type = \"status\" , >>> value_only = True >>> ) . value { 'DataHall': { 'data_hall_0': { 'Rack': { 'rack_1': { 'Server': { 'server_3': {'Telemetry': {'FlowRate': 468.75}}, 'server_5': {'Telemetry': {'FlowRate': 892.7}}, 'server_6': {'Telemetry': {'FlowRate': 903.21}}, 'server_7': {'Telemetry': {'FlowRate': 879.9}} } } }, 'CRAC': {'crac_0': {'Telemetry': {'ActivePower': 732.24}}} } } } The above example queries the servers' current flow rates and CRACs' active powers at the same time, but Only considers the servers in the racks whose names contain _1 (in our case, only \"rack_1\"); Only considers the servers whose flow rates are less than 400; Only considers the CRACs whose return temperatures are between 15 and 25. Note Available filters: $name_match : match the asset name with the given regex pattern. $lt , $le , $gt , $ge : compare the value with the given number. $match , $not_match : match the string value with the given regex pattern. The data point name starts with _ will not be included in the returned data, but they can be used to filter the data points.","title":"More Complex Status Query"}]}